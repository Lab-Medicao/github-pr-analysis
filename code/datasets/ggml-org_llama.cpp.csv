number,title,author,createdAt,closedOrMergedAt,reviewsCount,hoursOpen
17,Add oneliner for batch quantization,jooray,2023-03-11T17:45:06Z,2023-03-13T16:26:23+00:00,3,46.69
20,Feature/repeat penalty,beiller,2023-03-11T19:25:03Z,2023-03-12T09:27:42+00:00,1,14.04
31,Windows fixes,etra0,2023-03-12T03:27:55Z,2023-03-12T20:15:00+00:00,2,16.78
40,Nix flake,niklaskorz,2023-03-12T09:25:56Z,2023-03-17T22:03:48+00:00,5,132.63
43,Reduce model loading time,maekawatoshiki,2023-03-12T10:43:37Z,2023-03-13T16:33:44+00:00,3,29.84
45,Fix typo in README,marckohlbrugge,2023-03-12T11:10:37Z,2023-03-12T20:30:09+00:00,1,9.33
48,"use fprintf for diagnostic output, keep printf only for printing model output",prusnak,2023-03-12T13:34:22Z,2023-03-13T16:39:56+00:00,2,27.09
51,fix: add POSIX functionality for Linux compilation,valentynbez,2023-03-12T14:42:43Z,2023-03-22T17:20:25+00:00,1,242.63
56,Add back top_k,beiller,2023-03-12T18:13:33Z,2023-03-12T20:23:15+00:00,2,2.16
68,Misc: Make the conversion script executable,Dietr1ch,2023-03-13T00:09:01Z,2023-03-14T17:13:39+00:00,1,41.08
72,Misc: Use argparse,Dietr1ch,2023-03-13T00:35:52Z,2023-03-13T16:17:55+00:00,2,15.7
75,Initial support for CMake,etra0,2023-03-13T02:33:08Z,2023-03-13T17:12:34+00:00,1,14.66
77,Create a C-style API similar to whisper.cpp,thomasantony,2023-03-13T03:04:55Z,2023-03-21T20:52:07+00:00,18,209.79
79,Fix UTF-8 handling (including colors),kharvd,2023-03-13T05:07:22Z,2023-03-13T16:24:19+00:00,2,11.28
90,Add NetBSD support.,0-wiz-0,2023-03-13T11:25:53Z,2023-03-13T16:40:54+00:00,1,5.25
92,Add quantize script for batch quantization,prusnak,2023-03-13T12:50:30Z,2023-03-13T16:15:21+00:00,1,3.41
98,Add windows to the CI,etra0,2023-03-13T18:33:45Z,2023-03-13T20:29:11+00:00,1,1.92
109,Refactoring `convert-pth-to-ggml.py`: more concise and readable,qunash,2023-03-14T00:59:06Z,2023-03-19T17:17:40+00:00,1,136.31
114,add ptread link to fix cmake build under linux,mmyjona,2023-03-14T04:22:51Z,2023-03-17T16:38:24+00:00,1,84.26
120,add SIGINT support for _WIN32 environments,bitRAKE,2023-03-14T06:40:00Z,2023-03-15T19:56:25+00:00,1,37.27
126,Fix potential licensing issue,musabgultekin,2023-03-14T10:15:26Z,2023-03-15T19:39:07+00:00,1,33.39
131,Adding missing features of CMakeLists.txt & Refactoring,nusu-github,2023-03-14T12:57:49Z,2023-03-21T00:37:17+00:00,9,155.66
132,🚀 Dockerize llamacpp,bernatvadell,2023-03-14T13:21:47Z,2023-03-17T09:47:06+00:00,9,68.42
139,Don't use vdotq_s32 if it's not available,Ronsor,2023-03-14T17:54:20Z,2023-03-14T19:34:37+00:00,2,1.67
142,Use `tokenizer.vocab_size()` instead of hardcoding 32000 when converting,Ronsor,2023-03-14T20:34:30Z,2023-03-15T19:37:50+00:00,1,23.06
148,added ctx_size parameter,RazeLighter777,2023-03-15T01:50:38Z,2023-03-15T19:42:40+00:00,1,17.87
149,fixed color reset on exit,RazeLighter777,2023-03-15T01:57:37Z,2023-03-15T19:39:38+00:00,2,17.7
151,fixed warning with std::ignore about unused function result,RazeLighter777,2023-03-15T02:38:08Z,2023-03-18T11:44:09+00:00,3,81.1
154,Use F16 for memory_k and memory_v (as suggested in #146),ty-everett,2023-03-15T06:16:30Z,2023-03-19T18:18:42+00:00,1,108.04
161,"FIX: ""inline"" -> ""static inline"" for bytesFromNibbles and packNibbles",hoangmit,2023-03-15T15:09:12Z,2023-03-15T19:05:15+00:00,1,3.93
176,Streaming conversion with no torch,diimdeep,2023-03-15T20:25:58Z,2023-03-30T19:34:27+00:00,2,359.14
181,Add parameter to ignore end of text token,slaren,2023-03-15T21:17:50Z,2023-03-19T18:22:49+00:00,1,93.08
193,Q4_1 acceleration,blackhole89,2023-03-16T00:14:41Z,2023-03-17T04:48:40+00:00,1,28.57
198,Add chatLLaMa script,j3k0,2023-03-16T07:59:02Z,2023-03-21T16:23:16+00:00,1,128.4
216,Making weights loading faster,oKatanaaa,2023-03-16T22:39:40Z,2023-03-30T07:45:14+00:00,2,321.09
221,"fix coloring of last `n_batch` of prompt, and refactor line input",bitRAKE,2023-03-17T02:24:08Z,2023-03-19T19:44:31+00:00,1,65.34
222,Improved quantize script,SuajCarrot,2023-03-17T03:33:34Z,2023-03-19T18:38:45+00:00,16,63.09
226,fix compile error on centos 7.5,leaout,2023-03-17T07:01:04Z,2023-03-17T14:44:04+00:00,1,7.72
230,CI Improvements,anzz1,2023-03-17T08:11:11Z,2023-03-18T07:27:13+00:00,1,23.27
232,improvement(tools): optimize convert-pth-to-ggml,tpoisonooo,2023-03-17T09:02:11Z,2023-03-19T17:18:03+00:00,1,56.26
235,Docker - Fix publish docker image in GitHub Registry,bernatvadell,2023-03-17T09:57:36Z,2023-03-20T17:05:21+00:00,1,79.13
242,Implement non-greedy tokenizer that tries to maximize token lengths,thement,2023-03-17T16:36:53Z,2023-03-17T20:05:58+00:00,5,3.48
252,sentencepiece bpe compatible tokenizer,eiz,2023-03-18T03:05:18Z,2023-03-20T10:17:24+00:00,6,55.2
254,Fix n^2 loop in tokenization,glinscott,2023-03-18T05:34:52Z,2023-03-18T11:17:19+00:00,1,5.71
262,Remove unused code since n_vocab is model.hparams.n_vocab,tiendung,2023-03-18T12:32:06Z,2023-03-18T13:51:49+00:00,1,1.33
270,Compute perplexity over prompt,glinscott,2023-03-18T21:16:47Z,2023-03-21T16:27:43+00:00,1,67.18
273,Added script to invoke alpaca model,nullhook,2023-03-18T21:44:48Z,2023-04-13T12:52:13+00:00,1,615.12
278,Proof of concept TCP server mode,tarruda,2023-03-19T02:46:27Z,2023-03-22T21:03:19+00:00,4,90.28
282,Add embedding mode with arg flag. Currently working,StrikingLoo,2023-03-19T06:39:36Z,2023-03-24T15:05:13+00:00,15,128.43
283,Fixed color reset problem in interactive mode.,mqy,2023-03-19T07:03:27Z,2023-03-19T18:10:01+00:00,1,11.11
284,"   bugfix: centos 7, gcc (GCC) 11.2.1 20220127 (Red Hat 11.2.1-9)",OvJat,2023-03-19T09:11:15Z,2023-03-21T06:44:33+00:00,1,45.55
293,Add tqdm to Python requirements,sw,2023-03-19T12:18:20Z,2023-03-20T08:24:12+00:00,1,20.1
294,Command line switch to use F16 for memory_k and memory_v (refactor of #154),Green-Sky,2023-03-19T13:55:56Z,2023-03-19T17:57:00+00:00,1,4.02
299,Support for multiple reverse prompts,tjohnman,2023-03-19T18:32:22Z,2023-03-19T19:33:07+00:00,1,1.01
301,Importer for GPTQ quantized LLaMA models,comex,2023-03-19T19:06:17Z,2023-03-21T16:42:25+00:00,1,45.6
305,We could use std::unordered_map over std::map,Fabio3rs,2023-03-19T21:52:07Z,2023-03-21T17:21:50+00:00,14,43.5
306,Reset token budget after every user intervention.,tjohnman,2023-03-19T22:05:19Z,2023-03-24T15:25:44+00:00,3,113.34
311,Enable ANSI colors on Windows 10+,anzz1,2023-03-19T23:58:18Z,2023-03-21T16:14:46+00:00,2,40.27
312,Fix color codes emitting mid-UTF8 code.,blackhole89,2023-03-20T01:58:04Z,2023-03-21T17:11:02+00:00,2,39.22
314,Add OpenBSD support,kevlo,2023-03-20T05:50:02Z,2023-03-21T15:50:10+00:00,1,34.0
319,"move file magic/version to header, print expected version",eiz,2023-03-20T10:33:08Z,2023-03-20T19:26:02+00:00,1,8.88
320,Add initial AVX512 support for dot product on Linux,Ameobea,2023-03-20T11:20:22Z,2023-03-21T14:35:42+00:00,9,27.26
325,Fixed tokenizer.model not found error when model dir is symlink,mqy,2023-03-20T12:58:29Z,2023-03-20T19:33:11+00:00,1,6.58
330,Check for reverse prompt by characters instead of tokens (#292),tjohnman,2023-03-20T15:08:05Z,2023-03-21T16:05:06+00:00,1,24.95
333,Replace EOS with newline to prevent context/memory being flushed by EOS in interactive mode,rabidcopy,2023-03-20T16:48:35Z,2023-03-23T20:22:48+00:00,15,75.57
335,Makefile: slightly cleanup for Mac Intel; replace './main -h' with echo.,mqy,2023-03-20T17:36:53Z,2023-03-21T15:44:11+00:00,1,22.12
338,sha256 check sums to verify original and converted model data,gjmulder,2023-03-20T20:19:11Z,2023-03-21T22:19:12+00:00,2,26.0
341,MMAP for Windows (not working atm),oKatanaaa,2023-03-20T22:07:32Z,2023-03-28T17:17:16+00:00,6,187.16
348,cmdline option for custom amount of model parts (--n_parts N),anzz1,2023-03-21T08:28:06Z,2023-03-21T15:42:43+00:00,3,7.24
354,Don't force immediate interactive without `-i`,tjohnman,2023-03-21T14:16:15Z,2023-03-22T17:16:36+00:00,2,27.01
370,IMPORTANT: Introduce C-style API - Major Refactoring,ggerganov,2023-03-21T20:51:40Z,2023-03-22T05:32:36+00:00,22,8.68
375,Enable Fused-Multiply-Add (FMA) and F16C/CVT16 vector extensions on MSVC,anzz1,2023-03-22T01:47:45Z,2023-03-28T19:44:30+00:00,1,161.95
376,CI: CMake: Separate build and test steps,anzz1,2023-03-22T02:41:30Z,2023-03-23T02:20:35+00:00,3,23.65
380,Fix Nix build,siraben,2023-03-22T05:38:38Z,2023-03-23T16:51:26+00:00,1,35.21
383,Deduplicate q4 quantization functions,sw,2023-03-22T07:35:27Z,2023-03-22T17:29:07+00:00,1,9.89
390,fix perplexity after c-api refactor,Green-Sky,2023-03-22T12:00:06Z,2023-03-22T16:09:39+00:00,1,4.16
392,cmake: make llama an actual library,Green-Sky,2023-03-22T13:47:10Z,2023-03-22T16:37:11+00:00,1,2.83
393,Add a Package.swift for SwiftPM support,j-f1,2023-03-22T14:06:01Z,2023-03-28T16:39:01+00:00,3,146.55
403,Converting GGML back to Torch checkpoint for HuggingFace/Pytorch consumption/training/finetuning,ductai199x,2023-03-22T17:40:07Z,2023-03-28T17:51:30+00:00,1,144.19
407,Add support for batch size to `--perplexity`,glinscott,2023-03-22T19:24:48Z,2023-04-13T21:50:42+00:00,1,530.43
408,Update tools.sh to use consolidated,RSereno,2023-03-22T20:30:04Z,2023-04-13T12:54:37+00:00,4,520.41
409,Fix instruct mode broken by PR #354,tjohnman,2023-03-22T21:24:08Z,2023-03-23T00:30:24+00:00,1,3.1
416,Delete SHA256SUMS for now,anzz1,2023-03-23T07:39:43Z,2023-03-23T10:26:19+00:00,1,2.78
418,Fix Makefile echo escape codes (by removing them).,KerfuffleV2,2023-03-23T08:00:40Z,2023-03-23T11:41:32+00:00,1,3.68
420,(Windows) Set console to UTF-8 on init,anzz1,2023-03-23T09:11:38Z,2023-03-25T20:29:22+00:00,5,59.3
421,Move model section from issue template to README.md,gjmulder,2023-03-23T09:49:01Z,2023-03-23T11:30:41+00:00,1,1.69
423,Fix GPTQ converter,R2D2FISH,2023-03-23T11:21:01Z,2023-03-23T20:18:13+00:00,3,8.95
424,Command line args bounds checking,anzz1,2023-03-23T11:50:53Z,2023-03-23T17:54:28+00:00,5,6.06
426,feat: '--in-prefix STRING' option,anzz1,2023-03-23T12:05:16Z,2023-03-25T12:03:19+00:00,2,47.97
428,Fix quantize script not finding models in parent directory,j-f1,2023-03-23T13:07:18Z,2023-03-23T20:42:52+00:00,1,7.59
430,Generate library with CMake,nusu-github,2023-03-23T14:14:19Z,2023-03-23T20:16:49+00:00,7,6.04
434,Add support for file load progress reporting callbacks,j-f1,2023-03-23T17:53:29Z,2023-03-25T05:26:29+00:00,3,35.55
438,dynamic estimate of required memory usage,Green-Sky,2023-03-23T18:24:04Z,2023-03-24T10:19:14+00:00,4,15.92
444,Update README.md to explicitly state model linking and download policy,gjmulder,2023-03-23T22:30:20Z,2023-03-24T15:23:10+00:00,3,16.88
453,Support calling mlock() on loaded model data on Linux and macOS,comex,2023-03-24T03:08:33Z,2023-03-24T15:19:05+00:00,2,12.18
454,additional optimizations for POWER9,classilla,2023-03-24T03:26:04Z,2023-03-24T15:19:27+00:00,1,11.89
458,Be more strict about converting float to double,sw,2023-03-24T09:45:14Z,2023-03-28T16:48:20+00:00,13,103.05
477,Trace model outputs to a binary file,Piezoid,2023-03-24T21:45:39Z,2023-05-26T19:50:55+00:00,3,1510.09
478,Add timings for the prompt evaluation,slaren,2023-03-24T22:18:13Z,2023-03-25T14:34:23+00:00,1,16.27
483,Add missing struct annotation,Doomsdayrs,2023-03-25T00:25:59Z,2023-03-25T05:21:24+00:00,1,4.92
491,Exit from interactive mode if input stream is bad,haraldF,2023-03-25T11:52:12Z,2023-03-26T05:25:46+00:00,5,17.56
497,CMake / CI additions,anzz1,2023-03-25T14:34:29Z,2023-03-25T21:38:12+00:00,12,7.06
509,Refactor quantized processing functions,sw,2023-03-25T20:53:40Z,2023-03-28T17:13:02+00:00,1,68.32
514,Add support for linux/arm64 platform during Docker Builds,gaby,2023-03-25T23:19:26Z,2023-03-26T14:48:42+00:00,1,15.49
515,Add AVX2 implementation of quantize_row_q4_1,slaren,2023-03-26T00:06:42Z,2023-03-28T18:06:03+00:00,4,65.99
526,Add backwards-compatibility for older model format,thement,2023-03-26T14:26:06Z,2023-03-26T19:16:22+00:00,1,4.84
527,ci: add debug build to sanitzier build matrix,Green-Sky,2023-03-26T14:27:11Z,2023-03-26T15:48:40+00:00,1,1.36
529,Continue on empty line,thement,2023-03-26T14:55:01Z,2023-03-27T13:34:53+00:00,1,22.66
539,Add script to convert old ggml files to newer version,thement,2023-03-26T20:48:52Z,2023-03-28T17:55:43+00:00,5,45.11
540,Add embedding example to Makefile,rjadr,2023-03-26T21:37:43Z,2023-03-28T06:11:10+00:00,1,32.56
542,Fix missing ggml link in cmake for executables in examples on w64-mingw32,marcom,2023-03-26T23:35:41Z,2023-03-27T04:55:26+00:00,1,5.33
545,New conversion script,comex,2023-03-27T04:11:54Z,2023-04-14T07:03:03+00:00,4,434.85
546,CI: fix subdirectory path globbing,anzz1,2023-03-27T04:53:48Z,2023-03-28T19:43:26+00:00,9,38.83
547,Refactored code for reduced memory usage and improved readability,fritzprix,2023-03-27T05:38:11Z,2023-03-28T17:02:34+00:00,2,35.41
551,Revert 7e53955 (#542) and fix properly,anzz1,2023-03-27T07:54:37Z,2023-03-28T18:23:09+00:00,1,34.48
555,"instruct.cpp, continue on empty line, endless instruct mode, refactors",anzz1,2023-03-27T13:33:55Z,2023-03-28T11:26:27+00:00,1,21.88
563,Fix usage of F16C intrinsics in AVX code,slaren,2023-03-27T21:38:28Z,2023-03-28T14:26:56+00:00,13,16.81
564,Added support for _POSIX_MAPPED_FILES if defined in source,CoderRC,2023-03-28T00:54:42Z,2023-03-28T21:26:25+00:00,1,20.53
571,"main.cpp fixes, refactoring",anzz1,2023-03-28T12:44:09Z,2023-03-28T14:09:56+00:00,5,1.43
577,Use the same batch size threshold for enabling OpenBLAS and disabling ggml threading,Piezoid,2023-03-28T14:58:28Z,2023-03-29T16:10:07+00:00,1,25.19
581,parallelize the quantization process,tristan0x,2023-03-28T15:52:58Z,2023-03-30T19:42:06+00:00,2,51.82
583,add example of re-act pattern,tobi,2023-03-28T21:08:48Z,2023-03-29T15:10:24+00:00,3,18.03
584,CI: Re-enable AVX512 testing (Windows-MSVC),anzz1,2023-03-28T22:02:00Z,2023-03-29T20:44:39+00:00,1,22.71
586,Add support for memory mapping models,slaren,2023-03-29T00:44:59Z,2023-03-30T00:31:32+00:00,6,23.78
592,Create chat-13B.bat,Royalphax,2023-03-29T08:09:52Z,2023-03-29T17:21:09+00:00,2,9.19
593,Fix typo in llama.h,anzz1,2023-03-29T11:23:38Z,2023-03-29T13:19:29+00:00,1,1.93
598,"Enable -std= for cmake builds, fix warnings",sw,2023-03-29T15:45:58Z,2023-03-31T19:19:17+00:00,5,51.56
600,rename convert_ggml_to_pth.py -> convert-ggml-to-pth.py,prusnak,2023-03-29T17:06:48Z,2023-03-29T18:09:26+00:00,1,1.04
605,py : cleanup the code,prusnak,2023-03-29T19:37:00Z,2023-03-31T08:32:01+00:00,5,36.92
607,Remove unused variable,Ameobea,2023-03-29T21:12:40Z,2023-03-30T17:53:35+00:00,2,20.68
609,Use -march=native -mtune=native on x86 (Also Enables AVX512 on macOS),cmdrf,2023-03-29T22:21:34Z,2023-04-02T07:17:05+00:00,1,80.93
613,Make loading weights 10-100x faster ,jart,2023-03-29T23:51:39Z,2023-03-30T19:28:25+00:00,13,19.61
615,fix darwin f16c flags check,keen99,2023-03-30T00:50:37Z,2023-03-30T17:34:45+00:00,1,16.74
617,Add AVX acceleration,perserk,2023-03-30T05:10:47Z,2023-03-31T11:55:44+00:00,1,30.75
626,Allow larger tensor sizes.,Seltsamsel,2023-03-30T16:25:44Z,2023-04-02T10:21:31+00:00,1,65.93
642,Optimize AVX2 ggml_vec_dot_q4_0,slaren,2023-03-31T01:18:01Z,2023-03-31T15:55:52+00:00,1,14.63
651,Fix memory bugs in loading code,jart,2023-03-31T12:14:08Z,2023-04-13T12:42:45+00:00,3,312.48
653,Benchmark test case for q4_0 matrix multiplication,SebastianApel,2023-03-31T14:52:06Z,2023-04-13T12:46:24+00:00,15,309.9
654,10+% performance improvement of ggml_vec_dot_q4_0 on AVX2,SebastianApel,2023-03-31T16:21:54Z,2023-04-03T07:52:28+00:00,9,63.51
656,Show error message when -f fails,slaren,2023-03-31T18:05:22Z,2023-04-01T14:08:40+00:00,1,20.05
658,examples: add gpt4all script,leonardohn,2023-03-31T19:02:17Z,2023-04-02T07:56:21+00:00,1,36.9
664,Use getopts for example scripts,siraben,2023-03-31T22:38:34Z,2023-04-13T12:49:31+00:00,1,302.18
665,Remove torch GPU dependencies from the Docker.full image,bsilvereagle,2023-03-31T22:53:37Z,2023-04-02T22:13:03+00:00,3,47.32
678,Clean up QK and file and tensor types,sw,2023-04-01T12:09:10Z,2023-04-15T16:29:37+00:00,8,340.34
680,Update README.md,rimoliga,2023-04-01T13:04:49Z,2023-04-01T14:57:30+00:00,2,1.88
682,Be nice to CI machines by not allocating buffers,sw,2023-04-01T15:05:22Z,2023-04-02T07:18:53+00:00,1,16.23
685,Feature: Added api for getting/setting the kv_cache,chrfalch,2023-04-01T15:43:21Z,2023-04-02T10:23:05+00:00,3,18.66
689,Fix for temp == 0 issue #684,Fabio3rs,2023-04-01T20:03:21Z,2023-04-03T00:19:30+00:00,3,28.27
690,Add a missing step to the gpt4all instructions,ThatcherC,2023-04-01T21:08:14Z,2023-04-02T10:48:57+00:00,1,13.68
697,Fix default params for examples/main,mvrilo,2023-04-02T01:33:18Z,2023-04-02T02:41:12+00:00,2,1.13
700,Update ReadMe,Shreyas-ITB,2023-04-02T05:16:09Z,2023-04-02T11:10:31+00:00,3,5.91
703,Optimize non-SIMD Q4 vector dot product,sw,2023-04-02T09:22:35Z,2023-04-13T14:59:51+00:00,4,269.62
706,Add -n -1 to alpaca and gpt4all scripts,niansa,2023-04-02T10:37:32Z,2023-04-13T13:03:39+00:00,1,266.44
709,Introduce enum llama_ftype,sw,2023-04-02T11:42:30Z,2023-04-11T15:03:52+00:00,4,219.36
720,Define non-positive temperature behavior,ivanstepanovftw,2023-04-02T20:08:35Z,2023-04-03T00:19:05+00:00,2,4.17
724,Add Miku.sh,at8u,2023-04-02T21:47:32Z,2023-04-05T14:32:42+00:00,1,64.75
728,Add quantize-stats command for testing quantization,unbounded,2023-04-03T01:55:12Z,2023-04-07T22:09:19+00:00,2,116.24
729,Use full range for q4_0 quantization,unbounded,2023-04-03T02:09:54Z,2023-04-25T17:20:46+00:00,1,543.18
734,Fix model loading time through prefetching the file on another thread,CoderRC,2023-04-03T04:21:54Z,2023-04-19T02:36:25+00:00,4,382.24
736,Windows: reactivate sigint handler after each Ctrl-C,mgroeber9110,2023-04-03T06:42:24Z,2023-04-03T16:00:55+00:00,1,9.31
747,"Simplify to include lower-case windows.h always, fix compile on mingw32",marcom,2023-04-03T19:05:14Z,2023-04-10T17:57:59+00:00,2,166.88
748,README: Update with CMake and windows example,adithyab94,2023-04-03T20:07:06Z,2023-04-05T14:36:12+00:00,5,42.48
763,Added missing host optimizations in CXXFLAGS,ivanstepanovftw,2023-04-04T12:33:47Z,2023-04-05T14:38:38+00:00,1,26.08
765,Add Accelerate/BLAS when using Swift,a10y,2023-04-04T15:02:54Z,2023-04-05T10:44:24+00:00,2,19.69
768,AVX2 ggml_vec_dot_q4_0 performance improvement ~5%,x02Sylvie,2023-04-04T20:02:43Z,2023-04-13T13:13:38+00:00,1,209.18
769,"feat: add ""stop"" keywords as alternative to eot token",longregen,2023-04-04T20:42:57Z,2023-04-14T15:40:15+00:00,7,234.96
770,Fix magic in convert-gptq-to-ggml.py,prusnak,2023-04-05T07:25:03Z,2023-04-05T16:19:12+00:00,1,8.9
773,Add build.zig,iacore,2023-04-05T11:40:59Z,2023-04-05T15:06:03+00:00,1,3.42
775,Avoid heavy V transpose operation + improvements,ggerganov,2023-04-05T14:11:22Z,2023-04-05T19:07:34+00:00,2,4.94
781,ggml : multi-thread ggml_rope() (~3-4 times faster on M1),ggerganov,2023-04-05T16:17:01Z,2023-04-05T19:11:04+00:00,3,2.9
785,Make docker instructions more explicit,prusnak,2023-04-05T17:20:39Z,2023-04-06T06:56:58+00:00,1,13.61
796,Do not crash when it has nothing to say.,l29ah,2023-04-05T22:59:24Z,2023-04-06T15:59:12+00:00,2,17.0
797,ADD libllama.so target for llama-cpp-python,bhubbb,2023-04-05T23:05:01Z,2023-04-07T16:11:59+00:00,1,41.12
801,Rewrite loading code to try to satisfy everyone,comex,2023-04-06T05:57:51Z,2023-04-09T23:10:47+00:00,6,89.22
806,Allow the user to specify the path to the OpenBLAS headers. (#627),vpxyz,2023-04-06T10:22:03Z,2023-05-26T19:27:21+00:00,2,1209.09
807,optimize rope function to avoid call powf in the loop,howard0su,2023-04-06T11:56:27Z,2023-04-14T06:24:52+00:00,1,186.47
809,Add detection code for avx/avx2/etc,howard0su,2023-04-06T12:50:03Z,2024-02-19T14:26:47+00:00,2,7657.61
812,Always sort logits before nucleus sampling,ivanstepanovftw,2023-04-06T14:17:32Z,2023-04-07T16:02:12+00:00,1,25.74
814,Fix build.zig,iacore,2023-04-06T14:22:11Z,2023-04-07T16:05:29+00:00,1,25.72
816,ggml: refactor compute thread: merge three spin variables into one,mqy,2023-04-06T15:42:23Z,2023-04-13T11:26:33+00:00,1,163.74
820,Add LoRA support,slaren,2023-04-06T21:47:36Z,2023-04-17T15:28:55+00:00,17,257.69
824,Multi-thread `ggml_cpy()`,ironman5366,2023-04-07T05:05:32Z,2023-04-10T20:03:29+00:00,2,86.97
838,fix cblas_sgemm call in ggml_compute_forward_mul_mat_*?,bogdad,2023-04-07T16:21:20Z,2023-04-13T13:24:30+00:00,1,141.05
839,cmake should link openblas with -lopenblas,unknown,2023-04-07T20:25:24Z,2023-04-08T11:15:17+00:00,1,14.83
840,fix for windows utf-8 input,aroidzap,2023-04-07T21:50:09Z,2023-04-08T15:49:39+00:00,2,17.99
848,flake.nix: add all binaries from bin,prusnak,2023-04-08T10:49:58Z,2023-04-13T13:49:05+00:00,3,122.99
863,"Alternative for instruct mode (Vicuna support, etc.)",aroidzap,2023-04-09T04:23:35Z,2023-04-14T15:19:18+00:00,14,130.93
872,Update build.zig,foldl,2023-04-10T09:55:21Z,2023-04-13T13:43:23+00:00,1,75.8
874,Adding unary and binary map operations,KerfuffleV2,2023-04-10T13:14:19Z,2023-04-14T14:43:55+00:00,10,97.49
877,Add BAIR's Koala to supported models,qouoq,2023-04-10T17:55:31Z,2023-04-10T20:41:53+00:00,1,2.77
883,"Fix whitespace, add .editorconfig, add GitHub workflow",prusnak,2023-04-10T20:13:42Z,2023-04-11T19:45:45+00:00,1,23.53
884,Use aligned_alloc or _aligned_malloc,prusnak,2023-04-10T20:54:51Z,2023-04-13T14:08:32+00:00,2,65.23
888,Fixed rlimit error message.,apaz-cli,2023-04-11T00:13:51Z,2023-04-21T18:48:06+00:00,1,258.57
890,Windows fixes,comex,2023-04-11T02:48:18Z,2023-04-11T13:19:54+00:00,1,10.53
896,More accurate Q4_0 and Q4_1 quantizations,ikawrakow,2023-04-11T15:22:29Z,2023-04-21T15:33:12+00:00,5,240.18
902,readme: link to sha256sums file,NicoWeio,2023-04-11T23:27:28Z,2023-04-12T06:46:20+00:00,2,7.31
908,do not force the prompt file to end with a new line,prusnak,2023-04-12T06:34:30Z,2023-04-13T09:33:16+00:00,2,26.98
911,update readme: llama node binding,hlhr202,2023-04-12T08:28:00Z,2023-04-13T13:54:28+00:00,1,29.44
922,Use VLAs when possible,Rhialto,2023-04-12T18:50:30Z,2024-02-19T14:38:44+00:00,4,7507.8
924,Add commands to interactive mode. ,wbpxre150,2023-04-12T21:08:25Z,2023-04-14T17:46:06+00:00,2,44.63
927,Fix for quantize fail on init due to undefined static initialization of complex objects,arikpoz,2023-04-12T23:37:24Z,2023-04-17T14:41:53+00:00,2,111.07
929,Remove python 3.10 warning,CRD716,2023-04-13T01:31:36Z,2023-04-13T13:59:53+00:00,1,12.47
933,≈65% speedup of the AVX-512 implementation of `ggml_vec_dot_q4_0()`,dfyz,2023-04-13T03:10:33Z,2023-04-17T13:10:58+00:00,1,106.01
934,"fix(perf/UX): Use num physical cores by default, warn about E/P cores",jon-chuang,2023-04-13T05:01:35Z,2023-04-30T18:41:35+00:00,9,421.67
951,Add Q8_0 quantization for intermediate results,ggerganov,2023-04-13T20:09:03Z,2023-04-15T14:53:23+00:00,11,42.74
953,Unit test for quantization functions,unbounded,2023-04-13T22:51:49Z,2023-04-22T09:10:39+00:00,5,202.31
960,py : fix flake8 and isort nitpicks,prusnak,2023-04-14T08:29:09Z,2023-04-14T12:23:22+00:00,1,3.9
962,py : cleanup dependencies,prusnak,2023-04-14T08:36:52Z,2023-04-14T13:37:11+00:00,3,5.01
976,py : bump sentencepiece to 0.1.98 to support Python 3.11,prusnak,2023-04-14T18:43:28Z,2023-04-14T19:46:49+00:00,1,1.06
979,examples : switch input_noecho to input_echo to remove negation,deadprogram,2023-04-14T19:11:05Z,2023-05-02T16:13:26+00:00,2,429.04
980,readme : update gpt4all instructions,prusnak,2023-04-14T19:16:29Z,2023-04-23T08:21:26+00:00,2,205.08
981,nix: use convert.py instead of legacy wrapper convert-pth-to-ggml.py,prusnak,2023-04-14T19:18:30Z,2023-04-25T21:19:58+00:00,3,266.02
986,Fix potential int8_t overflow in non-SIMD vec_dot,sw,2023-04-14T20:35:20Z,2023-04-15T18:28:56+00:00,1,21.89
987,Fix result validation in benchmark-q4_0-matmult,dfyz,2023-04-14T20:37:33Z,2023-04-15T05:51:55+00:00,1,9.24
991,convert.py: Fix loading safetensors and ggml format on Windows,comex,2023-04-15T02:01:42Z,2023-04-15T21:53:21+00:00,1,19.86
992,add finding the OpenBLAS header file,katsu560,2023-04-15T02:18:49Z,2023-04-15T05:51:12+00:00,1,3.54
1009,Fix msys2 build error and warnings,na-na-hi,2023-04-16T02:48:34Z,2023-04-16T09:13:42+00:00,1,6.42
1014,examples : evaluate tokens in batches after swapping context,grencez,2023-04-16T09:57:11Z,2023-04-21T18:18:09+00:00,1,128.35
1021,print timings on ctrl+c exit,wbpxre150,2023-04-17T07:06:36Z,2023-04-22T08:56:35+00:00,4,121.83
1025,Created a Server example,zrthxn,2023-04-17T11:51:35Z,2023-05-11T11:27:39+00:00,1,575.6
1029,readme : add Ruby bindings,yoshoku,2023-04-17T15:23:14Z,2023-04-17T19:34:36+00:00,1,4.19
1031,add 4_0 to default outfile namestr dict,cammytown,2023-04-17T16:16:52Z,2023-04-17T18:26:24+00:00,1,2.16
1032,Make reverse prompt option act as a stop token in non-interactive sce…,data-angel,2023-04-17T17:20:22Z,2023-05-19T17:24:59+00:00,9,768.08
1035,Multi-threaded ggml_cpy,slaren,2023-04-18T01:32:24Z,2023-04-18T22:53:24+00:00,8,21.35
1040,Interface improvements and `--multiline-input` (previously `--author-mode`),DannyDaemonic,2023-04-18T10:42:58Z,2023-05-09T02:45:48+00:00,5,496.05
1041,Adding a simple program to measure speed of dot products,ikawrakow,2023-04-18T14:41:40Z,2023-04-18T19:00:14+00:00,1,4.31
1042,fix: ld link test-tokenizer-0 error,fumiama,2023-04-18T16:14:50Z,2023-04-21T18:27:07+00:00,4,74.2
1043,ggml : test dot product q4_0 x f32,ggerganov,2023-04-18T16:23:32Z,2023-04-19T16:01:31+00:00,1,23.63
1044,Add NVIDIA cuBLAS support,slaren,2023-04-18T16:41:16Z,2023-04-19T09:22:46+00:00,7,16.69
1046,ggml : Q4_2 ARM,ggerganov,2023-04-18T18:19:12Z,2023-04-18T20:54:57+00:00,2,2.6
1061,Q4 cleanup,sw,2023-04-19T14:18:03Z,2023-04-19T16:06:38+00:00,1,1.81
1062,Q4_2 quantization with rmse-optimized scale and quants,ikawrakow,2023-04-19T15:15:55Z,2023-04-19T18:20:14+00:00,10,3.07
1065,Improve cuBLAS performance by dequantizing on the GPU,slaren,2023-04-19T16:08:38Z,2023-04-20T01:14:15+00:00,4,9.09
1068,AVX2 optimization for vec_dot_q4_2_q8_0,sw,2023-04-19T18:48:47Z,2023-04-20T06:45:41+00:00,1,11.95
1073,Continuous layouts for quantization q4_0c,unbounded,2023-04-19T21:58:03Z,2023-10-10T21:34:02+00:00,1,4175.6
1074,Remove the LLAMA_ACCELERATE matrix dimension from Ubuntu builds in the CI,dfyz,2023-04-20T02:23:59Z,2023-04-20T15:15:19+00:00,1,12.86
1075,Multi-threaded quantization,ikawrakow,2023-04-20T05:45:29Z,2023-04-20T17:42:27+00:00,1,11.95
1080,fix: LLAMA_CUBLAS=1 undefined reference 'shm_open',fumiama,2023-04-20T11:09:53Z,2023-04-20T13:28:43+00:00,1,2.31
1082,Add Q4_3 quantization (ARM NEON),ggerganov,2023-04-20T16:00:00Z,2023-04-20T17:35:53+00:00,1,1.6
1083,A faster version for Q4_1 x Q8_0 dot products,ikawrakow,2023-04-20T17:01:00Z,2023-04-21T15:18:26+00:00,1,22.29
1087,ROCm Port,SlyEcho,2023-04-20T18:43:33Z,2023-08-25T09:09:42+00:00,15,3038.44
1088,"Add ggml-model-*.bin checksums for 7B, 13B, 30B",sw,2023-04-20T19:53:07Z,2023-04-20T21:56:45+00:00,1,2.06
1091,Have n_batch default to 512 when BLAS is enabled,unknown,2023-04-20T21:17:05Z,2023-04-22T08:27:05+00:00,1,35.17
1094,Improve cuBLAS performance by using a memory pool,slaren,2023-04-20T22:19:06Z,2023-04-21T19:59:17+00:00,2,21.67
1096,Show perplexity ETA in hours and minutes,slaren,2023-04-21T01:54:42Z,2023-04-21T12:57:57+00:00,2,11.05
1099,AVX2 optimization for vec_dot_q4_3_q8_0 and refactoring,sw,2023-04-21T09:25:15Z,2023-04-22T07:37:05+00:00,4,22.2
1100,Fix build under Windows when enable BUILD_SHARED_LIBS,howard0su,2023-04-21T13:22:57Z,2023-04-22T08:18:21+00:00,1,18.92
1105,"Feature: Added api for getting/setting the complete state: rng, logits, embedding and kv_cache",xaedes,2023-04-21T15:27:25Z,2023-04-22T06:21:33+00:00,3,14.9
1106,RMSE-optimized quants for all quantization types,ikawrakow,2023-04-21T15:29:58Z,2023-05-03T20:25:20+00:00,2,292.92
1107,Improve Alpaca Default Repeat Penalty: Better Match Alpaca.cpp Experience,HanClinto,2023-04-21T16:52:34Z,2023-04-22T06:54:33+00:00,1,14.03
1109,ggml : alternative Q4_3 implementation using modified Q8_0,ggerganov,2023-04-21T17:56:41Z,2023-04-22T07:55:36+00:00,1,13.98
1119,A better `packNibbles` and `mul_sum_i8_pairs_float` implementation using AVX512,MeouSker77,2023-04-22T09:41:37Z,2023-04-23T07:57:06+00:00,2,22.26
1123,Handle signals properly on Windows,DannyDaemonic,2023-04-22T11:31:53Z,2023-05-03T01:01:58+00:00,1,253.5
1126,"Sample interface, new samplers,",ivanstepanovftw,2023-04-22T12:36:01Z,2023-04-29T05:34:41+00:00,6,160.98
1128,Fix cuda compilation,slaren,2023-04-22T14:25:30Z,2023-04-24T15:29:58+00:00,1,49.07
1131,Example readme and some light refactoring,mgroeber9110,2023-04-22T18:45:05Z,2023-04-24T15:45:33+00:00,4,45.01
1139,Added README.md for main with examples and explanations,DannyDaemonic,2023-04-23T10:06:03Z,2023-04-23T15:37:02+00:00,8,5.52
1143,llama : refactor get / set state + remove redundant kv cache API,ggerganov,2023-04-23T15:57:53Z,2023-04-24T04:40:02+00:00,1,12.7
1145,Fix meaning of LoRA acronym in main README,slaren,2023-04-23T19:34:28Z,2023-04-23T21:03:45+00:00,1,1.49
1150,add save_load_state example,xaedes,2023-04-24T03:38:01Z,2023-04-24T16:23:31+00:00,1,12.76
1162,Implement scalar sum over all rows in ggml_compute_forward_sum_f32,xaedes,2023-04-24T19:41:30Z,2023-04-24T21:02:02+00:00,1,1.34
1164,CLBlast support,0cc4m,2023-04-24T20:17:44Z,2023-04-28T14:57:17+00:00,27,90.66
1168,Jeopardy Example Script,CRD716,2023-04-25T03:00:02Z,2023-04-28T16:13:33+00:00,1,85.23
1169,Save and restore prompt evaluation state for much faster startup times,ejones,2023-04-25T03:09:44Z,2023-04-28T15:59:37+00:00,1,84.83
1170,force int cast,ostix360,2023-04-25T07:21:47Z,2023-04-25T21:33:08+00:00,7,14.19
1173,Process escape sequences given in prompts,DannyDaemonic,2023-04-25T14:47:37Z,2023-05-03T01:46:20+00:00,1,178.98
1179,ggml : add Q8_0 quantization format (rename the old one to Q8_1) (ARM NEON),ggerganov,2023-04-25T18:45:36Z,2023-04-25T20:40:51+00:00,5,1.92
1181,Update SHA256SUMS after quantization change,sw,2023-04-25T19:10:29Z,2023-04-25T21:41:56+00:00,1,2.52
1183,Updating build instructions to include BLAS support,daniandtheweb,2023-04-25T22:24:12Z,2023-04-26T20:03:04+00:00,12,21.65
1184,Allow setting the rng seed after initialization.,asgeir,2023-04-25T23:24:13Z,2023-04-26T20:08:43+00:00,5,20.74
1187,ggml : add Q5_0 and Q5_1 quantization,ggerganov,2023-04-26T07:39:03Z,2023-04-26T20:14:13+00:00,2,12.59
1196,read chat prompts from a template file,khimaros,2023-04-26T19:12:27Z,2023-05-03T17:58:11+00:00,2,166.76
1198,Getting started documentation,TheNotary,2023-04-26T21:26:33Z,2025-08-22T13:44:28+00:00,11,20368.3
1203,platform independent script to verify sha256 checksums,KASR,2023-04-27T07:59:42Z,2023-05-03T15:31:29+00:00,3,151.53
1207,cuBLAS: use host pinned memory and dequantize while copying,slaren,2023-04-27T20:10:32Z,2023-04-29T00:04:18+00:00,1,27.9
1211,"add avx2 for dot_q8_0_q8_0, 2x faster than scalar",YannFollet,2023-04-28T01:34:32Z,2023-04-28T11:59:48+00:00,1,10.42
1212,Add Manjaro CUDA include and lib dirs to Makefile,JohannesGaessler,2023-04-28T06:38:50Z,2023-04-28T13:40:33+00:00,1,7.03
1215,cuBLAS: non-contiguous tensor support,SlyEcho,2023-04-28T14:44:48Z,2023-04-28T23:31:57+00:00,4,8.79
1218,Remove Q4_3 which is no better than Q5,sw,2023-04-28T17:49:05Z,2023-04-28T23:10:43+00:00,2,5.36
1225,"CLBlast: q5_0, q5_1, q8_0 dequant kernels",0cc4m,2023-04-29T08:59:29Z,2023-04-30T18:34:52+00:00,4,33.59
1226,Adjust mul_mat_f16 work memory,ggerganov,2023-04-29T09:45:07Z,2023-04-29T15:43:28+00:00,1,5.97
1229,ggml : fix #if for f32_f32 mul_mat (CLBlast),ggerganov,2023-04-29T11:47:20Z,2023-04-29T15:43:42+00:00,1,3.94
1232,Add git-based build information for better issue tracking,DannyDaemonic,2023-04-29T13:05:47Z,2023-05-01T16:23:47+00:00,10,51.3
1233,cuBLAS: fall back to pageable memory if pinned alloc fails,slaren,2023-04-29T16:27:34Z,2023-05-01T11:32:23+00:00,1,43.08
1237,Generalize `quantize_fns` for simpler FP16 handling,sw,2023-04-29T18:13:58Z,2023-07-05T16:13:06+00:00,5,1605.99
1251,"build: add armv{6,7,8} support to cmake",prusnak,2023-04-30T09:13:57Z,2023-04-30T18:48:38+00:00,2,9.58
1253,Various fixes to mat_mul benchmark,sw,2023-04-30T10:23:58Z,2023-04-30T12:32:37+00:00,8,2.14
1259,cuBLAS: refactor and optimize f16 mat mul performance,slaren,2023-04-30T22:06:50Z,2023-05-01T16:11:08+00:00,8,18.07
1260,"Hotfix prompt caching introduced in #1169, fixes #1257",ivanstepanovftw,2023-05-01T00:35:39Z,2023-05-01T10:16:55+00:00,1,9.69
1261,llama : let context be const when accessing const data,grencez,2023-05-01T03:19:33Z,2023-05-01T07:24:20+00:00,1,4.08
1263,llama : fix session load / save,ggerganov,2023-05-01T08:08:54Z,2023-05-01T11:55:00+00:00,2,3.77
1264,ggml_used_mem can segfault if called before any objects are created.,KerfuffleV2,2023-05-01T10:24:48Z,2023-05-01T11:56:07+00:00,1,1.52
1266,update stubs for systems without mmap and mlock,xloem,2023-05-01T11:46:43Z,2023-05-01T12:58:51+00:00,1,1.2
1268,ggml: add names to tensors,slaren,2023-05-01T15:22:00Z,2023-05-02T14:03:00+00:00,1,22.68
1271,ci : add cublas to windows release,Green-Sky,2023-05-02T00:10:02Z,2023-05-05T20:56:09+00:00,1,92.77
1272,llama : only copy used KV cache in get / set state,ejones,2023-05-02T04:05:57Z,2023-05-03T02:26:14+00:00,2,22.34
1275,Adding the ability to have zero '0' as a seed number.,rbrisita,2023-05-02T08:33:14Z,2023-05-02T16:23:44+00:00,1,7.84
1276,Improve ability to convert safetensors files.,ubik2,2023-05-02T09:50:14Z,2023-05-08T11:54:26+00:00,5,146.07
1277,CI: add Windows CLBlast and OpenBLAS builds,SlyEcho,2023-05-02T11:50:11Z,2023-05-07T11:20:09+00:00,9,119.5
1289,fix build-info.h for git submodules,kuvaus,2023-05-02T19:41:38Z,2023-05-03T00:43:43+00:00,4,5.03
1296,Update main's README.md with new features,DannyDaemonic,2023-05-03T11:17:39Z,2023-05-04T10:03:00+00:00,2,22.76
1297,fix #1224 reverse prompt and multi line,newTomas,2023-05-03T11:49:48Z,2023-05-04T10:02:31+00:00,1,22.21
1298,Various Prompt and Example Fixes,CRD716,2023-05-03T13:44:46Z,2023-05-03T15:26:47+00:00,1,1.7
1301,quantize: make output filename optional,slaren,2023-05-03T17:08:44Z,2023-05-04T22:58:56+00:00,1,29.84
1303,llama : require first token to be BOS,ggerganov,2023-05-03T17:26:44Z,2023-05-08T14:41:55+00:00,2,117.25
1304,Model agnostic DAN prompt,CRD716,2023-05-03T20:10:51Z,2023-05-11T15:10:19+00:00,1,186.99
1308,Steps to support the Dolly model,devkral,2023-05-03T22:09:42Z,2023-05-04T15:47:52+00:00,1,17.64
1309,Convert: Support DT_BF16 tensors,ivanstepanovftw,2023-05-04T01:12:02Z,2023-05-04T16:54:37+00:00,3,15.71
1310,"llama, main : save state incrementally",ejones,2023-05-04T05:24:47Z,2023-05-06T03:29:22+00:00,4,46.08
1311,Only escape prompts when used with `-e`,DannyDaemonic,2023-05-04T06:25:54Z,2023-05-04T12:08:26+00:00,12,5.71
1314,use pause asm insn in busyloop to run the CPU (13600K) 10 °C cooler,Safari77,2023-05-04T10:58:59Z,2023-05-09T12:29:20+00:00,8,121.51
1316,Wrap exceptions in std::exception to verbose output on exception.,ivanstepanovftw,2023-05-04T12:13:37Z,2023-05-04T16:56:27+00:00,1,4.71
1318,Adding --in-suffix option,44670,2023-05-04T13:14:54Z,2023-05-04T15:41:12+00:00,1,2.44
1324,readme: add missing info,prusnak,2023-05-04T19:00:47Z,2023-05-05T14:43:37+00:00,2,19.71
1325,First rough draft of recoverable errors feature.,KerfuffleV2,2023-05-04T20:53:00Z,2023-05-25T06:13:47+00:00,4,489.35
1327,Convert.py @staticmethod,blecaillon,2023-05-04T21:50:45Z,2023-05-05T00:17:07+00:00,2,2.44
1329,Fix for OpenCL / clbast builds on macOS.,IonoclastBrigham,2023-05-04T23:10:49Z,2023-05-05T12:18:21+00:00,1,13.13
1332,Automatic Arch Linux detection,daniandtheweb,2023-05-05T16:37:35Z,2023-05-05T21:57:14+00:00,6,5.33
1336,ggml : Allow usage of CLBlast alongside Accelerate.framework,swittk,2023-05-06T01:25:04Z,2023-05-07T03:03:24+00:00,1,25.64
1338,main : add option to save full output to session,ejones,2023-05-06T03:28:18Z,2023-05-10T15:37:15+00:00,7,108.15
1341,More GPU threads for dequantization,JohannesGaessler,2023-05-06T10:26:54Z,2023-05-13T14:58:08+00:00,1,172.52
1343,Remove default arguments from sampling functions,j-f1,2023-05-06T13:27:21Z,2023-05-06T21:01:47+00:00,1,7.57
1346,"Documented CUDA reproducibility, added warning",JohannesGaessler,2023-05-06T19:01:51Z,2023-05-08T00:42:01+00:00,1,29.67
1359,Added Table of Content and Pygmalion to README,AlpinDale,2023-05-07T19:26:55Z,2023-05-08T16:33:31+00:00,1,21.11
1360,Implement backward passes for llama with small training llama from scratch example,xaedes,2023-05-07T21:36:41Z,2023-05-13T12:56:41+00:00,8,135.33
1366,fix typo in default model path,ott2,2023-05-08T12:09:23Z,2023-05-16T15:46:35+00:00,5,195.62
1367,llama: fix shadowing,prusnak,2023-05-08T13:45:29Z,2023-05-08T14:48:21+00:00,1,1.05
1375,Proof of concept: GPU-accelerated token generation,JohannesGaessler,2023-05-09T12:30:39Z,2023-05-13T14:51:15+00:00,30,98.34
1405,ggml : remove bit shuffling,ggerganov,2023-05-11T16:48:14Z,2023-05-11T21:23:08+00:00,4,4.58
1407,Add clang-tidy reviews to CI,slaren,2023-05-11T20:57:39Z,2023-05-12T13:40:54+00:00,1,16.72
1409,Add the dotnet binding info.,SanftMonster,2023-05-12T03:58:24Z,2023-05-12T05:39:40+00:00,1,1.69
1412,GPU-accelerated token generation (new quantization format),JohannesGaessler,2023-05-12T09:49:18Z,2023-05-13T13:38:36+00:00,15,27.82
1413,Adding SSE instructions to ggml_vec_dot_q4_0_q8_0,3ooabkhxtn,2023-05-12T09:58:54Z,2023-05-13T08:43:33+00:00,3,22.74
1416,"CLI args use - instead of _, backwards compatible",JohannesGaessler,2023-05-12T13:27:23Z,2023-05-12T14:34:55+00:00,3,1.13
1422,Fix OpenCL kernels for the new formats,SlyEcho,2023-05-12T22:04:56Z,2023-05-13T06:01:15+00:00,3,7.94
1428,ggml : multi-thread mul and diag_mask ops,ggerganov,2023-05-13T08:30:12Z,2023-05-13T13:48:04+00:00,1,5.3
1430,ggml : add AVX support based on AVX2 code,katsu560,2023-05-13T13:47:55Z,2023-05-14T10:03:52+00:00,2,20.27
1435,OpenCL: Fixes for older devices.,SlyEcho,2023-05-13T19:13:23Z,2023-05-20T14:57:40+00:00,20,163.74
1436,fix get_num_physical_cores(),zrm,2023-05-13T19:21:56Z,2023-05-15T02:25:43+00:00,7,31.06
1443,Server example with API Rest,FSSRepo,2023-05-14T02:55:32Z,2023-05-21T17:51:19+00:00,22,182.93
1453,Deduplicated dequantization code,JohannesGaessler,2023-05-14T15:08:28Z,2023-05-14T18:53:24+00:00,1,3.75
1458,"benchmark-matmul: fix clang-tidy issues, report results in GFLOPS",slaren,2023-05-14T19:43:00Z,2023-05-14T20:46:01+00:00,1,1.05
1459,OpenCL dequant_mul_mat,0cc4m,2023-05-14T20:35:08Z,2023-05-22T21:33:24+00:00,15,192.97
1461,docker: add support for CUDA in docker,canardleteer,2023-05-15T00:37:51Z,2023-07-07T18:25:25+00:00,13,1289.79
1462,Fix for mingw compilers (including wx64devkit) - fixes #1423 fixes #1529,DannyDaemonic,2023-05-15T05:22:14Z,2023-05-20T07:40:02+00:00,1,122.3
1466,cuBLAS doc + error if -ngl > 0 and no cuBLAS,JohannesGaessler,2023-05-15T09:33:50Z,2023-07-31T12:34:02+00:00,3,1851.0
1469,convert.py: Support models which are stored in a single pytorch_model.bin,TheBloke,2023-05-15T21:18:00Z,2023-05-16T22:04:35+00:00,1,24.78
1476,Add alternate include path for openblas,sandyiscool,2023-05-16T07:15:37Z,2023-05-16T08:30:15+00:00,1,1.24
1477,~7% faster Q5_1 AVX2 code,ilyakurdyukov,2023-05-16T10:19:33Z,2023-05-16T18:36:47+00:00,1,8.29
1482,ggml: add map_ternary_f32(),cztomsik,2023-05-16T15:52:37Z,2023-07-01T18:47:09+00:00,3,1106.91
1483,"Loading models directly into VRAM, norm calculation on GPUs, broadcasting for ggml_mul",JohannesGaessler,2023-05-16T15:54:15Z,2023-05-20T12:19:29+00:00,14,92.42
1484,Readme: add akx/ggify to tools,akx,2023-05-16T16:19:59Z,2024-05-26T12:09:42+00:00,1,9019.83
1485,Adds WizardLM to the list of supported models to README.md,dakennedyd,2023-05-16T17:57:55Z,2023-05-19T17:16:30+00:00,2,71.31
1486,benchmark-matmul: Print the average of the test results,rankaiyx,2023-05-16T18:30:37Z,2023-05-17T01:25:51+00:00,1,6.92
1490,benchmark-matmul: Print the average of the test results,rankaiyx,2023-05-17T01:43:38Z,2023-05-17T14:47:58+00:00,1,13.07
1495,examples : add persistent chat,ejones,2023-05-17T03:40:59Z,2023-05-19T17:39:51+00:00,1,61.98
1502,feature: add blis and other BLAS implementation support,zenixls2,2023-05-17T10:42:45Z,2023-05-20T09:02:48+00:00,5,70.33
1509,Remove unused n_parts parameter,sw,2023-05-17T18:49:30Z,2023-05-17T22:12:02+00:00,1,3.38
1513,Fixes #1511 lambda issue for w64devkit (mingw),DannyDaemonic,2023-05-18T11:42:35Z,2023-05-18T17:30:41+00:00,5,5.8
1517,make kv_f16 the default for api users,Green-Sky,2023-05-18T13:21:52Z,2023-05-18T17:31:01+00:00,1,4.15
1520,Define magic numbers as integer constants (#1518),imaami,2023-05-18T15:19:36Z,2023-05-20T12:58:15+00:00,2,45.64
1526,Fix name shadowing and C4146,maximegmd,2023-05-19T15:37:04Z,2023-05-20T07:22:37+00:00,2,15.76
1527,llama: initialize f16 tables in quantize c api.,SanftMonster,2023-05-19T16:37:34Z,2023-05-20T08:06:46+00:00,1,15.49
1528,Fix bug in main.cpp (penalize_nl=false doesn't work). Supress warning on mingw.,tom7,2023-05-19T17:00:06Z,2023-08-26T18:12:56+00:00,5,2377.21
1530,CUDA performance optimizations,JohannesGaessler,2023-05-19T19:42:49Z,2023-05-25T21:07:29+00:00,12,145.41
1531,A simple script merge lora to HF,FNsi,2023-05-20T00:32:31Z,2023-07-10T23:23:19+00:00,7,1246.85
1536,Feature: support blis and other blas implementation ,zenixls2,2023-05-20T09:32:16Z,2023-05-20T14:58:31+00:00,1,5.44
1550,Some improvements to loading the session with --prompt-cache,KerfuffleV2,2023-05-21T11:33:26Z,2023-05-26T02:18:01+00:00,10,110.74
1554,examples : fix benchmark-matmult,ggerganov,2023-05-21T14:02:25Z,2023-09-20T07:02:39+00:00,1,2921.0
1556,Numa,zrm,2023-05-21T21:18:44Z,2023-06-26T17:57:59+00:00,10,860.65
1558,Add `--simple-io option` for subprocesses and separate console to it's own header and cpp,DannyDaemonic,2023-05-22T06:22:40Z,2023-08-04T15:20:12+00:00,4,1784.96
1564,chat-persistent.sh : GNU Grep compatibility,Senemu,2023-05-22T21:19:46Z,2023-05-24T06:16:23+00:00,1,32.94
1565,"Fix handling of ""invalid property"" when creating OpenCL command queue",mthuurne,2023-05-22T22:21:41Z,2023-05-23T16:01:16+00:00,1,17.66
1568,readme : add docs for chat-persistent.sh,ejones,2023-05-23T03:47:21Z,2023-05-24T06:24:01+00:00,1,26.61
1570,Server Example Refactor and Improvements,digiwombat,2023-05-23T13:29:12Z,2023-06-17T11:53:05+00:00,91,598.4
1579,Change CMake files to library target,SlyEcho,2023-05-23T22:21:41Z,2023-08-29T16:58:41+00:00,1,2346.62
1580,Update CLBlast to 1.6.0,SlyEcho,2023-05-23T22:42:14Z,2023-05-24T07:30:10+00:00,1,8.8
1584,fix some unicode prob,CRGBS,2023-05-24T08:39:05Z,2023-05-25T01:30:59+00:00,1,16.86
1588,OpenLLaMA 3B support,SlyEcho,2023-05-24T18:37:54Z,2023-05-30T18:24:22+00:00,6,143.77
1594,Fixed WSL cuda's OOM error,JoelSeniorLiang,2023-05-25T15:31:20Z,2023-06-11T13:20:53+00:00,8,405.83
1597,Leverage mmap to offloading the tensors,howard0su,2023-05-26T06:52:13Z,2023-06-12T12:44:16+00:00,24,413.87
1598,convert.py: add mapping for safetensors bf16,akx,2023-05-26T10:04:34Z,2023-07-07T13:12:50+00:00,1,1011.14
1599,Consistently catch and throw only exceptions deriving from std::exception,mgroeber9110,2023-05-26T10:45:33Z,2023-06-05T20:24:29+00:00,2,249.65
1604,Add documentation about CLBlast,SlyEcho,2023-05-26T21:34:21Z,2023-05-27T15:47:55+00:00,1,18.23
1606,[CI] CLBlast: Fix directory name,SlyEcho,2023-05-26T23:03:47Z,2023-05-27T12:18:25+00:00,1,13.24
1607,"Cuda refactor, multi GPU support",JohannesGaessler,2023-05-27T09:28:34Z,2023-06-06T19:34:03+00:00,41,250.09
1609,Work around for recalculating logits in cached prompts (Fixes #1585),DannyDaemonic,2023-05-27T10:30:30Z,2023-05-29T12:13:40+00:00,5,49.72
1610,Include server in releases + other build system cleanups,KerfuffleV2,2023-05-27T11:15:45Z,2023-05-27T17:04:15+00:00,8,5.81
1611,Use strstr to check if fp16 supported,howard0su,2023-05-27T12:14:25Z,2023-05-28T17:09:56+00:00,1,28.93
1612,No need to allocate cl_mem on heap,howard0su,2023-05-27T12:19:17Z,2023-05-28T17:13:37+00:00,1,28.91
1614,Add --alias option to gpt_params to set use friendly model name,epicfilemcnulty,2023-05-27T14:30:28Z,2023-05-28T17:14:24+00:00,1,26.73
1616,ADD Support for the RISCV Architecture,apcameron,2023-05-27T14:39:02Z,2023-05-27T20:03:25+00:00,1,5.41
1617,LLAMA_DEBUG adds debug symbols,JohannesGaessler,2023-05-27T19:31:45Z,2023-05-28T19:01:02+00:00,1,23.49
1621,Adding git in container package dependencies,jpodivin,2023-05-28T07:41:28Z,2023-05-29T04:45:50+00:00,1,21.07
1625,Only show -ngl option when relevant + other doc/arg handling updates,KerfuffleV2,2023-05-28T10:13:58Z,2023-05-28T17:48:57+00:00,1,7.58
1632,"Fine tune MUL_MAT, new threading (spin+wait/notify), speedup q_f32 BLAS by splitting COMPUTE stage",mqy,2023-05-29T00:55:42Z,2023-06-26T22:01:57+00:00,22,693.1
1638,fix(avx): workaround for missing _mm256_set_m128i in GCC < 8,xingchensong,2023-05-29T13:01:22Z,2023-06-10T07:49:40+00:00,1,282.81
1640,main: add the possibility to open the prompt cache read-only,wtarreau,2023-05-29T15:57:27Z,2023-06-07T02:10:17+00:00,1,202.21
1641,Replacing call to `convert-pth-to-ggml.py` with `convert.py`,jpodivin,2023-05-29T17:07:27Z,2023-06-03T12:11:53+00:00,1,115.07
1642,llama : Metal inference,ggerganov,2023-05-29T17:53:41Z,2023-06-04T20:34:30+00:00,12,146.68
1652,Train Text from scratch,xaedes,2023-05-30T16:16:18Z,2023-06-13T19:04:43+00:00,9,338.81
1653,"OpenCL: Fix duplication of layers in VRAM and RAM, add GPU mul kernel",0cc4m,2023-05-30T17:12:37Z,2023-06-04T06:12:07+00:00,9,108.99
1659,Update Makefile to add SSSE3 compilation use cases,rankaiyx,2023-06-01T00:48:54Z,2023-06-10T06:41:59+00:00,1,221.88
1666,[WIP] Add normalfloat4 as Q4_2,howard0su,2023-06-01T14:19:49Z,2023-06-09T14:12:03+00:00,2,191.87
1673,Add llama.cpp docker support for non-latin languages,qingfengfenga,2023-06-02T09:44:20Z,2023-06-08T07:58:54+00:00,6,142.24
1674,docs(performance): Add performance troubleshoot + example benchmark documentation,Yuval-Peled,2023-06-02T12:32:54Z,2023-06-05T20:32:36+00:00,1,80.0
1675,Clblast fixes + enhancements to save VRAM and offload more layers,LostRuins,2023-06-02T15:27:56Z,2023-06-06T17:00:02+00:00,5,97.53
1678,Fix prompt cache saving and chat-persistent rollover,ejones,2023-06-03T05:13:36Z,2023-06-03T11:28:45+00:00,2,6.25
1681,Pre-commit hooks,jpodivin,2023-06-03T12:26:01Z,2023-06-17T10:32:48+00:00,1,334.11
1684,k-quants,ikawrakow,2023-06-03T15:24:31Z,2023-06-05T19:56:19+00:00,6,52.53
1691,Support requantizing models instead of only allowing quantization from 16/32bit,KerfuffleV2,2023-06-04T17:45:31Z,2023-06-10T07:59:17+00:00,1,134.23
1692,Added tensor layer numbers,dkun7944,2023-06-04T19:07:27Z,2024-05-10T13:25:19+00:00,2,8178.3
1696,Share buffers between CPU and GPU,kiltyj,2023-06-05T04:39:04Z,2023-06-05T20:24:04+00:00,1,15.75
1698,Increase 3B scratch buffers.,SlyEcho,2023-06-05T09:09:18Z,2023-06-05T10:43:09+00:00,2,1.56
1701,Arch Makefile update,daniandtheweb,2023-06-05T14:00:32Z,2023-06-17T16:17:23+00:00,1,290.28
1702,ggml: Fix internal overflow in ggml_time_us on Windows,grahameth,2023-06-05T15:02:10Z,2023-06-05T20:11:50+00:00,1,5.16
1703,"Multi GPU support, CUDA refactor, CUDA scratch buffer",JohannesGaessler,2023-06-05T18:43:28Z,2023-06-06T19:33:23+00:00,10,24.83
1706,Add checks for buffer size with Metal,spencersutton,2023-06-05T21:37:57Z,2023-06-06T03:28:18+00:00,1,5.84
1720,Fix warning on fprintf,sroussey,2023-06-06T20:00:37Z,2023-06-08T07:12:29+00:00,4,35.2
1724,update flake to support metal on m1/m2,jpetrucciani,2023-06-06T22:55:01Z,2023-06-07T04:15:31+00:00,1,5.34
1733,Q4_K implementation for Metal,ikawrakow,2023-06-07T07:02:12Z,2023-06-08T07:08:23+00:00,1,24.1
1738,ggml : Load data into int8x16x4_t using vld4q_s8 on arm64,lindeer,2023-06-07T08:40:17Z,2023-06-08T16:47:57+00:00,1,32.13
1741,OpenCL: Add release memory,edp1096,2023-06-07T13:56:41Z,2023-06-09T16:24:41+00:00,2,50.47
1748,Add missing compile definition to CMakeLists for k_quants,johnson442,2023-06-08T03:02:26Z,2023-06-08T07:02:49+00:00,2,4.01
1752,Q6_K implementation for Metal,ikawrakow,2023-06-08T08:11:07Z,2023-06-08T16:46:22+00:00,1,8.59
1753,Windows nvcc workaround,JohannesGaessler,2023-06-08T08:57:21Z,2023-06-09T11:58:16+00:00,1,27.02
1757,Updated CMakeLists.txt slightly to allow building this project in Android Studio with the latest Android NDK,l3utterfly,2023-06-08T12:46:56Z,2023-06-16T03:17:28+00:00,3,182.51
1762,metal : add Q2_K implementation,ikawrakow,2023-06-08T15:12:24Z,2023-06-08T19:28:22+00:00,1,4.27
1770,Add gelu implementation to metal.,manyoso,2023-06-09T03:31:17Z,2023-06-09T08:00:52+00:00,1,4.49
1772,doc: fix the wrong address of BLIS.md,Aisuko,2023-06-09T04:29:35Z,2023-06-10T14:08:11+00:00,1,33.64
1773,llama : add grammar-based sampling,ejones,2023-06-09T05:08:03Z,2023-07-24T03:58:11+00:00,17,1078.84
1774,add a grpc server for embedding and completion,extrame,2023-06-09T05:37:24Z,2023-10-17T19:30:03+00:00,16,3133.88
1782,Fix issue with ggml-metal.metal path. Closes #1769,abetlen,2023-06-09T16:23:19Z,2023-06-10T14:47:35+00:00,1,22.4
1785,metal : add Q4_1 implementation,ikawrakow,2023-06-10T06:50:42Z,2023-06-10T08:28:11+00:00,1,1.62
1787,"Allow ""quantizing"" to f16 and f32",KerfuffleV2,2023-06-10T11:07:49Z,2023-06-13T10:23:23+00:00,8,71.26
1789,Fix issue where interactive mode crashes when input exceeds ctx size,KerfuffleV2,2023-06-10T13:50:17Z,2023-06-11T14:19:17+00:00,8,24.48
1791,Fix cmake error when using LLAMA_METAL,spencersutton,2023-06-10T15:25:46Z,2023-06-10T19:57:22+00:00,2,4.53
1792,[#1783] GCC12 compilation fix.,vagran,2023-06-10T17:39:45Z,2023-06-10T19:51:36+00:00,1,2.2
1797,Make model stateless and context stateful (llama_state),didzis,2023-06-11T02:16:07Z,2023-06-24T08:47:58+00:00,6,318.53
1798,Update SHA256SUMS with current hashes for models quantized using q4_0,rlanday,2023-06-11T03:05:45Z,2023-06-11T09:38:53+00:00,1,6.55
1800,Fix CI build when changing only the CUDA sources,slaren,2023-06-11T13:29:59Z,2023-06-12T17:12:47+00:00,2,27.71
1807,Metal implementation for all k_quants,ikawrakow,2023-06-11T20:41:33Z,2023-06-12T19:39:21+00:00,1,22.96
1817,metal : fix failure to load model,ikawrakow,2023-06-12T10:11:16Z,2023-06-12T11:31:36+00:00,1,1.34
1821,Update baby-llama.cpp,0xspringtime,2023-06-12T13:58:24Z,2023-06-13T19:37:55+00:00,2,29.66
1823,"add metal kernels for norm, alibi, f16->f16 copy",apage43,2023-06-12T17:42:11Z,2023-06-17T14:37:49+00:00,4,116.93
1825,Draft: Metal max buffer workaround,kiltyj,2023-06-12T18:47:56Z,2023-06-18T06:12:54+00:00,1,131.42
1827,"CUDA full GPU acceleration, KV cache in VRAM",JohannesGaessler,2023-06-12T19:35:30Z,2023-06-14T17:47:20+00:00,6,46.2
1828,"docs - Alternative way to build at Android, with CLBlast.",gustrd,2023-06-13T01:45:35Z,2023-06-17T09:01:06+00:00,1,103.26
1830,Set include path for OpenBlas,okigan,2023-06-13T04:27:16Z,2023-06-15T17:51:27+00:00,1,61.4
1831,swift Package compile breaks due to ggml-metal.metal,Schaltfehler,2023-06-13T05:46:16Z,2023-06-15T17:47:04+00:00,1,60.01
1836,K-Quant-Support for OpenCL,0cc4m,2023-06-13T14:10:43Z,2023-06-16T18:59:50+00:00,2,76.82
1840,Minimalist example,SuperUserNameMan,2023-06-13T17:15:55Z,2023-06-16T18:58:09+00:00,1,73.7
1850,make training example accessible,daboe01,2023-06-14T10:26:24Z,2023-06-15T17:42:48+00:00,1,31.27
1854,Add an example script that works with the Vicuna model. Adding a small bit of documenting comment in llama.h.,yangli2,2023-06-14T14:15:49Z,2023-06-15T18:05:54+00:00,1,27.83
1857,Update Makefile to clean *.so files too.,sandyiscool,2023-06-14T16:13:33Z,2023-06-15T17:36:06+00:00,2,25.38
1860,metal : parallel command buffer encoding,ggerganov,2023-06-14T18:09:49Z,2023-06-15T17:29:48+00:00,1,23.33
1861,Better error when using both LoRA + GPU layers,JohannesGaessler,2023-06-14T19:25:44Z,2023-06-15T17:06:47+00:00,1,21.68
1862,CUDA : faster k-quant dot kernels,ikawrakow,2023-06-14T19:56:19Z,2023-06-16T17:08:44+00:00,4,45.21
1863,expose build outputs via apps so that they can be invoked by nix run,faezs,2023-06-14T20:58:38Z,2023-06-17T12:13:05+00:00,1,63.24
1872,Fix the validation of main device,howard0su,2023-06-15T13:34:29Z,2023-06-15T17:29:59+00:00,1,3.92
1874,Update README.md,nivibilla,2023-06-15T13:38:45Z,2023-06-15T17:36:38+00:00,1,3.96
1879,Fixed CUDA runtime version check,JohannesGaessler,2023-06-15T17:58:51Z,2023-06-15T19:49:09+00:00,1,1.84
1886,fix: add auto detection on the BLAS_INCLUDE_DIRS,zenixls2,2023-06-16T04:21:21Z,2023-06-16T18:53:04+00:00,1,14.53
1887,examples : generate JSON according to schema,ejones,2023-06-16T05:31:46Z,2023-08-03T02:05:44+00:00,1,1148.57
1889,build : fix and ignore MSVC warnings,iboB,2023-06-16T07:22:51Z,2023-06-16T18:23:54+00:00,1,11.02
1891,Fixed embeddings when offloading non-repeating layers,JohannesGaessler,2023-06-16T13:40:36Z,2023-06-16T18:25:52+00:00,1,4.75
1892,Fixed possible macro redefinition,FrankHB,2023-06-16T14:20:10Z,2023-06-16T18:25:02+00:00,1,4.08
1893,Fixed a mismatch format,FrankHB,2023-06-16T14:22:54Z,2023-06-16T18:24:38+00:00,1,4.03
1896,Allow cmake to build ggml as a library,KerfuffleV2,2023-06-16T15:22:02Z,2023-06-17T07:49:42+00:00,1,16.46
1898,CUDA performance optimization: asynchronous computation by using only one cudaStream,JohannesGaessler,2023-06-16T18:40:47Z,2023-06-17T17:15:02+00:00,2,22.57
1902,Pass pointer to params in llama_init_from_file,mudler,2023-06-16T21:49:01Z,2023-06-19T16:41:09+00:00,1,66.87
1903,Fix uninitialized var causing crash on Windows using MSVC.,dranger003,2023-06-16T23:20:16Z,2023-06-17T16:31:28+00:00,1,17.19
1904,Code refactor and optimize using reserve,GermanAizek,2023-06-16T23:36:27Z,2024-02-16T14:00:51+00:00,3,5870.41
1905,Get raw text instead of page with html,Davidy22,2023-06-17T00:15:16Z,2023-06-17T06:51:54+00:00,1,6.61
1907,#1869 Fix null reference errors when training from scratch with CUDA,robyngraf,2023-06-17T06:01:41Z,2023-06-24T18:10:29+00:00,3,180.15
1908,Fix warnings under MSVC,howard0su,2023-06-17T11:38:45Z,2023-06-17T15:46:15+00:00,1,4.12
1910,Support input float tensor directly  ,ningshanwutuobang,2023-06-17T13:49:29Z,2023-06-28T15:53:37+00:00,5,266.07
1913,Convert vector to f16 for dequantize mul mat vec,JohannesGaessler,2023-06-17T18:06:41Z,2023-06-19T08:23:56+00:00,2,38.29
1916,k_quants : add AVX support to dot functions,katsu560,2023-06-18T00:54:24Z,2023-06-26T16:46:08+00:00,6,207.86
1917,Add CUDA_ARCHITECTURES to new target ggml_static,howard0su,2023-06-18T01:52:42Z,2023-06-18T04:29:47+00:00,2,2.62
1918,Could this be a typo?,l3utterfly,2023-06-18T05:34:19Z,2023-06-18T11:19:17+00:00,2,5.75
1924,ggml : sync latest ggml repo,ggerganov,2023-06-18T12:15:16Z,2023-06-19T15:12:34+00:00,8,26.95
1925,Fixed incorrectly applying RMS norm twice,JohannesGaessler,2023-06-18T12:38:02Z,2023-06-18T14:07:10+00:00,2,1.49
1929,Fix build shared ggml when CUDA is enabled,howard0su,2023-06-18T15:44:14Z,2023-06-19T15:10:37+00:00,1,23.44
1930,Faster k-quants on older GPUs,ikawrakow,2023-06-18T20:23:33Z,2023-06-19T15:14:09+00:00,1,18.84
1931,Improve support for special tokens,Igoorx,2023-06-19T00:06:07Z,2023-10-12T00:12:28+00:00,1,2760.11
1932,Only use Q6_K for output weights if tensor size is multiple of 256,ikawrakow,2023-06-19T05:43:14Z,2023-06-19T15:17:04+00:00,8,9.56
1934,memory is not guaranteed to be aligned properly during ggml_init call from loading saved sessions,l3utterfly,2023-06-19T06:52:56Z,2023-06-19T15:20:06+00:00,4,8.45
1935,VRAM optimization + matrix multiplication discussion,JohannesGaessler,2023-06-19T12:42:31Z,2023-07-31T12:35:26+00:00,1,1007.88
1936,Workaround struct misalignment during value-copy,mudler,2023-06-19T12:50:33Z,2023-06-20T01:24:40+00:00,5,12.57
1937,[Fix] Reenable server embedding endpoint,SlyEcho,2023-06-19T13:10:12Z,2023-06-19T22:12:39+00:00,1,9.04
1946,Update README.md,howard0su,2023-06-20T02:03:10Z,2023-06-21T14:38:07+00:00,1,36.58
1949,Fix typo in CLBlast build ,sammysun0711,2023-06-20T08:26:44Z,2023-06-20T12:42:41+00:00,2,4.27
1953,Fix top-p sampling to match the canonical definition,alexrenda,2023-06-20T18:45:29Z,2023-06-24T10:15:02+00:00,1,87.49
1954,Add OpenLLaMA instructions to the README,unknown,2023-06-20T19:49:22Z,2023-06-23T08:38:02+00:00,3,60.81
1958,rework convert.py to read params from config to allow open_llama 3B,Green-Sky,2023-06-21T13:19:49Z,2023-06-22T12:20:48+00:00,1,23.02
1959,"cmake: revert CUDA arch default to 52, 61 if f16",JohannesGaessler,2023-06-21T13:46:50Z,2023-06-21T21:49:25+00:00,2,8.04
1961,Fix typo in README.md,RahulVivekNair,2023-06-21T18:28:36Z,2023-06-21T21:48:43+00:00,1,3.34
1962,server: add option to output probabilities for completion,WangHaoranRobin,2023-06-21T21:35:32Z,2023-07-02T21:38:44+00:00,21,264.05
1966,Porting the improved K-Quant CUDA kernels to OpenCL,LostRuins,2023-06-22T10:13:55Z,2023-06-29T03:56:44+00:00,9,161.71
1967,Allow specifying p scale factor for ggml rope and rope_back ops,KerfuffleV2,2023-06-22T11:36:07Z,2023-06-27T15:03:06+00:00,8,123.45
1970,CUDA acceleration when using LoRAs,JohannesGaessler,2023-06-22T19:32:34Z,2023-06-28T16:35:54+00:00,2,141.06
1973,readme : fixed termux instructions,albbus-stack,2023-06-23T07:20:11Z,2023-06-24T10:32:13+00:00,1,27.2
1974,Fix ggml-metal.metal path and run nixfmt,novafacing,2023-06-23T07:34:19Z,2023-06-24T11:07:08+00:00,1,27.55
1975,convert : fix invalid params in write_vocab_only,aisk,2023-06-23T14:46:00Z,2023-06-24T11:02:07+00:00,1,20.27
1977,Fix top k sampler in server example,unknown,2023-06-23T15:49:17Z,2023-06-25T08:48:36+00:00,3,40.99
1978,"ggml: improve ggml_graph_dump_dot, add ggml_format_name",slaren,2023-06-23T15:52:14Z,2023-06-24T10:57:18+00:00,1,19.08
1981,upgrade zig build system support,coderonion,2023-06-23T17:33:34Z,2023-06-25T05:45:44+00:00,1,36.2
1988,Clean up compiler warnings in train-text,Davidy22,2023-06-25T03:47:45Z,2023-06-26T19:45:32+00:00,12,39.96
1990,fix test quantize perf,katsu560,2023-06-25T07:22:32Z,2023-06-26T16:47:02+00:00,3,33.41
1992,ggml : do not use _GNU_SOURCE gratuitously,ggerganov,2023-06-25T13:43:01Z,2023-07-22T11:59:58+00:00,1,646.28
1995,broken change: deprecate GGML_TASK_INIT and GGML_TASK_FINALIZE,mqy,2023-06-25T19:12:53Z,2023-07-01T15:42:43+00:00,10,140.5
1998,Simple webchat for server,tobi,2023-06-26T01:36:06Z,2023-07-04T14:05:27+00:00,17,204.49
1999,ggml : change ggml_graph_compute() API to not require context,mqy,2023-06-26T07:56:48Z,2023-07-07T16:24:02+00:00,7,272.45
2000,Remove shards weight file support,howard0su,2023-06-26T11:26:57Z,2023-06-28T17:13:03+00:00,4,53.77
2001,k-quants with super-block size of 64,ikawrakow,2023-06-26T11:56:07Z,2023-06-26T16:43:07+00:00,1,4.78
2006,Use unsigned for random seed,howard0su,2023-06-26T15:00:55Z,2023-06-29T13:15:16+00:00,2,70.24
2007,doc - LD_LIBRARY_PATH complement for some Android devices when building with CLBlast inside Termux,gustrd,2023-06-26T15:58:15Z,2023-06-26T19:34:45+00:00,1,3.61
2009,Add an API example using server.cpp similar to OAI.,jwj7140,2023-06-26T17:24:40Z,2023-07-04T18:06:13+00:00,7,192.69
2012,"use const auto &kv instead of auto &kv, statements into braces",mendax0110,2023-06-26T20:03:42Z,2023-06-28T17:22:45+00:00,4,45.32
2016,Fix build in baby-llama after ggml_rope change,howard0su,2023-06-27T00:23:26Z,2023-06-27T05:07:13+00:00,1,4.73
2019,Modified RoPE with linear scaling,ikawrakow,2023-06-27T13:20:32Z,2023-06-29T15:15:26+00:00,2,49.91
2020,fix pthreads setaffinity usage on android,Green-Sky,2023-06-27T13:56:45Z,2023-06-27T17:06:33+00:00,1,3.16
2022,Convert checks in llama_load_session_file to throw and handle them,randxie,2023-06-27T14:32:59Z,2023-07-01T16:02:58+00:00,4,97.5
2027,Fixed missing const qualifier in casts,set-soft,2023-06-28T00:55:47Z,2023-06-28T17:26:27+00:00,1,16.51
2028,Removed nchannels_x argument from mul_mat_vec_nc_f16_f32,set-soft,2023-06-28T00:59:45Z,2023-06-28T17:27:31+00:00,1,16.46
2035,Do not use _GNU_SOURCE gratuitously.,przemoc,2023-06-28T10:11:00Z,2023-09-08T12:09:22+00:00,1,1729.97
2039,Vulkan implementation (via Kompute),niansa,2023-06-28T14:14:57Z,2023-08-15T14:18:45+00:00,2,1152.06
2043,dequantize + matrix multiplication CUDA kernels,JohannesGaessler,2023-06-28T20:34:07Z,2023-07-31T12:35:45+00:00,1,784.03
2053,convert.py xgen support,tmm1,2023-06-30T02:34:58Z,2024-06-09T10:48:00+00:00,6,8288.22
2054,Implement customizable RoPE,jxy,2023-06-30T04:37:10Z,2023-07-15T10:34:17+00:00,5,365.95
2055,add support of baichuan-7b,foldl,2023-06-30T07:40:04Z,2023-07-01T17:00:26+00:00,1,33.34
2056,Test-based VRAM scratch size + context adjustment,JohannesGaessler,2023-06-30T09:43:43Z,2023-07-01T19:47:27+00:00,1,34.06
2057,Better CUDA synchronization logic,JohannesGaessler,2023-06-30T19:07:45Z,2023-07-01T19:49:45+00:00,1,24.7
2059,Vulkan Implementation,0cc4m,2023-06-30T19:12:58Z,2024-01-28T17:03:59+00:00,36,5085.85
2062,release metal buffers when freeing metal context,apage43,2023-06-30T23:21:03Z,2023-07-01T18:14:59+00:00,2,18.9
2063,cmake: don't force -mcpu=native on aarch64,dsd,2023-07-01T07:19:03Z,2023-07-01T18:31:44+00:00,2,11.21
2064,Fix crash of test-tokenizer-0 under Debug build,howard0su,2023-07-01T14:37:34Z,2023-07-03T18:43:55+00:00,1,52.11
2067,Quantized dot products for CUDA mul mat vec,JohannesGaessler,2023-07-01T19:04:47Z,2023-07-05T12:19:43+00:00,9,89.25
2073,"Add BPE dropout support, use it in training.",howard0su,2023-07-02T14:57:26Z,2024-05-17T07:13:47+00:00,3,7672.27
2074,Update model file name in `examples/alpaca.sh`,tslmy,2023-07-02T22:41:16Z,2023-07-06T16:17:50+00:00,1,89.61
2076,Fix server crashes,SlyEcho,2023-07-03T00:42:05Z,2023-07-03T21:05:23+00:00,1,20.39
2079,[llama] Don't need check file version when loading vocab score,howard0su,2023-07-03T01:52:04Z,2023-07-03T11:58:59+00:00,1,10.12
2081,update for baichuan,foldl,2023-07-03T03:31:14Z,2023-07-06T16:23:50+00:00,1,84.88
2082,Fixed OpenCL offloading prints,JohannesGaessler,2023-07-03T08:02:39Z,2023-07-05T06:58:05+00:00,1,46.92
2086,Fix opencl build by wrap #if-else-endif with \n,howard0su,2023-07-03T13:46:30Z,2023-07-07T03:34:18+00:00,1,85.8
2088,fix index for ne03 value,gotope,2023-07-03T15:37:57Z,2023-07-03T23:50:01+00:00,1,8.2
2095,Support using mmap when applying LoRA,howard0su,2023-07-04T08:07:30Z,2023-07-11T14:37:01+00:00,2,174.49
2099,Distributed inference via MPI,evanmiller,2023-07-04T12:48:00Z,2023-07-10T15:49:56+00:00,5,147.03
2103,Update Server Instructions For Web Front End,jessejohnson,2023-07-04T15:57:41Z,2023-07-05T15:13:36+00:00,3,23.27
2105,Fix input embedding example unsigned int seed,pnb,2023-07-04T18:18:34Z,2023-07-04T23:33:34+00:00,1,5.25
2107,Don't double count the sample time,howard0su,2023-07-05T01:14:22Z,2023-07-05T10:31:23+00:00,4,9.28
2113,Update Server Instructions,jessejohnson,2023-07-05T16:02:29Z,2023-07-05T18:03:19+00:00,1,2.01
2115,"Fix buidling with Intel MKL but ask for ""cblas.h"" issue (#2104)",clyang,2023-07-05T18:22:03Z,2023-07-09T08:12:20+00:00,4,85.84
2116,Expose generation timings from server & update completions.js,tobi,2023-07-05T19:26:36Z,2023-07-05T20:51:13+00:00,3,1.41
2120,Enhanced code readability and consistency,mendax0110,2023-07-05T21:34:10Z,2023-07-08T14:13:52+00:00,1,64.66
2127,Update README.md to add more docs indexes,rankaiyx,2023-07-07T02:47:13Z,2023-07-09T07:38:43+00:00,1,52.86
2135,Implement classifier-free guidance,bullno1,2023-07-07T14:48:15Z,2023-07-11T16:18:44+00:00,2,97.51
2140,5.5x more CUDA performance with 5 minutes of work,JohannesGaessler,2023-07-07T21:12:42Z,2023-07-07T22:25:15+00:00,1,1.21
2144,Fixed OpenLLaMA 3b CUDA mul_mat_vec_q,JohannesGaessler,2023-07-08T09:39:06Z,2023-07-08T18:01:44+00:00,3,8.38
2146,Fix ggml_tensor_extra_gpu memory leak,eugen-ajechiloae-clearml,2023-07-08T12:43:18Z,2023-07-09T08:38:14+00:00,1,19.92
2147,Update README.md,unknown,2023-07-08T12:51:12Z,2023-07-09T08:20:44+00:00,1,19.49
2148,Possible solution to allow K-quants on models with n_vocab!=32000,LostRuins,2023-07-08T13:02:47Z,2023-07-11T14:01:09+00:00,1,72.97
2151,Escape prompt prefix/suffix along with prompt,pnb,2023-07-09T00:02:56Z,2023-07-09T08:56:19+00:00,1,8.89
2153,"Remove ""first token must be BOS"" restriction",oobabooga,2023-07-09T03:00:32Z,2023-07-09T08:59:53+00:00,1,5.99
2160,CUDA: Quantized matrix matrix multiplication,JohannesGaessler,2023-07-09T21:42:01Z,2023-07-29T21:04:45+00:00,2,479.38
2177,FP16 is supported in CM=6.0,howard0su,2023-07-11T09:43:50Z,2023-07-12T12:18:40+00:00,2,26.58
2178,ggml : remove src0 and src1 from ggml_tensor and rename opt to src,spencersutton,2023-07-11T15:21:58Z,2023-07-11T16:31:11+00:00,1,1.15
2188,metal: new q4_0 mat-vec mul kernel ,lshzh-ww,2023-07-12T05:49:51Z,2023-07-12T20:10:55+00:00,7,14.35
2189,Fixed __dp4a compute capability: 6.0 -> 6.1,JohannesGaessler,2023-07-12T07:16:07Z,2023-07-12T08:38:53+00:00,1,1.38
2192,Support broadcast add & mul on CUDA (fixed),li-plus,2023-07-12T10:25:26Z,2023-07-14T18:38:25+00:00,5,56.22
2193,Add missing quotes to bash script,bodograumann,2023-07-12T14:45:43Z,2023-07-13T13:49:14+00:00,1,23.06
2197,Add functions that work directly on model,bullno1,2023-07-12T15:53:21Z,2023-07-14T18:55:24+00:00,1,51.03
2203,CUDA mul mat vec q kernels for k-quants,JohannesGaessler,2023-07-12T22:08:46Z,2023-07-14T17:44:09+00:00,1,43.59
2206,"Revert ""Support using mmap when applying LoRA (#2095)""",howard0su,2023-07-12T23:52:56Z,2023-07-13T13:58:25+00:00,1,14.09
2207,Fix CI error on Windows with CUDA,howard0su,2023-07-13T00:20:04Z,2023-07-13T13:58:09+00:00,1,13.63
2208,Enable LLAMA_METAL and LLAMA_MPI in Makefile,magnusviri,2023-07-13T01:42:15Z,2023-07-14T17:34:41+00:00,1,39.87
2212,Metal: faster Q4_0 and Q4_1 matrix x vector kernels,ikawrakow,2023-07-13T09:27:59Z,2023-07-14T09:46:22+00:00,1,24.31
2214,examples: fixed path typos in embd-input,xushangning,2023-07-13T13:07:39Z,2023-07-14T18:40:06+00:00,1,29.54
2216,build.zig: install config header,alichraghi,2023-07-13T19:29:30Z,2023-07-14T18:50:58+00:00,1,23.36
2218,Fix static_assert with older compilers,evanmiller,2023-07-14T10:58:01Z,2023-07-14T18:55:56+00:00,1,7.97
2219,fixed runtime bugs and compile errors related to GGML_PERF and GGML_DEBUG,mqy,2023-07-14T12:44:22Z,2023-07-16T19:57:29+00:00,2,55.22
2220,Allocate all temporary ggml_tensor_extra_gpu from a fixed-size buffer,bullno1,2023-07-14T14:04:48Z,2023-07-14T19:00:58+00:00,1,4.94
2222,"Fix #2221, use pkg-config",Freed-Wu,2023-07-14T14:26:32Z,2023-07-14T19:05:09+00:00,1,4.64
2225,An easy python script to create quantized (k-bit support) GGML models from local HF Transformer models.,richardr1126,2023-07-14T20:32:13Z,2023-07-21T18:32:44+00:00,2,166.01
2228,support bpe tokenizer in convert,ftgreat,2023-07-15T06:20:36Z,2023-07-25T13:22:10+00:00,1,247.03
2230,"ggml backends interface, ggml-cuda refactor",slaren,2023-07-15T10:40:44Z,2023-07-16T08:20:04+00:00,15,21.66
2234,add log_callback to llama_context_params for custom logging.,grahameth,2023-07-15T20:54:14Z,2023-08-09T20:46:40+00:00,12,599.87
2235,make : fix embdinput library and server examples building on MSYS2,przemoc,2023-07-15T23:27:34Z,2023-07-21T07:42:22+00:00,1,128.25
2238,llama : fix t_start_sample_us initialization warning,grencez,2023-07-16T05:03:19Z,2023-07-16T21:01:45+00:00,1,15.97
2239,"ggml backends interface, ggml-cuda refactor",slaren,2023-07-16T08:19:43Z,2023-08-25T10:14:18+00:00,5,961.91
2242,Support dup & cont ops on CUDA,li-plus,2023-07-16T11:43:28Z,2023-07-17T17:39:29+00:00,1,29.93
2244,Setting new target for test binaries,jpodivin,2023-07-16T15:56:01Z,2023-07-21T10:09:16+00:00,1,114.22
2248,metal: minor q4 optimization and reduce code size,lshzh-ww,2023-07-17T05:27:19Z,2023-07-20T10:32:23+00:00,7,77.08
2250,ci : integrate with ggml-org/ci,ggerganov,2023-07-17T13:38:01Z,2023-07-18T11:24:44+00:00,4,21.78
2256,"Fix #2252, add install() to CMakeLists.txt",Freed-Wu,2023-07-18T07:54:48Z,2023-07-19T07:01:11+00:00,2,23.11
2258,ggml : adapt Metal to new ggml_backend interface,ggerganov,2023-07-18T13:56:22Z,2023-10-08T10:59:30+00:00,27,1965.05
2267,Add llama_beam_search().,mattpulver,2023-07-18T19:57:03Z,2023-08-25T15:18:49+00:00,3,907.36
2268,llama: implement YaRN RoPE scaling,cebtenzzre,2023-07-18T23:00:11Z,2023-11-01T22:04:33+00:00,18,2543.07
2275,Support customized LLAMA_CUDA_NVCC and LLAMA_CUDA_CCBIN,skyan,2023-07-19T12:37:03Z,2023-07-21T10:38:57+00:00,1,46.03
2276,llama : grouped-query attention + LLaMAv2 70B support,ggerganov,2023-07-19T13:01:47Z,2023-07-23T12:09:48+00:00,1,95.13
2280,remove cfg smooth factor,Vermeille,2023-07-19T17:44:25Z,2023-07-21T10:58:36+00:00,2,41.24
2284,Little changes in .gitignore for Poetry users and Makefile for FreeBSD users,yukiteruamano,2023-07-20T04:26:24Z,2023-07-21T10:53:28+00:00,1,30.45
2287,MIKU MAYHEM: Upgrading the Default Model for Maximum Fun 🎉,at8u,2023-07-20T08:35:16Z,2023-07-21T08:13:18+00:00,1,23.63
2294,Faster Q5_K and Q6_K on Metal,ikawrakow,2023-07-20T14:10:19Z,2023-07-20T15:19:45+00:00,1,1.16
2295,Custom RoPE + bettter memory management for CUDA,ikawrakow,2023-07-20T15:17:02Z,2023-07-21T14:27:51+00:00,1,23.18
2297,Faster Q2_K on Metal,ikawrakow,2023-07-20T16:57:48Z,2023-07-21T07:44:40+00:00,5,14.78
2304,add `--in-prefix-bos` to prefix BOS to user inputs; keep EOS,jxy,2023-07-21T03:15:07Z,2023-07-25T12:19:12+00:00,3,105.07
2306,server: allow json array in prompt or content for direct token input,jxy,2023-07-21T05:48:34Z,2023-08-23T07:12:13+00:00,9,793.39
2307,Faster Q3_K implementation on Metal,ikawrakow,2023-07-21T07:56:42Z,2023-07-21T14:05:31+00:00,1,6.15
2308,Obtaining LLaMA 2 instructions,niansa,2023-07-21T16:05:24Z,2023-07-28T01:14:11+00:00,2,153.15
2312,Perplexity: Compute scores correlated to HellaSwag,klosax,2023-07-21T20:32:41Z,2023-07-22T12:21:24+00:00,1,15.81
2313,CUDA: Fixed 7b q3_K_S with mul_mat_vec_q,JohannesGaessler,2023-07-21T22:33:29Z,2023-07-22T19:27:34+00:00,1,20.9
2315,llama : fix tokenizer,goerch,2023-07-21T22:54:13Z,2023-08-21T20:20:19+00:00,20,741.43
2317,VIM plugin for server exe,whoreson,2023-07-22T05:53:19Z,2023-07-22T10:34:52+00:00,1,4.69
2322,Speed up Q4_K,ikawrakow,2023-07-22T13:29:03Z,2023-07-23T05:49:21+00:00,3,16.34
2323,Support bcast add & dup & cont op on MPS backend,li-plus,2023-07-22T14:12:51Z,2023-07-23T11:00:37+00:00,1,20.8
2327,Improvements to VIM plugin,AustinMroz,2023-07-22T19:17:59Z,2023-07-23T11:16:48+00:00,1,15.98
2329,Improve graph build time,slaren,2023-07-22T20:56:30Z,2023-07-25T12:32:21+00:00,3,63.6
2331,Fixes for CLBLAST compile support in FreeBSD,yukiteruamano,2023-07-22T21:14:08Z,2023-07-23T11:52:08+00:00,1,14.63
2333,ggml: move op parameters from tensors to ggml_tensor::op_params,slaren,2023-07-22T23:10:30Z,2023-07-23T12:36:02+00:00,1,13.43
2336,Print max tensor size to stderr,crasm,2023-07-23T09:31:03Z,2023-07-23T11:56:34+00:00,1,2.43
2337,Support `nix build '.#opencl'`,Freed-Wu,2023-07-23T10:29:33Z,2023-07-23T11:57:02+00:00,1,1.46
2338,Llama--Enable-pipe-friendly-help-output,maddes8cht,2023-07-23T10:41:57Z,2023-07-23T11:59:49+00:00,1,1.3
2339,k_quants : add AVX support to dot functions with QK_K as 64,katsu560,2023-07-23T11:26:32Z,2023-07-25T12:13:42+00:00,1,48.79
2346,Some more Q4_K and Q5_K speedup on CUDA,ikawrakow,2023-07-23T13:24:27Z,2023-07-23T21:19:48+00:00,7,7.92
2349,Fix line breaking error in build-info.sh,hesenp,2023-07-23T15:44:20Z,2023-07-25T12:24:10+00:00,1,44.66
2351,Add gqa parameter support to the server,IgnacioFDM,2023-07-23T16:51:46Z,2023-07-23T20:31:17+00:00,6,3.66
2356,CI: Add non-AVX scalar build/test,netrunnereve,2023-07-24T04:24:15Z,2023-07-25T12:16:13+00:00,1,31.87
2357,Add C grammar,siraben,2023-07-24T04:36:43Z,2023-09-01T13:32:15+00:00,3,944.93
2358,metal: concurrently dispatch commands,lshzh-ww,2023-07-24T05:46:34Z,2023-07-25T12:00:19+00:00,8,30.23
2359,Fix Q4_K and Q5_K for QK_K = 64 on CUDA,ikawrakow,2023-07-24T06:40:17Z,2023-07-25T10:48:04+00:00,1,28.13
2362,Fix scalar version of Q5_K when QK_K = 64,ikawrakow,2023-07-24T07:55:40Z,2023-07-24T09:55:02+00:00,1,1.99
2366,Chat UI extras,akx,2023-07-24T09:15:06Z,2023-07-24T14:54:22+00:00,1,5.65
2368,[Server] Escape HTML in webchat,SlyEcho,2023-07-24T09:54:26Z,2023-07-25T07:27:34+00:00,2,21.55
2371,Relax contiguous contraints in activation function,li-plus,2023-07-24T11:43:28Z,2023-07-25T12:58:32+00:00,1,25.25
2372,ggml : mul mat tweaks,ggerganov,2023-07-24T12:44:36Z,2023-08-07T11:25:58+00:00,3,334.69
2374,make rms_norm_eps a parameter,slaren,2023-07-24T14:19:46Z,2023-07-24T15:57:13+00:00,1,1.62
2375,Another speed gain for Q4_0 and Q4_1 on Metal,ikawrakow,2023-07-24T15:50:16Z,2023-07-25T10:48:30+00:00,4,18.97
2380,Add rms_norm_eps parameter to the server example,slaren,2023-07-24T20:48:27Z,2023-07-25T09:36:18+00:00,1,12.8
2384,Default for RMS epsilon,ikawrakow,2023-07-25T12:42:46Z,2023-07-25T15:35:54+00:00,1,2.89
2389,Hellaswag scores,klosax,2023-07-25T16:24:37Z,2023-07-28T18:25:36+00:00,1,74.02
2391,Fixing race condition in server and partial stream handling in frontend.,snichols,2023-07-25T17:04:59Z,2023-08-04T11:37:25+00:00,1,234.54
2392,ggml : allocate graphs in a context,slaren,2023-07-25T18:48:39Z,2023-07-26T13:56:53+00:00,5,19.14
2394,make : build with -Wmissing-prototypes,cebtenzzre,2023-07-25T22:43:43Z,2023-07-26T18:00:05+00:00,1,19.27
2398,GGUF,ggerganov,2023-07-26T08:19:08Z,2023-08-21T20:07:43+00:00,22,635.81
2400,add: server chat mode with llama2,nhamanasu,2023-07-26T10:25:12Z,2023-07-28T18:02:10+00:00,1,55.62
2405,fix:workaround for missing _mm256_setr_m128i in GCC < 8 in new-added k_quants.c.,lx200916,2023-07-26T16:58:09Z,2023-07-28T18:17:45+00:00,1,49.33
2410,ggml : fix assert in ggml_set_unary_op,slaren,2023-07-26T20:12:30Z,2023-07-26T21:57:23+00:00,1,1.75
2411,ggml : add graph tensor allocator,slaren,2023-07-26T23:25:08Z,2023-07-30T13:58:02+00:00,9,86.55
2414,server : Support dark mode,ebraminio,2023-07-27T06:37:17Z,2023-08-01T08:56:24+00:00,3,122.32
2416,metal : fix out-of-bounds access + style changes,ggerganov,2023-07-27T07:12:15Z,2023-08-07T07:52:57+00:00,1,264.68
2420,supporting more diverse tokenizers,eric8607242,2023-07-27T09:00:45Z,2023-07-28T18:10:05+00:00,1,33.16
2426,GGUF : write tensor,monatis,2023-07-27T19:27:39Z,2023-07-28T08:34:17+00:00,1,13.11
2431,Fix the description of the Tail free sampling (TFS) method.,WeirdConstructor,2023-07-28T06:17:05Z,2023-07-28T08:44:44+00:00,1,2.46
2433,Use n_embd_gqa instead of n_embd to handle llama-2 70B,randxie,2023-07-28T07:28:44Z,2023-07-28T08:42:53+00:00,1,1.24
2439,Train mem usage and other improvements ,xaedes,2023-07-28T22:20:48Z,2023-08-28T19:51:47+00:00,3,741.52
2448,CUDA: faster multi GPU synchronization,JohannesGaessler,2023-07-29T19:32:08Z,2023-07-29T21:04:11+00:00,1,1.53
2449,install ggml-meta.metal if LLAMA_METAL,ickc,2023-07-29T20:11:45Z,2023-08-16T20:09:49+00:00,1,431.97
2451,Fix compilation warnings (Linux/GCC),netrunnereve,2023-07-29T21:06:12Z,2023-08-02T08:06:20+00:00,14,83.0
2453,"CUDA: mmq CLI option, fixed mmq build issues",JohannesGaessler,2023-07-30T10:43:30Z,2023-07-31T13:44:35+00:00,3,27.02
2454,Validate-Params: refactoring-gpt-params-parse,maddes8cht,2023-07-30T14:10:01Z,2025-07-12T14:08:12+00:00,8,17111.97
2455,Fix Metal backend broken from the allocator changes,slaren,2023-07-30T16:28:15Z,2023-07-31T09:02:53+00:00,2,16.58
2458,CUDA: fewer memory bank conflicts for mul_mat_q,JohannesGaessler,2023-07-30T20:32:13Z,2023-07-31T11:18:52+00:00,1,14.78
2459,Updated mul_mat_f16_f32 metal kernel to allow llama-2-70B on metal,mbosc,2023-07-30T22:04:16Z,2023-08-01T07:43:12+00:00,4,33.65
2466,Chat UI: add a subtle loading animation,akx,2023-07-31T10:20:02Z,2023-09-04T08:28:55+00:00,2,838.15
2468,CUDA: Implemented row flattening for non-glm RoPE,JohannesGaessler,2023-07-31T11:04:22Z,2023-07-31T12:32:30+00:00,1,1.47
2470,CUDA: enable peer access between devices,JohannesGaessler,2023-07-31T15:31:20Z,2023-09-17T14:37:54+00:00,7,1151.11
2471,CUDA: fixed cmake F16 option,JohannesGaessler,2023-07-31T16:42:55Z,2023-07-31T17:52:22+00:00,1,1.16
2475,Add Chinese LLaMA-2 / Alpaca-2 to supported models,ymcui,2023-08-01T00:15:57Z,2023-08-02T06:18:31+00:00,2,30.04
2476,Setting correct format string for long unsigned.,jpodivin,2023-08-01T08:55:29Z,2023-08-03T00:13:25+00:00,1,39.3
2480,CUDA: Fix output size,JohannesGaessler,2023-08-01T14:21:02Z,2023-08-02T14:48:10+00:00,2,24.45
2483,CUDA: faster non k-quant mul_mat_q kernels,JohannesGaessler,2023-08-01T21:53:18Z,2023-08-02T16:04:04+00:00,1,18.18
2486,Server - support for saving templates in browser LocalStorage,staviq,2023-08-02T04:15:37Z,2023-08-17T23:34:02+00:00,2,379.31
2487,support Aquila-7B model series,ftgreat,2023-08-02T07:00:05Z,2023-08-02T08:21:11+00:00,2,1.35
2488,Stream save llama context data to file instead of allocating entire buffer upfront,l3utterfly,2023-08-02T08:44:22Z,2023-08-04T11:29:53+00:00,7,50.76
2489,server : display token probabilities in the UI,jhen0409,2023-08-02T08:58:12Z,2023-08-25T10:32:45+00:00,4,553.58
2491,convert.py: fix failure caused by missing abstract methods,oss-dev-somewhere,2023-08-02T15:17:52Z,2023-08-06T06:34:05+00:00,1,87.27
2495,Vim plugin: Streaming and more,AustinMroz,2023-08-02T21:48:27Z,2023-08-08T11:44:48+00:00,2,133.94
2499,build : fix several cast and printf warnings,iboB,2023-08-03T10:41:08Z,2023-08-04T10:07:21+00:00,1,23.44
2505,CUDA: check if event is NULL before cudaStreamWaitEvent,cebtenzzre,2023-08-03T19:06:53Z,2023-08-04T15:34:33+00:00,1,20.46
2506,CUDA: use min compute capability of GPUs actually used,cebtenzzre,2023-08-03T20:49:02Z,2023-08-04T15:35:22+00:00,1,18.77
2514,[Zig] Rewrite build for Zig 0.11,SlyEcho,2023-08-04T15:19:07Z,2023-08-07T05:35:54+00:00,1,62.28
2515,server: regenerate completion.js.hpp,cebtenzzre,2023-08-04T16:26:07Z,2023-08-04T19:00:57+00:00,1,2.58
2521,Fix issue related to Windows console mode persistence,DannyDaemonic,2023-08-05T04:32:50Z,2023-08-06T06:49:35+00:00,1,26.28
2524,server: add --numa support,TerrorJack,2023-08-05T12:37:33Z,2023-08-14T13:36:43+00:00,1,216.99
2525,CUDA: faster k-quant mul_mat_q kernels,JohannesGaessler,2023-08-05T14:18:08Z,2023-08-05T16:20:45+00:00,2,2.04
2529,Fixed mmap prefetch for GPU offloading,JohannesGaessler,2023-08-06T08:27:26Z,2023-08-07T08:09:41+00:00,11,23.7
2532,Allow passing grammar to completion endpoint,krasserm,2023-08-06T10:46:51Z,2023-08-08T13:29:20+00:00,2,50.71
2536,Refactor makefile fix build with CLBlast in arm ,GiviMAD,2023-08-06T23:18:19Z,2023-08-07T06:21:46+00:00,3,7.06
2538,llama : replace (permute + reshape + view_1d) with (view_3d),ggerganov,2023-08-07T09:35:09Z,2023-08-17T07:47:09+00:00,1,238.2
2543,"Multiline autocompletion for VIM example, get rid of ""^@"".",chaihahaha,2023-08-07T16:10:27Z,2023-08-08T12:07:02+00:00,1,19.94
2546,CUDA: tuned mul_mat_q kernels,JohannesGaessler,2023-08-07T21:28:46Z,2023-08-09T07:42:35+00:00,2,34.23
2548,main: add --prompt-cache-clobber option,crasm,2023-08-08T01:18:06Z,2023-10-10T07:10:48+00:00,5,1517.88
2549,Merge tokenizer fixes,goerch,2023-08-08T06:29:30Z,2023-08-14T16:30:28+00:00,3,154.02
2553,Fix unicode in grammars (fixes #2501),ejones,2023-08-08T13:05:20Z,2023-08-17T23:54:44+00:00,1,226.82
2554,[Zig] Fixing Zig build and improvements,SlyEcho,2023-08-08T15:07:01Z,2023-08-17T20:11:18+00:00,1,221.07
2559,Adding support for llama2.c models,byte-6174,2023-08-08T18:33:45Z,2023-08-11T23:17:26+00:00,5,76.73
2562,ggml-alloc: Don't try to re-use buffers of external tensors,smspillaz,2023-08-08T21:18:15Z,2023-08-09T20:47:42+00:00,2,23.49
2564,metal : print error of load pipeline state,jhen0409,2023-08-09T05:23:07Z,2023-08-16T20:09:03+00:00,1,182.77
2565,Add --n-predict -2 for stopping generation on full context,crasm,2023-08-09T06:25:48Z,2023-08-10T14:28:28+00:00,1,32.04
2566,Fix grammar-based sampling issue in server,krasserm,2023-08-09T13:36:00Z,2023-08-10T10:16:39+00:00,1,20.68
2568,CUDA: Removed obsolete cmake CUDA arch,JohannesGaessler,2023-08-09T15:14:08Z,2023-08-12T22:26:52+00:00,1,79.21
2569,Handle VT processing more gracefully on earlier versions of Windows,DannyDaemonic,2023-08-09T15:15:40Z,2023-08-10T20:11:36+00:00,1,28.93
2573,metal : return null instead of exit(1),jhen0409,2023-08-10T00:50:48Z,2023-08-14T13:37:40+00:00,1,108.78
2579,server: fixed wrong variable name in timing json,Equim-chan,2023-08-10T16:00:32Z,2023-08-11T22:35:15+00:00,1,30.58
2588,server : implement json-schema-to-grammar.mjs & add grammar param in the UI,jhen0409,2023-08-12T03:03:33Z,2023-08-14T07:16:55+00:00,1,52.22
2590,"CUDA: Fixed OpenLLaMA 3b mmq, reduced compile time",JohannesGaessler,2023-08-12T10:48:24Z,2023-08-12T22:24:46+00:00,1,11.61
2591,Add --cfg-negative-prompt-file option for examples,KerfuffleV2,2023-08-12T11:44:20Z,2023-08-17T13:29:45+00:00,3,121.76
2592,Enhance Windows 7 and below compatibility.,unknown,2023-08-12T12:21:08Z,2023-08-14T03:59:16+00:00,2,39.64
2594,adds simple grammar parsing tests,drbh,2023-08-12T21:57:39Z,2023-08-13T14:00:49+00:00,2,16.05
2596,"CUDA: Add launch bounds for Pascal, small q4_K, q5_K refactor",JohannesGaessler,2023-08-12T22:14:22Z,2023-08-14T08:41:23+00:00,3,34.45
2603,enable CPU HBM,jikunshang,2023-08-14T05:18:21Z,2023-09-08T01:46:56+00:00,5,596.48
2610,"Zig @cImport(""llama.h"") requires enum keyword in function signatures",cztomsik,2023-08-14T11:29:41Z,2023-08-14T13:35:16+00:00,1,2.09
2613,llama : sync gguf-llama with llama,ggerganov,2023-08-14T16:58:52Z,2023-08-14T18:33:33+00:00,2,1.58
2615,metal: matrix-matrix multiplication kernel,lshzh-ww,2023-08-14T18:38:27Z,2023-08-16T20:07:05+00:00,1,49.48
2616,server : add missing /json-schema-to-grammar.mjs,jhen0409,2023-08-14T20:21:19Z,2023-08-14T22:14:15+00:00,1,1.88
2618,adds simple llama grammar tests,drbh,2023-08-15T03:58:18Z,2023-08-17T07:41:01+00:00,1,51.71
2620,llama : refactor model loading code,ggerganov,2023-08-15T07:07:50Z,2023-08-16T11:34:04+00:00,3,28.44
2626,llama : add benchmark example,slaren,2023-08-15T19:54:47Z,2023-08-18T10:44:58+00:00,5,62.84
2627,metal: enable ggml-alloc,lshzh-ww,2023-08-16T01:30:09Z,2023-08-16T20:08:29+00:00,2,18.64
2629,gguf : deduplicate,ggerganov,2023-08-16T11:59:51Z,2023-08-16T16:25:29+00:00,1,4.43
2632,Finetune LORA,xaedes,2023-08-16T15:14:51Z,2023-09-28T18:40:12+00:00,22,1035.42
2635,convert-new.py : output gguf,ggerganov,2023-08-16T17:45:52Z,2023-08-17T14:19:52+00:00,1,20.57
2639,ggml-alloc: fix discrepency between measure&eval,lshzh-ww,2023-08-17T01:26:42Z,2023-08-17T07:35:54+00:00,2,6.15
2643,server : always regenerate asset hpp before compile,jhen0409,2023-08-17T11:58:54Z,2023-08-17T22:09:10+00:00,1,10.17
2644,Gguf: write tensors in a single pass,monatis,2023-08-17T12:21:52Z,2023-08-17T18:57:39+00:00,1,6.6
2646,server : better default prompt,ggerganov,2023-08-17T20:33:14Z,2023-08-18T21:45:37+00:00,1,25.21
2649,server : update xxd usage for older versions compatibility,jhen0409,2023-08-17T22:28:31Z,2023-08-18T21:41:32+00:00,1,23.22
2650,ci : add LoRA test to CI,slaren,2023-08-18T00:59:13Z,2023-08-27T07:03:27+00:00,1,222.07
2657,YAML logging and presets,JohannesGaessler,2023-08-18T10:36:51Z,2023-08-28T15:59:39+00:00,13,245.38
2659,Add link to clojure bindings to Readme.,phronmophobic,2023-08-18T18:09:22Z,2023-08-18T19:39:23+00:00,1,1.5
2663,ggml : move all type info to ggml_type_traits,slaren,2023-08-19T00:57:13Z,2023-08-20T20:17:53+00:00,1,43.34
2665,Fix CUDA softmax by subtracting max value before exp,li-plus,2023-08-19T04:03:29Z,2023-08-22T18:27:07+00:00,2,86.39
2668,Improve token type support,goerch,2023-08-19T12:42:34Z,2023-08-21T15:56:02+00:00,3,51.22
2669,CUDA: fix __builtin_assume for CUDA < 11.2,JohannesGaessler,2023-08-19T15:22:55Z,2023-09-16T14:56:05+00:00,5,671.55
2670,ggml: support CUDA's half type for aarch64(#1455),KyL0N,2023-08-19T16:16:52Z,2023-08-22T07:14:23+00:00,1,62.96
2674,ggml: create thread pool lazily,JohannesGaessler,2023-08-19T21:14:46Z,2023-09-21T21:07:25+00:00,1,791.88
2675,make scripts executable,cebtenzzre,2023-08-19T21:21:09Z,2023-08-23T14:29:09+00:00,4,89.13
2677,More efficient HellaSwag implementation,ikawrakow,2023-08-20T07:39:25Z,2023-08-20T13:44:46+00:00,1,6.09
2681,HellaSwag: split token evaluation into batches if needed,ikawrakow,2023-08-20T14:45:52Z,2023-08-21T08:11:31+00:00,1,17.43
2682,Add script to convert GGMLv3 LLaMA models to GGUF,KerfuffleV2,2023-08-20T15:36:56Z,2023-08-21T14:45:52+00:00,6,23.15
2683,CUDA: use mul_mat_q kernels by default,JohannesGaessler,2023-08-20T15:39:59Z,2023-08-22T20:47:06+00:00,1,53.12
2684,ggml-cuda : use graph allocator,slaren,2023-08-20T19:37:16Z,2023-08-22T13:25:19+00:00,3,41.8
2685,Fix import of llama2.c models that don't share weights between embedding layers,ochafik,2023-08-20T22:30:58Z,2023-08-23T19:33:06+00:00,3,69.04
2686,metal: temporary fix for the mat-mut kernel,lshzh-ww,2023-08-21T05:54:42Z,2023-08-21T10:59:29+00:00,1,5.08
2688,server : fallback to default if client param is null,jhen0409,2023-08-21T08:02:45Z,2023-08-22T00:32:01+00:00,1,16.49
2690,Feature: Integrate with unified SYCL backend for Intel GPUs ,abhilash1910,2023-08-21T16:19:31Z,2024-01-28T15:56:23+00:00,59,3839.61
2695,llama-bench : minor fixes,slaren,2023-08-21T22:05:37Z,2023-08-22T07:56:03+00:00,1,9.84
2699,metal: add missing barriers for mul-mat,lshzh-ww,2023-08-21T23:53:14Z,2023-08-22T06:18:41+00:00,1,6.42
2700,Skip computation of much of last layer & unused logits during prompt eval / large N,ochafik,2023-08-22T00:27:11Z,2024-03-22T14:28:52+00:00,10,5126.03
2701,docs : add grammar docs,ejones,2023-08-22T03:15:58Z,2023-08-23T01:01:58+00:00,1,21.77
2707,Quantization improvements for k_quants,ikawrakow,2023-08-22T09:58:22Z,2023-08-22T16:14:09+00:00,1,6.26
2710,gguf : add ftype meta info to the model,ggerganov,2023-08-22T11:42:17Z,2023-08-22T17:05:59+00:00,5,5.39
2714,Strided perplexity,ikawrakow,2023-08-22T15:22:08Z,2023-08-23T09:56:42+00:00,6,18.58
2718,Clarifying error message,apetenchea,2023-08-22T17:29:50Z,2023-08-22T18:58:16+00:00,1,1.47
2724,Fix for #2721,goerch,2023-08-22T19:41:46Z,2023-08-22T21:10:43+00:00,1,1.48
2725,Improve handling of special tokens in GGML to GGUF converter,KerfuffleV2,2023-08-22T20:38:12Z,2023-08-22T23:39:39+00:00,1,3.02
2727,main : insert bos if no tokens,klosax,2023-08-22T23:54:03Z,2023-08-23T14:46:03+00:00,3,14.87
2729,Fix .gitignore for windows,akawrykow,2023-08-23T00:39:52Z,2023-08-23T14:31:35+00:00,1,13.86
2732,Tag release with build number instead of short hash,DannyDaemonic,2023-08-23T06:46:19Z,2023-08-24T13:58:02+00:00,3,31.2
2733,Fix ggml to gguf conversion on Windows,IgnacioFDM,2023-08-23T07:13:29Z,2023-08-23T09:31:10+00:00,1,2.29
2735,Minor: fix values shown in the quantize tool help,ikawrakow,2023-08-23T07:25:27Z,2023-08-23T09:57:13+00:00,1,2.53
2738,Fix convert-lora-to-ggml.py,slaren,2023-08-23T11:46:32Z,2023-08-23T14:46:54+00:00,1,3.01
2740,Explain the user that GGML isn't supported anymore,IgnacioFDM,2023-08-23T12:42:17Z,2023-10-08T10:57:29+00:00,2,1102.25
2748,main: log file,staviq,2023-08-23T18:39:25Z,2023-08-30T06:29:32+00:00,15,155.84
2751,Update llama2.c converter to read vocab and write models in GGUF format,ochafik,2023-08-23T21:09:34Z,2023-08-27T14:13:32+00:00,1,89.07
2753,Allow convert.py to convert directly to q8_0,KerfuffleV2,2023-08-23T22:00:55Z,2023-08-26T20:13:37+00:00,6,70.21
2756,llama : fix grammar sometimes generating null char,ejones,2023-08-24T01:00:58Z,2023-08-24T04:07:13+00:00,1,3.1
2757,metal: bug-fix when enable ggml-alloc,lshzh-ww,2023-08-24T05:32:58Z,2023-08-24T16:27:25+00:00,1,10.91
2759,Publish gguf as a Pip package,monatis,2023-08-24T06:18:48Z,2023-08-25T06:26:06+00:00,1,24.12
2760,cuda : add RoPE kernel for mode == 2 (NeoX),ggerganov,2023-08-24T08:52:59Z,2023-08-25T08:55:59+00:00,3,24.05
2762,metal : fix memory leak,ggerganov,2023-08-24T10:15:26Z,2023-08-28T07:59:08+00:00,1,93.73
2763,metal : add Q8_0 support,ggerganov,2023-08-24T10:42:01Z,2023-08-24T13:19:57+00:00,1,2.63
2765,Fix cublas NaNs in Falcon,klosax,2023-08-24T12:26:45Z,2023-08-27T12:12:58+00:00,7,71.77
2771,llama-bench : add model sizes,slaren,2023-08-24T20:08:04Z,2023-08-25T13:16:19+00:00,1,17.14
2772,convert.py : Get rope scale from HuggingFace models,pnb,2023-08-24T20:16:06Z,2023-08-25T14:41:53+00:00,4,18.43
2773,convert.py : add rope_freq_base when converting CodeLlama from an HF model,slaren,2023-08-24T20:17:25Z,2023-08-25T12:08:53+00:00,1,15.86
2776,ggml-alloc: enlarge parse_seq to contain barriers,lshzh-ww,2023-08-25T01:55:42Z,2023-08-25T05:58:00+00:00,1,4.04
2793,convert.py : Handle null rope scaling value in HF config.json,pnb,2023-08-25T17:05:41Z,2023-08-26T12:11:18+00:00,2,19.09
2800,llama : use std::abs in llama_sample_tail_free,cebtenzzre,2023-08-25T22:20:39Z,2023-08-26T16:53:52+00:00,4,18.55
2802,Add a `/detokenize` endpoint to the example server,BruceMacD,2023-08-26T01:19:45Z,2023-08-26T23:11:46+00:00,1,21.87
2805,Fix HellaSwag,ikawrakow,2023-08-26T08:09:21Z,2023-08-26T13:48:54+00:00,1,5.66
2806,Fix spm whitespaces,klosax,2023-08-26T10:19:56Z,2023-08-26T11:45:54+00:00,4,1.43
2807,Better perplexity for 2- and 3-bit quantization for LLaMA-v2-70B,ikawrakow,2023-08-26T12:10:21Z,2023-08-26T14:27:49+00:00,1,2.29
2810,llama : more tokenizer fixes,ggerganov,2023-08-26T12:57:25Z,2023-08-27T11:19:19+00:00,14,22.36
2811,k-quants : remove unnecessary tensor shape restrictions,ggerganov,2023-08-26T13:08:48Z,2023-08-26T14:37:36+00:00,1,1.48
2814,Use Unicode Escape Sequence to replace encoded characters,drasticactions,2023-08-26T14:22:12Z,2023-08-26T18:27:07+00:00,5,4.08
2816,k_quants tuning for Falcon-7b,ikawrakow,2023-08-26T16:42:14Z,2023-08-27T12:19:59+00:00,2,19.63
2817,llama : move #includes out of _GNU_SOURCE conditional,cebtenzzre,2023-08-26T17:16:01Z,2023-08-26T18:17:51+00:00,2,1.03
2819,Small (and unreliable) AVX2 ggml_vec_dot_q4_K_q8_K improvement - remove instruction dependency,hydroo,2023-08-26T17:51:21Z,2023-08-28T12:51:08+00:00,2,43.0
2821,gguf : add 64-bit support (GGUF v2),ggerganov,2023-08-26T18:53:48Z,2023-08-27T11:19:55+00:00,2,16.44
2823,quantize : make output filename optional again,cebtenzzre,2023-08-27T05:30:06Z,2023-08-28T06:32:25+00:00,2,25.04
2832,llama-bench : set locale to utf8,slaren,2023-08-27T13:45:08Z,2023-08-28T17:19:19+00:00,4,27.57
2833,"CUDA: fix RoPE asserts, block sizes",JohannesGaessler,2023-08-27T14:00:42Z,2023-08-28T11:23:55+00:00,1,21.39
2840,Improved README instructions for building in Termux on Android devices,unknown,2023-08-27T17:45:10Z,2023-09-06T09:26:42+00:00,4,231.69
2842,Various script cleanups/fixes + convert merges and special token handling,KerfuffleV2,2023-08-27T18:34:41Z,2023-08-30T08:25:51+00:00,38,61.85
2847,llama.h: add missing struct keyword for C compat in callback type,igarnier,2023-08-28T04:47:47Z,2023-08-28T08:20:00+00:00,1,3.54
2848,tests : add a C compliance test,cebtenzzre,2023-08-28T06:05:40Z,2023-08-30T06:20:27+00:00,1,48.25
2849,server : avoid antiprompt in probabilities of final response,jhen0409,2023-08-28T06:52:21Z,2023-09-02T00:31:47+00:00,2,113.66
2855,makefile: fix tests build,alonfaraj,2023-08-28T14:09:51Z,2023-08-28T15:38:35+00:00,1,1.48
2857,added `struct` to llama_dump_timing_info_yaml's `llama_context`,MarcusDunn,2023-08-28T19:36:12Z,2023-08-29T06:33:28+00:00,1,10.95
2859,"make : fix clang tests build, add missing examples",cebtenzzre,2023-08-28T22:11:32Z,2023-08-29T08:42:42+00:00,1,10.52
2861,fix most gcc and clang warnings,cebtenzzre,2023-08-28T22:53:36Z,2023-09-01T13:34:51+00:00,6,86.69
2869,readme : add react-native binding,jhen0409,2023-08-29T07:51:32Z,2023-08-29T09:30:10+00:00,1,1.64
2874,ggml : add view_src and view_offs to ggml_tensor for views,slaren,2023-08-29T13:58:15Z,2023-08-29T21:24:43+00:00,1,7.44
2876,10X faster BPE tokenizer,ikawrakow,2023-08-29T15:12:33Z,2023-08-29T20:55:03+00:00,6,5.71
2879,"Stop generation at multiple linebreaks, bind to <F2>",chaihahaha,2023-08-29T15:59:29Z,2023-08-30T06:50:55+00:00,1,14.86
2881,README : remove outdated references to -eps and -gqa,slaren,2023-08-29T16:22:26Z,2023-08-29T21:17:34+00:00,1,4.92
2882,Tell users attmepting to run perplexity with too few tokens to use more,ikawrakow,2023-08-29T16:36:39Z,2023-08-29T20:55:45+00:00,1,4.32
2884,[Docker] fix tools.sh argument passing.,SlyEcho,2023-08-29T16:49:42Z,2023-08-30T16:14:53+00:00,6,23.42
2885,docs: add `node-llama-cpp` to `README.md`,giladgd,2023-08-29T20:47:22Z,2023-08-30T08:40:13+00:00,1,11.88
2886,make : support overriding CFLAGS/CXXFLAGS/CPPFLAGS/LDFLAGS,cebtenzzre,2023-08-29T21:07:36Z,2023-09-03T05:26:59+00:00,3,104.32
2887,[falcon] Fix Falcon for rw-1b model,akawrykow,2023-08-29T21:19:35Z,2023-08-30T14:57:42+00:00,1,17.64
2889,llama : fix bpe tokenize from byte,opparco,2023-08-30T02:42:58Z,2023-09-03T10:18:10+00:00,3,103.59
2895,Update api_like_OAI.py,superchargez,2023-08-30T06:26:58Z,2023-09-06T06:46:04+00:00,2,168.32
2896,gguf : add workflow for Pypi publishing,monatis,2023-08-30T07:57:12Z,2023-08-30T09:47:40+00:00,1,1.84
2897,Makefile: add test and update CI,alonfaraj,2023-08-30T08:35:24Z,2023-08-30T09:42:52+00:00,1,1.12
2901,build : on Mac OS enable Metal by default,ggerganov,2023-08-30T10:12:22Z,2023-09-04T19:26:24+00:00,10,129.23
2903,Update clblast readme,bandoti,2023-08-30T14:00:16Z,2023-09-02T12:53:18+00:00,1,70.88
2906,remove convert-llama-7b-pth-to-gguf.py and convert-llama-hf-to-gguf.py,slaren,2023-08-30T14:47:17Z,2023-08-31T23:32:09+00:00,1,32.75
2908,quick start command fix,gklab,2023-08-30T15:03:03Z,2023-09-01T14:06:44+00:00,1,47.06
2910,CUDA: mul_mat_q RDNA2 tunings,JohannesGaessler,2023-08-30T17:52:31Z,2023-09-13T09:20:25+00:00,6,327.46
2911,logs: fix mingw-like builds (fixes #2898),staviq,2023-08-30T18:04:50Z,2023-09-01T09:07:06+00:00,13,39.04
2912,CUDA: mul_mat_q=true as default for llama_context_params,JohannesGaessler,2023-08-30T18:14:26Z,2023-08-30T19:46:19+00:00,1,1.53
2913,llama2c : fix segfault and alloc-dealloc-mismatch,cebtenzzre,2023-08-30T18:52:28Z,2023-09-01T09:03:49+00:00,1,38.19
2914,[Falcon] Use stated vocab size,akawrykow,2023-08-30T19:11:58Z,2023-09-14T17:19:42+00:00,2,358.13
2915,CUDA: use 1 thread if model is fully offloaded,JohannesGaessler,2023-08-30T20:05:39Z,2023-09-21T08:43:53+00:00,3,516.64
2916,"convert : fix python 3.8 support, modernize type annotations",cebtenzzre,2023-08-30T20:13:26Z,2023-08-31T05:02:24+00:00,1,8.82
2917,metal : slight speed-up for add and mul kernels,ggerganov,2023-08-30T20:23:40Z,2023-09-01T10:42:41+00:00,1,38.32
2920,k-quants : fix build on armv7 (android only),jhen0409,2023-08-31T01:57:03Z,2023-09-02T12:23:45+00:00,3,58.45
2927,scripts: Use local gguf package when running from repo,KerfuffleV2,2023-08-31T10:05:16Z,2023-08-31T22:49:24+00:00,3,12.74
2928,Add Code Coverage and codecov.io integration,alonfaraj,2023-08-31T10:21:33Z,2023-09-03T08:48:49+00:00,1,70.45
2929,[GGML] Added RISC-V Vector Intrinsics Support,Tameem-10xE,2023-08-31T10:55:53Z,2023-09-01T13:27:40+00:00,1,26.53
2931,"Allow quantize to only copy tensors, other improvements",KerfuffleV2,2023-08-31T11:47:08Z,2023-09-01T14:02:49+00:00,3,26.26
2934,[WIP] Add Fill-In-Middle example,apaz-cli,2023-08-31T14:53:30Z,2023-10-08T10:51:30+00:00,3,907.97
2935,docs: add java-llama.cpp to README.md,kherud,2023-08-31T15:08:53Z,2023-09-01T13:36:14+00:00,1,22.46
2938,convert.py: BPE fixes?,KerfuffleV2,2023-08-31T17:19:12Z,2023-09-03T05:52:13+00:00,1,60.55
2942,cuda : vsubss4 for older versions of ROCm/clang,Engininja2,2023-08-31T19:37:13Z,2023-09-01T21:33:19+00:00,2,25.93
2943,Fix Compile Errors in gptneox-main.cpp and gptneox quantization convert error,mmnga,2023-08-31T20:50:54Z,2023-09-03T05:36:29+00:00,1,56.76
2945,make : use unaligned vector moves on MinGW,cebtenzzre,2023-08-31T23:00:34Z,2023-09-01T13:53:14+00:00,1,14.88
2952,ggml_metal_init: Show all Metal device instances in the system,knweiss,2023-09-01T09:45:06Z,2023-09-02T12:29:09+00:00,2,26.73
2955,[ggml-openc.cpp]: fix a bug in ggml_cl_pool_malloc() for ggml_cl_mul_…,rchardx,2023-09-01T10:17:49Z,2023-09-03T08:46:45+00:00,3,46.48
2959,More optimizations on metal,ikawrakow,2023-09-01T16:12:59Z,2023-09-03T08:06:22+00:00,3,39.89
2960,CMake: add relocatable Llama package,bandoti,2023-09-01T16:35:25Z,2023-09-14T17:04:40+00:00,1,312.49
2966,logging: Fix creating empty file even when disabled,KerfuffleV2,2023-09-02T09:22:01Z,2023-09-02T17:53:55+00:00,2,8.53
2969,KV cache quantized to q8_0,JohannesGaessler,2023-09-02T15:19:40Z,2023-12-10T18:45:35+00:00,7,2379.43
2973,ggml-alloc : use virtual memory for measurement,slaren,2023-09-02T20:38:58Z,2023-09-03T18:34:10+00:00,2,21.92
2978,Add missing c file to Package.swift,kchro3,2023-09-03T01:34:23Z,2023-09-03T05:27:25+00:00,1,3.88
2984,gguf: Fix special vocab handling when id < 0,KerfuffleV2,2023-09-03T07:44:44Z,2023-09-03T10:38:43+00:00,1,2.9
2985,2x faster (rms) norm cuda kernels (3.7% e2e improvement),li-plus,2023-09-03T09:00:12Z,2023-09-04T06:53:30+00:00,6,21.89
2991,speculative : add grammar support,ggerganov,2023-09-03T12:23:13Z,2023-09-05T05:46:17+00:00,5,41.38
2994,ggml-opencl : store GPU buffer in ggml_tensor::extra,slaren,2023-09-03T18:27:52Z,2023-09-04T12:59:53+00:00,1,18.53
2995,metal: Q3_K speedup,ikawrakow,2023-09-03T19:12:42Z,2023-09-08T16:01:05+00:00,1,116.81
2998,examples : add compiler version and target to build info,cebtenzzre,2023-09-03T23:58:30Z,2023-09-15T20:59:50+00:00,7,285.02
2999,llama-bench : make cpp file non-executable,cebtenzzre,2023-09-04T01:51:52Z,2023-09-04T10:40:19+00:00,1,8.81
3003,Update makefile and gitignore for speculative sampling,leng-yue,2023-09-04T07:56:35Z,2023-09-04T10:39:57+00:00,1,2.72
3006,Add heuristic algorithm for speculative,leng-yue,2023-09-04T09:21:39Z,2023-09-14T16:14:45+00:00,13,246.88
3007,arm64 support for windows,es0m,2023-09-04T09:30:28Z,2023-09-13T01:54:20+00:00,11,208.4
3009,"Feature: support baichuan serial models, by now, including Baichuan-7…",jameswu2014,2023-09-04T09:55:08Z,2023-09-14T16:32:11+00:00,9,246.62
3010,Guard against all weights in a super-block being zero,ikawrakow,2023-09-04T11:14:01Z,2023-09-05T07:55:33+00:00,5,20.69
3012,metal : bug with ggml_cont,ggerganov,2023-09-04T16:48:26Z,2024-04-29T14:25:20+00:00,4,5709.61
3017,examples : replace fprintf to stdout with printf,cebtenzzre,2023-09-04T22:50:49Z,2023-09-05T19:10:27+00:00,1,20.33
3019,make : use new flag variables for recent changes,cebtenzzre,2023-09-05T02:13:50Z,2023-09-05T19:12:00+00:00,3,16.97
3023,convert-llama-ggmlv3-to-gguf: Try to handle files older than GGJTv3,KerfuffleV2,2023-09-05T09:42:47Z,2023-09-06T08:49:11+00:00,3,23.11
3024,Parallel RoPE on metal,ikawrakow,2023-09-05T10:37:56Z,2023-09-07T13:45:01+00:00,11,51.12
3028,fix convert.py not working with int filename_stem,Green-Sky,2023-09-05T14:10:47Z,2023-09-05T17:41:00+00:00,1,3.5
3031,make : improve test target,cebtenzzre,2023-09-05T18:43:26Z,2023-09-07T14:15:01+00:00,1,43.53
3033,build : add LLAMA_METAL_NDEBUG flag,cebtenzzre,2023-09-05T19:18:09Z,2023-09-05T22:21:10+00:00,4,3.05
3035,make : fix CPPFLAGS,cebtenzzre,2023-09-05T22:16:04Z,2023-09-07T14:13:51+00:00,1,39.96
3036,Add test coverage of the GGUFWriter class by adding several test cases.,mendax0110,2023-09-05T23:23:53Z,2023-10-01T11:23:01+00:00,3,611.99
3037,Posixify madvise and pagesize.,przemoc,2023-09-05T23:42:03Z,2023-09-07T08:15:06+00:00,2,32.55
3038,fix some warnings from gcc and clang-tidy,cebtenzzre,2023-09-05T23:55:09Z,2023-09-07T17:22:29+00:00,3,41.46
3041,update path to match default folder name for 7B model in readme,mtwebb,2023-09-06T02:50:16Z,2023-09-10T05:25:14+00:00,2,98.58
3042,adding train-text-from-scratch to flake.nix,takov751,2023-09-06T03:25:52Z,2023-09-08T16:06:26+00:00,1,60.68
3043,readme : fix typo,eltociear,2023-09-06T03:41:47Z,2023-09-08T16:04:32+00:00,1,60.38
3044,docker : add git to full-cuda.Dockerfile main-cuda.Dockerfile,polym,2023-09-06T07:31:16Z,2023-09-08T10:57:55+00:00,1,51.44
3048,convert : fix F32 ftype not being saved,cebtenzzre,2023-09-06T21:04:40Z,2023-09-07T18:27:42+00:00,1,21.38
3053,CI: add FreeBSD & simplify CUDA windows,alonfaraj,2023-09-07T09:09:07Z,2023-09-14T17:21:26+00:00,9,176.21
3063,speculative: add --n-gpu-layers-draft option,sozforex,2023-09-07T14:17:21Z,2023-09-13T06:50:47+00:00,2,136.56
3066,examples : make n_ctx warning work again,cebtenzzre,2023-09-07T17:36:15Z,2023-09-08T15:43:35+00:00,2,22.12
3078,Metal support for Swift,kchro3,2023-09-08T05:41:01Z,2023-09-09T09:12:10+00:00,24,27.52
3079,Update deprecated GGML TheBloke links to GGUF,SleepyYui,2023-09-08T07:08:52Z,2023-09-08T10:32:55+00:00,1,3.4
3080,Add a simpler main example,KerfuffleV2,2023-09-08T07:22:43Z,2024-05-26T13:32:22+00:00,6,6270.16
3084,Metal: PP speedup,ikawrakow,2023-09-08T16:20:36Z,2023-09-11T07:30:12+00:00,1,63.16
3086,build : fixes #2210 by adding a compiler flag check,RossComputerGuy,2023-09-08T19:54:07Z,2023-09-13T13:08:52+00:00,1,113.25
3089,metal : support build for iOS/tvOS,jhen0409,2023-09-08T23:16:25Z,2023-09-09T08:46:04+00:00,1,9.49
3095,gguf-py: Support identity operation in TensorNameMap,KerfuffleV2,2023-09-09T10:05:34Z,2023-09-14T16:32:26+00:00,1,126.45
3096,Reenable tokenizer test for LLaMa,goerch,2023-09-09T10:33:04Z,2023-09-13T13:19:44+00:00,1,98.78
3098,convert: remove most of the n_mult usage in convert.py,Green-Sky,2023-09-09T15:48:42Z,2023-09-10T15:06:53+00:00,5,23.3
3103,feat: docker gpu image CI builds,canardleteer,2023-09-09T23:25:44Z,2023-09-14T16:47:00+00:00,1,113.35
3110,CUDA: refactor ggml_cuda_op + lower GPU latency via quantization on main GPU and tiling,JohannesGaessler,2023-09-10T16:38:41Z,2023-09-11T17:55:51+00:00,3,25.29
3112,CUDA: add device number to error messages,JohannesGaessler,2023-09-10T22:44:57Z,2023-09-11T11:00:24+00:00,1,12.26
3115,llama : make quantize example up to 2.7x faster,cebtenzzre,2023-09-10T23:46:46Z,2023-09-15T01:09:54+00:00,4,97.39
3116,cmake : support build for iOS/tvOS,jhen0409,2023-09-11T00:04:50Z,2023-09-11T11:49:07+00:00,1,11.74
3130,CUDA: fix LoRAs,JohannesGaessler,2023-09-11T22:18:28Z,2023-09-12T22:15:33+00:00,3,23.95
3132,Enable build with CUDA 11.0 (make),Microflame,2023-09-11T22:59:32Z,2023-09-16T14:55:44+00:00,5,111.94
3152,metal : reusing llama.cpp logging,Ricardicus,2023-09-12T21:31:08Z,2023-09-27T15:48:34+00:00,2,354.29
3155,"make : fix clang++ detection, move some definitions to CPPFLAGS",cebtenzzre,2023-09-13T04:59:12Z,2023-09-14T17:22:47+00:00,1,36.39
3158,build: Fix ROCm shared library builds,abetlen,2023-09-13T06:24:06Z,2023-09-14T17:38:16+00:00,1,35.24
3160,Cloud-V CI for RISC-V builds,alitariq4589,2023-09-13T13:10:22Z,2023-09-15T08:06:56+00:00,2,42.94
3168,metal : relax conditions on fast matrix multiplication kernel,ggerganov,2023-09-14T14:12:28Z,2023-09-15T08:09:24+00:00,3,17.95
3170,Fixing the last deviations from sentencepiece indicated by test-tokenizer-1,goerch,2023-09-14T15:16:28Z,2023-09-16T11:41:33+00:00,12,44.42
3176,cmake : fix building shared libs for clang (rocm) on windows,Engininja2,2023-09-14T19:31:38Z,2023-09-15T12:24:30+00:00,1,16.88
3177,Remove mtest,rbur0425,2023-09-14T19:48:46Z,2023-09-15T07:28:45+00:00,1,11.67
3179,cmake: Fix llama.h file location when building shared library,abetlen,2023-09-14T20:24:08Z,2023-09-15T08:07:44+00:00,1,11.73
3184,check C++ code with -Wmissing-declarations,cebtenzzre,2023-09-15T00:48:40Z,2023-09-15T19:38:27+00:00,4,18.83
3185,convert : make ftype optional in simple scripts,cebtenzzre,2023-09-15T01:25:47Z,2023-09-15T16:29:02+00:00,1,15.05
3187,feat: support StarCoder model architectures,wsxiaoys,2023-09-15T04:57:07Z,2023-09-15T19:02:13+00:00,5,14.09
3192,sync : ggml (Metal F32 support + reduce ggml-alloc size),ggerganov,2023-09-15T11:55:16Z,2023-09-15T16:06:03+00:00,8,4.18
3200,build : enable more non-default compiler warnings,cebtenzzre,2023-09-15T20:20:41Z,2023-09-28T21:41:44+00:00,1,313.35
3202,nix: add cuda to the flake,Green-Sky,2023-09-15T21:03:50Z,2023-09-25T11:48:31+00:00,1,230.74
3205,make : restore build-info.h dependency for several targets,cebtenzzre,2023-09-16T03:20:05Z,2023-09-18T14:03:53+00:00,1,58.73
3206,llama : quantize up to 31% faster on Linux with mmap,cebtenzzre,2023-09-16T03:26:35Z,2023-09-29T13:48:45+00:00,3,322.37
3215,Enable BUILD_SHARED_LIBS=ON on all Windows builds,IsaacDynamo,2023-09-16T12:12:56Z,2023-09-16T17:35:25+00:00,1,5.37
3220,CUDA: fix scratch buffers being allocated on non-main device,JohannesGaessler,2023-09-16T19:52:39Z,2023-09-17T12:16:22+00:00,2,16.4
3223,llama.cpp : show model size and BPW on load,slaren,2023-09-16T23:14:21Z,2023-09-17T12:33:28+00:00,2,13.32
3224,Update examples/embedding/README.md,yuiseki,2023-09-17T00:54:57Z,2023-09-21T08:57:40+00:00,2,104.05
3228,llama : custom attention mask + parallel decoding + no context swaps,ggerganov,2023-09-17T15:05:32Z,2023-09-28T16:04:36+00:00,9,264.98
3231,CUDA: fix peer access logic,JohannesGaessler,2023-09-17T18:21:54Z,2023-09-17T21:35:21+00:00,1,3.22
3236,ci : switch cudatoolkit install on windows to networked,Green-Sky,2023-09-17T21:43:54Z,2023-09-18T00:21:47+00:00,1,2.63
3240,llama : allow gguf RoPE keys to be overridden with defaults,cebtenzzre,2023-09-18T00:48:00Z,2023-09-20T16:12:47+00:00,1,63.41
3252,Work on the BPE tokenizer,goerch,2023-09-18T17:25:07Z,2023-10-03T07:16:26+00:00,20,349.86
3254,ggml-cuda : update rope implementation for parallel decoding,slaren,2023-09-18T21:51:51Z,2023-09-19T08:31:36+00:00,3,10.66
3257,Made README more readable with simplified language and ordered list,AayushSameerShah,2023-09-19T07:54:19Z,2024-02-19T14:36:39+00:00,3,3678.71
3258,CI: FreeBSD fix,alonfaraj,2023-09-19T09:45:37Z,2023-09-20T12:06:36+00:00,2,26.35
3262,flake: Restore default package's buildInputs #3261,tpdns90321,2023-09-19T14:42:07Z,2023-09-20T13:48:22+00:00,1,23.1
3272,"ggml-cuda : add rope f16, restore performance with parallel decoding",slaren,2023-09-19T22:26:44Z,2023-09-20T11:00:28+00:00,5,12.56
3273,Make CMake LLAMA_NATIVE flag actually use the instructions supported by the processor,netrunnereve,2023-09-20T00:21:59Z,2023-10-03T16:53:16+00:00,12,328.52
3277,benchmark-matmult : do not use integer abs() on a float,cebtenzzre,2023-09-20T14:41:00Z,2023-09-20T16:06:08+00:00,1,1.42
3283,ggml_tensor: update the structure comments.,huajsj,2023-09-20T19:55:19Z,2023-09-28T20:06:19+00:00,2,192.18
3289,Update README.md,leedrake5,2023-09-20T23:52:06Z,2023-09-21T19:00:25+00:00,2,19.14
3290,Updated make-ggml.py compatibility with more models and GGUF,richardr1126,2023-09-21T01:25:11Z,2023-09-27T16:25:13+00:00,1,159.0
3292,Release the requested thread pool resource,yancaoweidaode,2023-09-21T03:50:11Z,2023-09-28T19:51:53+00:00,5,184.03
3296,infill mode for main and server,vvhg1,2023-09-21T08:45:38Z,2023-10-02T07:42:02+00:00,1,262.94
3299,Try to fix Baichuan2 models by using vocab size in config.json,KerfuffleV2,2023-09-21T15:54:22Z,2023-10-04T14:20:28+00:00,1,310.44
3300,ggml-opencl.cpp: Make private functions static,shibe2,2023-09-21T16:47:34Z,2023-09-21T18:10:26+00:00,1,1.38
3301,llama.cpp : split llama_context_params into model and context params,slaren,2023-09-21T17:36:23Z,2023-09-28T19:42:39+00:00,3,170.1
3309,Fix build-info on MSVC.,dranger003,2023-09-22T13:55:30Z,2023-09-25T22:45:33+00:00,5,80.83
3311,Multithreaded CI builds,netrunnereve,2023-09-23T00:15:28Z,2023-09-28T19:31:05+00:00,7,139.26
3315,examples : fix RoPE defaults to match PR #3240,cebtenzzre,2023-09-23T04:59:02Z,2023-09-23T09:28:50+00:00,1,4.5
3317,llama-bench : add README,slaren,2023-09-23T13:02:13Z,2023-09-23T19:48:25+00:00,2,6.77
3329,add refact model,ds5t5,2023-09-25T04:37:17Z,2023-10-04T13:23:39+00:00,5,224.77
3330,Fix CLBlast_DIR var.,fossilet,2023-09-25T08:42:10Z,2023-09-25T18:24:53+00:00,1,9.71
3333,lora : add support for non-llama models,slaren,2023-09-25T23:39:30Z,2023-12-16T17:58:46+00:00,1,1962.32
3341,gguf : fix a few general keys,cebtenzzre,2023-09-26T15:35:15Z,2023-09-27T16:18:07+00:00,4,24.71
3346,gguf : basic type checking in gguf_get_*,cebtenzzre,2023-09-26T17:03:40Z,2023-09-28T18:30:32+00:00,1,49.45
3347,gguf : make token scores and types optional,cebtenzzre,2023-09-26T17:57:31Z,2023-09-28T18:30:16+00:00,1,48.55
3362,Added the fact that llama.cpp supports Mistral AI release 0.1,paschembri,2023-09-27T12:36:47Z,2023-09-28T12:13:38+00:00,2,23.61
3364,remove bug in convert.py permute function,jzhang38,2023-09-27T13:40:32Z,2023-09-27T18:45:20+00:00,1,5.08
3370,ggml-cuda : perform cublas fp16 matrix multiplication as fp16,slaren,2023-09-27T18:05:50Z,2023-09-28T10:08:28+00:00,2,16.04
3375,docs : mark code as Bash,kevinji,2023-09-28T05:42:48Z,2023-09-28T13:11:32+00:00,1,7.48
3387,swift : fix build,jhen0409,2023-09-29T01:39:40Z,2023-09-29T05:25:13+00:00,1,3.76
3392,train : fix KQ_pos allocation,ggerganov,2023-09-29T08:52:41Z,2023-09-29T16:05:19+00:00,1,7.21
3400,llama : fix session saving/loading,ggerganov,2023-09-29T12:49:50Z,2023-10-03T18:04:01+00:00,1,101.24
3401,llama.cpp : add documentation about rope_freq_base and scale values,slaren,2023-09-29T15:02:22Z,2023-09-29T16:42:33+00:00,1,1.67
3402,CLBlast: Support broadcasting for matrix multiplication and GQA,shibe2,2023-09-29T18:06:12Z,2023-10-02T19:26:16+00:00,2,73.33
3403,ggml-cuda : explicitly use cmath for log2,slaren,2023-09-29T20:19:40Z,2024-02-19T14:36:05+00:00,3,3426.27
3406,metal : fix hardcoded constant in mul_vec_q_n_f32,cebtenzzre,2023-09-29T22:24:32Z,2023-10-04T03:53:55+00:00,1,101.49
3408,"gguf : add BERT, MPT, and GPT-J arch info",cebtenzzre,2023-09-29T22:35:24Z,2023-10-02T19:20:29+00:00,1,68.75
3409,gguf : general usability improvements,cebtenzzre,2023-09-29T22:55:13Z,2023-10-02T18:58:46+00:00,4,68.06
3410,Support Adept Persimmon 8b,phillip-kravtsov,2023-09-29T22:55:16Z,2023-10-07T07:12:43+00:00,14,176.29
3412,ggml-cuda : perform cublas mat mul of quantized types as f16,slaren,2023-09-30T10:35:38Z,2023-09-30T16:12:57+00:00,1,5.62
3416,Load parallel.cpp -f file.txt external prompt file,pudepiedj,2023-09-30T16:25:45Z,2023-10-06T13:16:38+00:00,16,140.85
3417,MPT support in llama.cpp,jploski,2023-09-30T17:16:24Z,2023-10-10T07:50:23+00:00,33,230.57
3418,llama : expose model's rope_freq_scale in the API,grencez,2023-09-30T21:43:14Z,2023-10-03T17:09:28+00:00,1,67.44
3419,Windows XP: support MinGW 8.1.0,guilt,2023-09-30T23:57:30Z,2024-01-14T08:41:44+00:00,12,2528.74
3420,cmake : make CUDA flags more similar to the Makefile,cebtenzzre,2023-10-01T02:03:56Z,2023-10-02T13:16:50+00:00,1,35.22
3421,convert : fix vocab size when not defined in hparams,cebtenzzre,2023-10-01T03:34:11Z,2023-10-02T22:07:25+00:00,2,42.55
3425,Consistent prefix/suffix coloring,h-h-h-h,2023-10-01T10:03:56Z,2023-10-03T18:16:16+00:00,5,56.21
3426,[metal] alibi for arbitrary number of heads,li-plus,2023-10-01T16:51:24Z,2023-10-03T16:55:21+00:00,1,48.07
3427,Set metal log callback before initializing metal.,phronmophobic,2023-10-01T19:22:21Z,2023-10-02T10:50:00+00:00,1,15.46
3436,Implement multimodal models (LLaVA),monatis,2023-10-02T11:23:02Z,2023-10-12T15:23:18+00:00,3,244.0
3446,"hparam comparison uses memcpy for floating point comparison, which is in general a bad idea",l3utterfly,2023-10-02T19:35:29Z,2023-10-06T10:47:59+00:00,9,87.21
3447,CLBlast: Fix handling of on-device tensor data,shibe2,2023-10-02T20:11:42Z,2023-10-05T14:25:23+00:00,1,66.23
3448,convert : update Falcon script for new HF config,cebtenzzre,2023-10-02T22:10:08Z,2023-10-05T19:00:34+00:00,2,68.84
3453,Added RISC-V Vector Support for K-Quants and improved the existing intrinsics,Tameem-10xE,2023-10-03T12:11:53Z,2023-10-03T18:38:19+00:00,1,6.44
3455,Workaround for #3454,goerch,2023-10-03T14:24:03Z,2023-10-07T04:57:01+00:00,1,86.55
3461,Process escape sequences in reverse prompts,staviq,2023-10-03T21:57:11Z,2023-10-05T16:17:30+00:00,1,42.34
3480,server : fix incorrect num_tokens_predicted,jhen0409,2023-10-05T00:46:43Z,2023-10-05T14:02:55+00:00,1,13.27
3481,swift : disable ACCELERATE_NEW_LAPACK,jhen0409,2023-10-05T00:51:15Z,2023-10-05T14:00:08+00:00,1,13.15
3482,ci : add swift build via xcodebuild,jhen0409,2023-10-05T00:58:56Z,2023-10-05T13:56:21+00:00,1,12.96
3490,PoC: server handling multiple clients with custom attention mask api,FSSRepo,2023-10-05T19:14:23Z,2023-10-22T19:54:00+00:00,4,408.66
3491,ci : fix xcodebuild destinations,jhen0409,2023-10-05T20:03:46Z,2023-10-06T10:36:44+00:00,1,14.55
3493,kv cache slot search improvements,KerfuffleV2,2023-10-05T22:15:38Z,2023-10-06T16:10:13+00:00,4,17.91
3494,server : reuse llama_sample_token common util,jhen0409,2023-10-05T23:27:54Z,2023-10-06T12:44:24+00:00,1,13.28
3499,fix comments about k-quant block sizing,jrudolph,2023-10-06T10:51:51Z,2023-10-08T10:21:19+00:00,1,47.49
3506,Server docs: fix default values and add n_probs,Mihaiii,2023-10-06T15:41:40Z,2023-10-06T18:39:33+00:00,1,2.96
3508,fixes infill incorrect tokenization,vvhg1,2023-10-06T17:06:54Z,2023-10-10T07:31:21+00:00,8,86.41
3521,quantize : fail fast on write errors,cebtenzzre,2023-10-07T03:15:20Z,2023-10-07T08:41:52+00:00,1,5.44
3522,metal : support default.metallib load & reuse code for swift package,jhen0409,2023-10-07T04:46:58Z,2023-10-07T08:40:27+00:00,1,3.89
3535,Fix missing break in Persimmon arch case statements,KerfuffleV2,2023-10-07T23:03:48Z,2023-10-08T05:22:18+00:00,2,6.31
3538,tokenizer : special token handling,staviq,2023-10-08T00:52:52Z,2023-10-17T15:11:01+00:00,22,230.3
3543,Fix mirostat state when using multiple sequences,KerfuffleV2,2023-10-08T10:02:33Z,2023-10-11T19:35:46+00:00,14,81.55
3544,CLBlast: Fix matrix-vector multiplication,shibe2,2023-10-08T12:41:55Z,2023-10-12T19:59:47+00:00,1,103.3
3548,sync : ggml (ggml-backend),ggerganov,2023-10-08T14:39:04Z,2023-10-08T17:19:14+00:00,4,2.67
3552,Enable llama.cpp on s390x big endian platform,chenqiny,2023-10-09T03:27:57Z,2023-10-20T11:19:40+00:00,24,271.86
3553,feat: Support bloom models,xingchensong,2023-10-09T07:46:42Z,2023-10-10T14:48:22+00:00,1,31.03
3557,add PLaMo model,okdshin,2023-10-09T13:42:44Z,2023-12-24T13:35:49+00:00,4,1823.88
3560,Minor code fixes and optimizations,GermanAizek,2023-10-09T16:28:00Z,2023-10-20T10:02:14+00:00,6,257.57
3562,fix swift package + add example swift implementation of batched api,zshannon,2023-10-09T22:01:59Z,2023-10-11T11:14:06+00:00,6,37.2
3567,Minor improvements in GPT2 tokenizer,goerch,2023-10-10T05:34:23Z,2023-10-10T16:59:52+00:00,1,11.42
3571,fixing tiny resource leak when reading /proc/cpuinfo on Linux,tpltnt,2023-10-10T16:03:55Z,2023-10-29T00:25:32+00:00,4,440.36
3582,server : add completion mode (no chat),akx,2023-10-11T13:03:36Z,2023-10-12T06:51:53+00:00,1,17.8
3584,"Adding -tb, --threads-batch to server.cpp",m18coppola,2023-10-11T16:34:04Z,2023-10-11T19:42:22+00:00,1,3.14
3585,Ignore tokens if their IDs in added token list are below the vocab's base size,seungduk-yanolja,2023-10-11T17:54:56Z,2023-10-28T12:03:31+00:00,3,402.14
3586,StableLM support,Galunid,2023-10-11T20:14:07Z,2023-11-14T10:17:12+00:00,15,806.05
3592,typo: it is `--n-gpu-layers` not `--gpu-layers`,ianscrivener,2023-10-12T03:34:51Z,2023-10-12T11:10:50+00:00,1,7.6
3601,sampling : one sequence per sampling context,ggerganov,2023-10-12T17:37:41Z,2023-10-16T09:46:39+00:00,1,88.15
3603,CLBlast: Fix temporary buffer size for f16 conversion (wsize),shibe2,2023-10-12T20:16:44Z,2023-10-17T17:02:31+00:00,2,116.76
3605,ggml : add context enumeration functions,slaren,2023-10-12T21:46:43Z,2023-10-13T10:23:10+00:00,1,12.61
3613,Expose Llava as a shared library for downstream projects,damian0815,2023-10-13T10:24:11Z,2023-11-06T21:36:23+00:00,23,587.2
3615,"Update hot-topics & models, detail windows release in usage",BarfingLemurs,2023-10-13T13:39:42Z,2023-10-17T18:13:21+00:00,4,100.56
3618,train-text-from-scratch : fix assert failure in ggml-alloc,slaren,2023-10-13T19:16:39Z,2023-10-17T17:00:58+00:00,1,93.74
3621,Fix Cuda offloading in llava,monatis,2023-10-14T00:16:01Z,2023-10-14T10:52:44+00:00,1,10.61
3623,escaping prompt for cfg_negative_prompt and consecutive prompts in main with interactive,vvhg1,2023-10-14T09:15:35Z,2023-10-22T18:09:51+00:00,11,200.9
3627,MPT : support GQA for replit-code-v1.5,cebtenzzre,2023-10-14T22:08:02Z,2023-10-15T06:32:06+00:00,1,8.4
3631,Address undeclared identifiers by adding conditional compilation,danemadsen,2023-10-15T09:50:11Z,2023-10-17T11:18:53+00:00,1,49.48
3632,Server.cpp: Documentation of JSON return value of /completion endpoint,coezbek,2023-10-15T10:04:43Z,2023-10-17T16:51:02+00:00,1,54.77
3633,support loading vocab from fast tokenizer config in convert.py,strutive07,2023-10-15T10:17:51Z,2023-12-14T08:09:34+00:00,21,1437.86
3635,Validate special token ids are in range when loading GGUF model,KerfuffleV2,2023-10-15T13:35:23Z,2023-10-22T18:14:56+00:00,20,172.66
3639,Server.cpp loadPrompt(): Fix segfault when prompt length exceeds ctx size,coezbek,2023-10-15T20:50:05Z,2023-10-18T06:19:56+00:00,1,57.5
3645,llava : fix tokenization to not add bos after system prompt,ggerganov,2023-10-16T18:33:31Z,2023-10-16T20:58:00+00:00,2,2.41
3648,metal : implement q5_0 and q5_1 kernels,jhen0409,2023-10-17T04:00:36Z,2023-10-18T12:21:48+00:00,5,32.35
3656,OpenCL: Fix element-wise multiplication,shibe2,2023-10-17T17:48:31Z,2023-10-18T12:09:23+00:00,1,18.35
3657,fix embeddings when using CUDA,slaren,2023-10-17T18:36:51Z,2023-10-17T20:24:50+00:00,1,1.8
3669,CLBlast: Add outer loops over src0 for broadcasting in mulmat,shibe2,2023-10-18T19:36:16Z,2023-10-20T18:30:53+00:00,1,46.91
3674,llava: Better error handling to avoid segfaults for non-existant CLIP models,monatis,2023-10-19T05:56:38Z,2023-10-19T13:59:12+00:00,1,8.04
3677,server : parallel decoding and multimodal (cont),ggerganov,2023-10-19T10:55:31Z,2023-10-22T19:53:08+00:00,11,80.96
3680,convert : restore compat with old Falcon models,cebtenzzre,2023-10-19T12:58:09Z,2023-10-20T05:32:09+00:00,1,16.57
3682,multimodal : add BakLLava conversion support,monatis,2023-10-19T14:46:57Z,2023-10-19T16:40:41+00:00,1,1.9
3706,server : fix multibyte handle in partial response (to server-rev branch),jhen0409,2023-10-21T03:23:09Z,2023-10-21T11:58:04+00:00,1,8.58
3715,Allow caller to handle help/argument exceptions,bandoti,2023-10-21T17:43:36Z,2023-11-01T17:42:01+00:00,1,263.97
3720,remove token functions with `context` args in favor of `model`,MarcusDunn,2023-10-21T23:09:19Z,2023-10-23T19:40:04+00:00,3,44.51
3727,server : allow to specify custom prompt for penalty calculation,z80maniac,2023-10-22T09:55:53Z,2023-12-23T09:31:50+00:00,2,1487.6
3728,Add test for MPT tokenization,goerch,2023-10-22T15:21:31Z,2023-10-22T19:21:43+00:00,8,4.0
3734,"server support for system, prefix, and suffix prompts with special to…",unknown,2023-10-23T02:32:40Z,2023-11-14T19:58:23+00:00,4,545.43
3739,Fix baichuan convert script not detecting model,Galunid,2023-10-23T08:49:33Z,2023-10-23T15:47:03+00:00,1,6.96
3742,Add more tokenizer tests,Galunid,2023-10-23T11:20:21Z,2023-10-24T07:17:17+00:00,6,19.95
3746,Update special token handling in conversion scripts for gpt2 derived tokenizers,Galunid,2023-10-23T14:59:45Z,2023-10-23T19:46:01+00:00,1,4.77
3747,Allow quantizing k-quants to fall back when tensor size incompatible,KerfuffleV2,2023-10-23T15:36:35Z,2023-10-28T11:54:24+00:00,1,116.3
3748,issue templates: Separate bug and enhancement template + no default title,monatis,2023-10-23T16:06:38Z,2023-10-23T19:57:16+00:00,1,3.84
3749,cuda : add batched cuBLAS GEMM for faster attention,ggerganov,2023-10-23T18:21:43Z,2023-10-24T13:48:37+00:00,4,19.45
3760,Fix detokenization of non-special added-tokens,goerch,2023-10-24T11:50:54Z,2024-01-13T16:28:23+00:00,2,1948.62
3762,finetune: Add -ngl parameter,AndrewGodfrey,2023-10-24T15:55:31Z,2023-11-01T11:49:04+00:00,6,187.89
3787,log.h: make generating separate log files optional.,staviq,2023-10-25T21:42:50Z,2023-11-01T14:18:27+00:00,3,160.59
3793,Try cwd for ggml-metal.metal if bundle lookup fails,akx,2023-10-26T10:35:15Z,2023-10-28T12:43:02+00:00,3,50.13
3797,flake : update flake.lock for newer transformers version + provide extra dev shell,Green-Sky,2023-10-26T14:11:47Z,2023-10-28T14:41:08+00:00,1,48.49
3803,Fix a segfault with simple.cpp,tterrasson,2023-10-26T21:46:38Z,2023-10-27T14:37:41+00:00,1,16.85
3812,speculative: Ensure draft and target model vocab matches,KerfuffleV2,2023-10-27T12:55:17Z,2023-10-27T21:40:08+00:00,5,8.75
3813,llama : add option for greedy sampling with probs,ggerganov,2023-10-27T13:13:20Z,2023-10-28T11:23:11+00:00,5,22.16
3814,cuda : improve multi-GPU performance using cuBLAS,ggerganov,2023-10-27T14:11:50Z,2024-01-13T16:19:51+00:00,1,1874.13
3816,cuda : speed-up by using CUBLAS_COMPUTE_32F instead of CUBLAS_COMPUTE_16F,ggerganov,2023-10-27T15:46:41Z,2024-03-18T07:52:25+00:00,2,3424.1
3818,llama : correctly report GGUFv3 format,cebtenzzre,2023-10-27T17:12:44Z,2023-10-27T21:33:53+00:00,1,4.35
3837,llama : refactor graph build code,ggerganov,2023-10-28T16:55:35Z,2023-11-01T06:04:02+00:00,6,85.14
3838,Generalize convert scripts,Galunid,2023-10-28T17:30:01Z,2023-11-09T10:09:29+00:00,16,280.66
3841,Min P sampler implementation [alternative to Top P/Top K],kalomaze,2023-10-28T22:28:58Z,2023-10-31T19:44:49+00:00,72,69.26
3842,make : remove unnecessary dependency on build-info.h,cebtenzzre,2023-10-29T01:38:11Z,2023-10-29T16:33:47+00:00,1,14.93
3843,Extend llama_kv_cache_seq_rm to allow matching any sequence,KerfuffleV2,2023-10-29T03:16:08Z,2023-10-29T17:31:41+00:00,5,14.26
3848,llama : add llm_build helper functions,ggerganov,2023-10-29T13:46:06Z,2023-10-31T17:23:13+00:00,8,51.62
3853,flake.nix: fix for rocm 5.7,Tungsten842,2023-10-29T21:45:40Z,2023-10-31T17:24:04+00:00,3,43.64
3861,ggml : move FP16 <-> FP32 stuff to ggml-impl.h,ggerganov,2023-10-30T14:40:24Z,2023-10-30T17:19:16+00:00,3,2.65
3871,"gguf : track writer state, free unneeded tensors, cleanup",cebtenzzre,2023-10-31T16:09:57Z,2023-11-07T17:43:04+00:00,4,169.55
3872,"added docs about `model`, `context`",MarcusDunn,2023-10-31T17:22:04Z,2023-11-25T01:28:00+00:00,1,584.1
3873,Enable sigint handler even when not in interactive mode,jaggzh,2023-10-31T19:13:37Z,2024-02-11T13:37:29+00:00,1,2466.4
3876,"server : re-enable completion and embedded at the same time, fixes #3815",a-h,2023-10-31T22:31:58Z,2023-11-01T09:28:28+00:00,1,10.94
3877,Update server.cpp with min_p after it was introduced in #3841,Mihaiii,2023-10-31T22:48:52Z,2023-11-09T02:00:34+00:00,1,195.19
3879,build : link against build info instead of compiling against it,cebtenzzre,2023-11-01T01:12:19Z,2023-11-02T06:50:16+00:00,6,29.63
3882,cuda : do not use batched GEMM when tensor cores are not available,ggerganov,2023-11-01T09:54:31Z,2023-11-02T06:35:11+00:00,2,20.68
3885,null grammar field after reset,l3utterfly,2023-11-01T11:56:21Z,2023-11-01T13:40:44+00:00,1,1.74
3891,ggml-cuda : compute ptrs for cublasGemmBatchedEx in a kernel,slaren,2023-11-01T18:32:16Z,2023-11-01T22:10:09+00:00,2,3.63
3895,Fix ROCM build by relaxing constness,KerfuffleV2,2023-11-02T01:27:39Z,2023-11-02T18:32:33+00:00,1,17.08
3897,cuda : fix RoPE after #2268,cebtenzzre,2023-11-02T04:35:28Z,2023-11-02T05:49:44+00:00,1,1.24
3903,CUDA memory pool with async memory allocation/deallocation,young-developer,2023-11-02T10:33:52Z,2023-11-02T17:10:40+00:00,4,6.61
3908,gguf : print error for GGUFv1 files,ggerganov,2023-11-02T12:31:58Z,2023-11-02T14:22:30+00:00,2,1.84
3912,sync : ggml (backend v2),ggerganov,2023-11-02T16:08:41Z,2023-11-13T12:16:23+00:00,13,260.13
3913,cuda : fix const ptrs warning causing ROCm build issues,ggerganov,2023-11-02T17:16:12Z,2023-11-02T18:32:12+00:00,1,1.27
3920,build : fix build info generation and cleanup Makefile,cebtenzzre,2023-11-02T21:57:27Z,2023-11-30T22:23:09+00:00,1,672.43
3921,ggml-cuda : move row numbers to x grid dim in mul mat vec kernels,slaren,2023-11-02T23:08:03Z,2023-11-03T11:13:09+00:00,2,12.09
3922,llama : change yarn_ext_factor placeholder to -1,cebtenzzre,2023-11-03T02:00:51Z,2023-11-03T06:31:58+00:00,1,4.52
3923,MSVC instruction detection (fixed up #809),netrunnereve,2023-11-03T03:19:37Z,2023-11-05T09:03:09+00:00,2,53.73
3928,Allow common process_escapes to handle \x sequences,KerfuffleV2,2023-11-03T11:24:16Z,2023-11-05T17:06:06+00:00,5,53.7
3931,Check CUDA memory pool support,young-developer,2023-11-03T12:41:22Z,2023-11-04T17:58:19+00:00,3,29.28
3943,gguf-py: Support 01.AI Yi models,KerfuffleV2,2023-11-04T18:40:03Z,2023-11-04T22:20:34+00:00,1,3.68
3944,Revert CUDA pool stuff,slaren,2023-11-04T20:29:11Z,2023-11-05T07:12:13+00:00,1,10.72
3945,feat: mark LLM_ARCH_STARCODER as full offload supported,wsxiaoys,2023-11-04T22:11:29Z,2023-11-05T12:40:08+00:00,1,14.48
3946,supports running on CPU for GGML_USE_CUBLAS=ON build,wsxiaoys,2023-11-04T22:15:44Z,2023-11-07T06:49:08+00:00,13,56.56
3950,server : allow continue edit on completion mode,jhen0409,2023-11-05T02:50:33Z,2023-11-10T22:49:33+00:00,1,139.98
3951,"cuda : fix disabling device with --tensor-split 1,0",cebtenzzre,2023-11-05T04:47:59Z,2023-11-05T15:08:57+00:00,3,10.35
3952,feat(ci): add an option to fail on compile warning,ananta,2023-11-05T05:15:52Z,2024-02-17T21:03:15+00:00,11,2511.79
3963,build: support ppc64le build for make and CMake,bufferoverflow,2023-11-05T17:43:55Z,2023-11-17T16:11:24+00:00,2,286.46
3967,Convert fixes,lorddoskias,2023-11-06T11:07:47Z,2024-01-13T16:38:46+00:00,8,1637.52
3970,CMake: fix issue with version info not getting baked into LlamaConfig.cmake,bandoti,2023-11-06T17:44:58Z,2023-11-27T19:25:43+00:00,2,505.68
3972,fix oai proxy,rhjdvsgsgks,2023-11-06T17:55:29Z,2023-11-30T20:50:40+00:00,2,578.92
3974,Fix backward rope after YaRN,xaedes,2023-11-06T22:58:58Z,2023-11-07T08:04:51+00:00,2,9.1
3977,make : do not add linker flags when compiling static llava lib,ggerganov,2023-11-07T08:38:12Z,2023-11-07T17:25:33+00:00,2,8.79
3981,gguf-py: Refactor and allow reading/modifying existing GGUF files,KerfuffleV2,2023-11-07T21:02:32Z,2023-11-11T05:04:51+00:00,48,80.04
3982,ggml-alloc : fix backend assignments of views,slaren,2023-11-07T23:08:25Z,2023-11-08T12:15:15+00:00,1,13.11
3986,Align the tokenized result between deepseek coder python model and gguf model,DOGEwbx,2023-11-08T03:52:31Z,2023-11-13T03:58:16+00:00,3,120.1
3996,server : fix crash when prompt exceeds context size,z80maniac,2023-11-08T18:42:24Z,2023-11-11T05:48:22+00:00,1,59.1
4002,adding in include needed,jmikedupont2,2023-11-09T07:46:08Z,2023-11-09T08:53:18+00:00,1,1.12
4003,Feature/save temps,jmikedupont2,2023-11-09T07:48:10Z,2023-12-12T09:44:29+00:00,2,793.94
4010,Unbreak persimmon after #3837,Galunid,2023-11-09T15:38:20Z,2023-11-10T13:24:55+00:00,1,21.78
4013,llama : add functions to get the model's metadata,slaren,2023-11-09T23:55:47Z,2023-11-17T15:17:37+00:00,3,183.36
4024,Make hipBLAS CMake more similar to cuBLAS,ardfork,2023-11-10T15:30:09Z,2024-02-19T14:18:04+00:00,1,2422.8
4025,Fix incorrect prompt tokenization in speculative example,AutonomicPerfectionist,2023-11-10T16:07:44Z,2023-11-20T09:50:04+00:00,5,233.71
4032,typos,richardkiss,2023-11-11T05:41:04Z,2023-11-12T06:04:58+00:00,2,24.4
4037,Fix gguf-convert-endian script,monatis,2023-11-11T10:21:52Z,2023-11-11T15:35:32+00:00,2,5.23
4039,examples : Add tokenize,zakkor,2023-11-11T16:32:29Z,2023-11-17T15:36:44+00:00,1,143.07
4040,Respect tokenizer.ggml.add_bos_token value when tokenizing,KerfuffleV2,2023-11-11T20:20:53Z,2023-11-17T02:14:37+00:00,1,125.9
4041,Add ReLU and SQR CUDA ops to fix Persimmon offloading,KerfuffleV2,2023-11-11T22:53:22Z,2023-11-13T08:58:15+00:00,3,34.08
4043,convert.py safetensors updates,afrideva,2023-11-12T04:27:52Z,2023-11-14T01:03:41+00:00,10,44.6
4046,main: Add ChatML functionality to main example,Sebby37,2023-11-12T10:39:15Z,2023-11-20T13:56:59+00:00,2,195.3
4052,Fix MacOS Sonoma model quantization,TortoiseHam,2023-11-12T23:14:26Z,2023-11-14T17:34:42+00:00,8,42.34
4056,llava : fix regression for square images in #3613,monatis,2023-11-13T08:57:37Z,2023-11-13T15:20:52+00:00,2,6.39
4069,Fix compilation warning that fread return value is not used,huawei-lin,2023-11-14T03:58:35Z,2023-11-17T15:22:56+00:00,2,83.41
4074,Moving number of gpu layers argument parsing to common/train.cpp,jpodivin,2023-11-14T15:02:28Z,2023-11-17T15:19:16+00:00,2,72.28
4079,"Use BLAS to implement ggml_compute_forward_out_prod_f32 for matrix src0, src1 (finetuning speedup ~5x).",gwjr,2023-11-14T17:00:45Z,2023-11-17T14:48:20+00:00,1,69.79
4080,Improve yaml log escaping,joennlae,2023-11-14T18:20:45Z,2023-11-17T15:24:08+00:00,2,69.06
4081,llama : restore prefix space in llama tokenizer,cebtenzzre,2023-11-14T22:25:01Z,2023-11-15T16:34:47+00:00,1,18.16
4082,finetune : zero the loraB initial vectors,AndrewGodfrey,2023-11-15T00:50:01Z,2023-11-17T10:23:12+00:00,4,57.55
4084,ggml-cuda : increase max graph size,slaren,2023-11-15T08:27:54Z,2023-11-15T12:58:13+00:00,1,4.51
4092,feat: Allow overriding GGUF metadata when loading model,KerfuffleV2,2023-11-15T19:00:23Z,2023-12-05T17:19:18+00:00,15,478.32
4095,Fix #4017,AndrewGodfrey,2023-11-16T02:18:09Z,2023-11-17T08:01:16+00:00,2,29.72
4100,gguf: fix potential infinite loops while parsing,texmex76,2023-11-16T12:46:03Z,2023-11-16T15:01:49+00:00,1,2.26
4101,llama : fix data units,ggerganov,2023-11-16T15:21:30Z,2023-11-17T08:00:15+00:00,1,16.65
4104,Falcon HF compatibility,cmp-nct,2023-11-16T22:26:11Z,2023-11-17T15:24:31+00:00,1,16.97
4115,llama : increase max nodes,slaren,2023-11-17T16:36:57Z,2023-11-17T19:39:11+00:00,1,3.04
4118,fix deepseek bug in stream mode,qhduan,2023-11-17T22:21:58Z,2023-12-05T14:09:54+00:00,3,423.8
4122,Update README.md for ROCm Windows support,jammm,2023-11-18T05:55:31Z,2023-11-20T15:02:46+00:00,2,57.12
4124,Clean up ggml-cuda.cu warnings when compiling with clang (for ROCM),KerfuffleV2,2023-11-18T11:28:45Z,2023-11-18T15:11:18+00:00,3,3.71
4125,gguf-py : export chat templates,slaren,2023-11-18T14:12:04Z,2023-11-19T10:10:52+00:00,7,19.98
4126,fix: tokenize example: Respect normal add BOS token behavior,KerfuffleV2,2023-11-18T15:27:17Z,2023-11-18T21:48:17+00:00,1,6.35
4127,Remove missed baichuan convert script,Galunid,2023-11-18T18:41:27Z,2023-11-18T20:08:33+00:00,1,1.45
4128,Add flag for gpu layers in finetune.cpp,csaben,2023-11-18T19:19:56Z,2023-11-19T16:56:38+00:00,2,21.61
4129,Add flake8 to github actions (python linting),Galunid,2023-11-18T20:44:35Z,2023-11-20T10:35:48+00:00,1,37.85
4131,server : relay error messages,SoftwareRenderer,2023-11-19T00:33:06Z,2023-11-19T16:54:11+00:00,1,16.35
4133,Fix incorrect format strings and uninitialized variables.,haohui,2023-11-19T04:53:33Z,2023-11-23T21:56:53+00:00,9,113.06
4135,fix: readme,vodkaslime,2023-11-19T09:25:07Z,2023-11-30T21:49:21+00:00,4,276.4
4153,stablelm : simplify + speedup generation,Galunid,2023-11-21T10:02:57Z,2023-11-21T15:22:30+00:00,1,5.33
4155,WIP Feature/cpp,jmikedupont2,2023-11-21T16:03:45Z,2023-11-25T12:43:19+00:00,1,92.66
4156,ggml-cuda : support stablelm rope,slaren,2023-11-21T16:51:25Z,2023-11-24T17:04:31+00:00,5,72.22
4159,iOS example with swift ui,bachittle,2023-11-22T03:25:32Z,2023-11-27T14:56:52+00:00,5,131.52
4160,Server: OpenAI-compatible POST /v1/chat/completions API endpoint,kir-gadjello,2023-11-22T05:34:21Z,2023-11-24T17:14:51+00:00,7,59.67
4170,llama : KV cache view API + better KV cache management,ggerganov,2023-11-22T15:18:16Z,2023-11-23T17:07:57+00:00,6,25.83
4172,ShareGPT4V compatibility (vision encoder only loading),cmp-nct,2023-11-22T18:26:02Z,2023-11-30T22:11:14+00:00,1,195.75
4173,convert : fix tensors using grad in some models,Galunid,2023-11-22T20:33:09Z,2023-11-24T14:02:50+00:00,1,41.49
4180,Allow exporting a view of the KV cache,KerfuffleV2,2023-11-23T10:34:23Z,2023-11-23T16:31:20+00:00,4,5.95
4189,Update docs for yarn_ext_factor <0.0 as unspecified instead of NaN,crasm,2023-11-23T20:34:03Z,2023-11-25T15:47:07+00:00,3,43.22
4195,Update README.md to use PATH for Windows ROCm,jammm,2023-11-24T06:34:43Z,2023-11-24T07:52:40+00:00,1,1.3
4198,server : OAI API compatibility,ggerganov,2023-11-24T08:53:36Z,2023-11-25T09:29:07+00:00,1,24.59
4202,Use mmap in torch load,Galunid,2023-11-24T14:21:59Z,2023-11-25T21:45:02+00:00,1,31.38
4203,Fixed ggml_tensor* b for ggml_add_or_set in GGML_OP_DIAG_MASK_INF,GermanAizek,2023-11-24T14:34:57Z,2023-12-22T09:26:49+00:00,3,666.86
4205,clip: enable gpu backend,FSSRepo,2023-11-24T17:07:40Z,2023-12-29T16:52:15+00:00,3,839.74
4209,Cleanup/Minimalization of C++ Reflection patch,jmikedupont2,2023-11-25T00:11:17Z,2023-12-12T09:44:29+00:00,1,417.55
4210,`reserve` space in `decode_utf8`,MarcusDunn,2023-11-25T01:36:09Z,2023-11-25T16:58:23+00:00,4,15.37
4213,Allow reusing results from `llama_token_to_piece` when sampling grammars,MarcusDunn,2023-11-25T04:59:32Z,2023-12-04T19:50:22+00:00,3,230.85
4220,metal: fix yarn,jxy,2023-11-25T21:43:28Z,2023-11-26T08:30:03+00:00,2,10.78
4231,ggml : fix -Warray-bounds warning with gcc,cebtenzzre,2023-11-27T02:51:48Z,2023-11-27T03:58:44+00:00,1,1.12
4232,Add single-client multi-prompt support,ziedbha,2023-11-27T03:46:42Z,2023-11-30T22:25:05+00:00,10,90.64
4236,Server UI improvements,mounta11n,2023-11-27T13:07:22Z,2024-06-11T06:19:06+00:00,2,4721.2
4242,ggml : restore abort() in GGML_ASSERT,cebtenzzre,2023-11-28T00:35:37Z,2023-11-28T09:51:12+00:00,4,9.26
4253,Fix typo in README.md,TortillaZHawaii,2023-11-29T13:21:47Z,2023-11-30T21:43:32+00:00,1,32.36
4256,ggml : add ggml_soft_max_ext,ggerganov,2023-11-29T15:19:33Z,2023-12-01T08:51:24+00:00,6,41.53
4261,Fix typical sampling.,tarcey,2023-11-30T00:02:02Z,2023-11-30T21:40:23+00:00,1,21.64
4272,Fix Apple clang determination bug.,unknown,2023-11-30T20:11:00Z,2023-11-30T22:23:45+00:00,1,2.21
4274,llama : sanity checks for access to logits,cebtenzzre,2023-11-30T21:22:37Z,2023-12-16T03:16:15+00:00,7,365.89
4275,build : enable libstdc++ assertions for debug builds,cebtenzzre,2023-11-30T23:19:56Z,2023-12-01T18:18:35+00:00,1,18.98
4277,build: add requirements file for convert-hf-to-gguf.py,danbev,2023-12-01T05:24:33Z,2023-12-01T09:41:57+00:00,1,4.29
4281,Merge qwen to llama cpp,simonJJJ,2023-12-01T09:26:19Z,2023-12-01T18:16:31+00:00,5,8.84
4283,Support attention_bias on LLaMA architecture,RealJosephus,2023-12-01T13:57:54Z,2023-12-01T18:17:06+00:00,6,4.32
4285,Samplers order parameters,MaggotHATE,2023-12-01T17:05:10Z,2023-12-05T10:05:51+00:00,5,89.01
4306,Check the full vocab for grammar only if necessary,kalomaze,2023-12-03T10:23:39Z,2023-12-23T09:27:07+00:00,8,479.06
4309,llama : per-layer KV cache,ggerganov,2023-12-03T15:57:37Z,2023-12-07T11:03:17+00:00,8,91.09
4311,Adding Tests for GGUFWriter Class,mendax0110,2023-12-03T19:31:30Z,2024-08-17T09:31:29+00:00,3,6182.0
4323,cmake: fix clang build when CUDA is enabled (#4208),jonppe,2023-12-04T13:38:27Z,2023-12-11T21:16:37+00:00,2,175.64
4332,Revert compiler checks for swift package,kchro3,2023-12-05T01:17:45Z,2023-12-05T07:29:47+00:00,1,6.2
4354,Use `typos` to fix comments and logs.,richardkiss,2023-12-07T05:34:48Z,2023-12-12T09:53:36+00:00,3,124.31
4359,"sync : ggml (new ops, tests, backend, etc.)",ggerganov,2023-12-07T11:54:27Z,2023-12-07T20:26:55+00:00,2,8.54
4362,Add Ava in the list of llama.cpp UIs,cztomsik,2023-12-07T15:48:11Z,2024-02-07T18:44:52+00:00,2,1490.94
4373,Fix `ggml_metal_log` on Intel macs,finnvoor,2023-12-08T10:35:46Z,2023-12-21T19:55:02+00:00,1,321.32
4388,Update README.md (Add a dash before the item),y10ab1,2023-12-09T04:34:08Z,2023-12-10T22:27:39+00:00,1,41.89
4406,llama : add Mixtral support,slaren,2023-12-11T10:35:42Z,2023-12-13T12:04:25+00:00,2,49.48
4414,build : detect host compiler and cuda compiler separately,cebtenzzre,2023-12-11T20:13:58Z,2023-12-13T17:10:11+00:00,2,44.94
4433,Add `--version` option to show build info in CLI,yusiwen,2023-12-13T08:23:12Z,2023-12-13T12:50:15+00:00,1,4.45
4441,Implement optional API Key Authentication for Secure Server-Client Communication,ShadovvBeast,2023-12-13T14:20:59Z,2023-12-15T11:49:01+00:00,1,45.47
4456,Nomic Vulkan backend,cebtenzzre,2023-12-13T23:29:43Z,2024-01-29T20:50:51+00:00,40,1125.35
4461,"Fix ""not enough space in the context's memory pool"" error when loading certain models.",LostRuins,2023-12-14T08:19:21Z,2023-12-14T12:13:34+00:00,3,3.9
4462,Add ability to cancel model loading,crasm,2023-12-14T09:52:33Z,2023-12-22T06:19:36+00:00,21,188.45
4469,ggml : remove n_dims from ggml_tensor,slaren,2023-12-14T14:10:10Z,2023-12-14T15:52:08+00:00,1,1.7
4472,ggml : use ggml_row_size where possible,slaren,2023-12-14T16:46:57Z,2023-12-14T19:05:22+00:00,1,2.31
4480,ggml : group mul_mat_id rows by matrix (cpu only),slaren,2023-12-14T23:39:04Z,2023-12-15T11:45:50+00:00,1,12.11
4484,Prompt lookup decoding,LeonEricsson,2023-12-15T13:30:58Z,2023-12-22T16:05:56+00:00,1,170.58
4485,build : Check the ROCm installation location,Su3h7aM,2023-12-15T17:11:39Z,2023-12-17T15:23:33+00:00,1,46.2
4486,finetune : keep allocs alive until all allocations are done,slaren,2023-12-15T19:30:42Z,2023-12-17T15:05:56+00:00,1,43.59
4489,gguf-py : fail fast on nonsensical special token IDs,cebtenzzre,2023-12-15T23:11:47Z,2023-12-17T15:45:46+00:00,1,40.57
4490,Support for Phi-2,ebeyabraham,2023-12-16T00:50:50Z,2023-12-18T17:27:48+00:00,7,64.62
4500,Server: allow requests larger than 8K,mzcu,2023-12-16T14:08:26Z,2023-12-17T14:54:38+00:00,1,24.77
4506,Link to cublas dynamically on Windows even with LLAMA_STATIC,bullno1,2023-12-17T08:05:47Z,2023-12-17T10:57:33+00:00,2,2.86
4514,Correctly implement credentialed CORS for server,Azeirah,2023-12-17T20:22:07Z,2024-01-11T18:02:49+00:00,1,597.68
4519,Fix try_override for bool_value which always return true ignoring ove…,hankcs,2023-12-18T06:51:10Z,2023-12-18T13:14:59+00:00,1,6.4
4520,llama : initial ggml-backend integration,slaren,2023-12-18T09:26:26Z,2023-12-21T20:07:46+00:00,12,82.69
4538,CUDA: Faster Mixtral prompt processing,JohannesGaessler,2023-12-19T22:11:55Z,2023-12-20T14:41:22+00:00,7,16.49
4540,allowed getting n_batch from llama_context in c api,MarcusDunn,2023-12-19T23:12:49Z,2023-12-21T19:57:49+00:00,2,44.75
4541,add the parameter : --no-display-prompt,YannFollet,2023-12-20T03:30:39Z,2024-01-13T16:09:08+00:00,4,588.64
4552,"Consolidate support for Phi-1, Phi-1.5, and Phi-2 models",teleprint-me,2023-12-20T20:50:16Z,2024-01-09T21:07:05+00:00,4,480.28
4553,CUDA: faster Mixtral prompt processing for partial offloading,JohannesGaessler,2023-12-20T22:11:29Z,2023-12-21T17:42:59+00:00,5,19.52
4554,Fix access violation in ggml_cuda_free_data if tensor->extra is NULL,LoganDark,2023-12-20T22:36:32Z,2023-12-21T09:59:27+00:00,1,11.38
4555,Add gpt2 architecture integration,manikbhandari,2023-12-20T22:48:17Z,2023-12-28T14:03:57+00:00,5,183.26
4556,cuda : replace asserts in wrong architecture checks with __trap,slaren,2023-12-20T23:59:01Z,2023-12-21T17:02:30+00:00,6,17.06
4558,use LOG_TEE for stderr in ggml-cuda.cu,YannFollet,2023-12-21T02:40:17Z,2023-12-22T06:48:28+00:00,2,28.14
4560,fix old jetson compile error,FantasyGmm,2023-12-21T06:45:43Z,2023-12-22T15:11:13+00:00,10,32.42
4561,CUDA : Make the error message easier to understand,bobqianic,2023-12-21T11:54:00Z,2023-12-21T17:06:44+00:00,3,5.21
4562,Disabled per-tensor info prints on model load,JohannesGaessler,2023-12-21T13:57:46Z,2023-12-21T16:34:17+00:00,1,2.61
4567,Fixed default MSVS build,notecola,2023-12-21T17:17:24Z,2023-12-25T21:02:42+00:00,2,99.75
4573,ggml : change ggml_scale to take a float instead of tensor,ggerganov,2023-12-21T18:51:55Z,2023-12-21T21:20:49+00:00,1,2.48
4577,replaced all API facing `int`'s with `int32_t`,MarcusDunn,2023-12-21T21:40:12Z,2024-01-02T14:15:16+00:00,2,280.58
4578,llama : fix platforms without mmap,slaren,2023-12-21T23:02:53Z,2023-12-22T11:12:54+00:00,2,12.17
4579,ggml : extend `enum ggml_log_level` with new `GGML_LOG_LEVEL_DEBUG` value,bobqianic,2023-12-21T23:35:10Z,2023-12-22T06:47:01+00:00,1,7.2
4581,Add zig bindings to README.md,Deins,2023-12-22T03:24:15Z,2023-12-22T06:49:55+00:00,1,3.43
4584,tag docker image with build number,rhuddleston,2023-12-22T03:46:20Z,2023-12-22T06:56:34+00:00,3,3.17
4585,python: add check-requirements.sh and GitHub workflow,crasm,2023-12-22T06:33:26Z,2023-12-29T14:50:29+00:00,10,176.28
4586,Overhaul ci/run.sh and enable tests with openllama model files,crasm,2023-12-22T06:50:48Z,2024-01-26T12:18:01+00:00,2,845.45
4589,Add support for AVX VNNI ,tikikun,2023-12-22T07:51:13Z,2023-12-30T08:07:49+00:00,2,192.28
4593,"Add AWQ (Activation-aware Weight Quantization) for llama, llama2, mpt, and mistral models",namtranase,2023-12-22T09:48:22Z,2023-12-27T15:39:46+00:00,8,125.86
4594,CUDA: fixed row rounding for 0 tensor splits,JohannesGaessler,2023-12-22T09:55:56Z,2023-12-23T08:16:33+00:00,1,22.34
4600,Fix potential infinite for-loop,texmex76,2023-12-22T13:40:10Z,2024-01-13T16:06:20+00:00,1,530.44
4604,Update comment for AdamW implementation reference.,unknown,2023-12-22T22:17:41Z,2023-12-26T10:42:08+00:00,1,84.41
4605,flake.nix: rewrite,philiptaron,2023-12-22T22:31:36Z,2023-12-29T14:42:26+00:00,47,160.18
4606,cuda : improve cuda pool efficiency using virtual memory,slaren,2023-12-22T22:50:02Z,2023-12-24T13:34:22+00:00,16,38.74
4610,fallback to CPU buffer if host buffer alloc fails,slaren,2023-12-23T12:16:16Z,2023-12-23T15:10:51+00:00,1,2.91
4620,cuda : fix vmm pool with multi GPU,slaren,2023-12-24T16:55:50Z,2023-12-26T20:24:00+00:00,9,51.47
4630,ggml : fix dot product for ARM,ggerganov,2023-12-25T15:19:15Z,2023-12-27T09:02:13+00:00,1,41.72
4633,"flake.nix: add missing meta, including maintainers",SomeoneSerge,2023-12-25T17:47:40Z,2023-12-25T18:49:12+00:00,2,1.03
4635,Fix new CUDA10 compilation errors,FantasyGmm,2023-12-26T02:02:38Z,2023-12-26T10:38:37+00:00,1,8.6
4641,Add byte token type when tokenizer.model is not exists,strutive07,2023-12-26T10:28:01Z,2023-12-27T08:37:25+00:00,1,22.16
4657,Add n_key_dim and n_value_dim,postmasters,2023-12-28T01:21:14Z,2024-01-02T11:51:29+00:00,6,130.5
4665,Fix Compilation Issue in main-cmake-pkg example,andrijdavid,2023-12-28T13:16:49Z,2023-12-29T14:18:20+00:00,1,25.03
4668,Fix OpenAI server sampling w.r.t. temp and seed,jart,2023-12-28T15:30:18Z,2023-12-28T19:20:00+00:00,1,3.83
4669,Refactor llava-cli to use sampling library,jart,2023-12-28T15:48:35Z,2023-12-29T14:38:38+00:00,1,22.83
4671,Fix ld warning duplicate libraries libllama.a,nguoithichkhampha,2023-12-28T17:09:10Z,2023-12-29T14:39:15+00:00,1,21.5
4673,Replace sleep with condition variables in server,jart,2023-12-28T17:45:38Z,2023-12-29T14:24:12+00:00,1,20.64
4674,"llama.swiftui : fix infinite loop, ouput timings, buff UI",psugihara,2023-12-28T18:44:31Z,2023-12-29T13:58:56+00:00,1,19.24
4675,Fix OpenAI server sampling w.r.t. penalty.,sakura-umi,2023-12-28T19:44:30Z,2023-12-29T14:22:44+00:00,1,18.64
4681,Allow server to generate multimodal embeddings via the `/embedding` endpoint,kseth,2023-12-29T08:14:54Z,2023-12-29T14:22:10+00:00,1,6.12
4682,CUDA: fix tensor core logic for Pascal and HIP,JohannesGaessler,2023-12-29T11:28:52Z,2023-12-29T22:12:53+00:00,9,10.73
4696,clip : refactor + bug fixes,ggerganov,2023-12-30T07:31:24Z,2023-12-30T21:24:42+00:00,1,13.89
4697,CUDA: fixed tensor cores not being used on RDNA3,JohannesGaessler,2023-12-30T10:30:41Z,2023-12-30T12:52:01+00:00,1,2.36
4700,flake.nix: fix typo,eltociear,2023-12-30T15:25:23Z,2024-01-05T16:02:44+00:00,1,144.62
4709,workflows: nix-ci: init; build&cache flake outputs,SomeoneSerge,2023-12-30T20:29:08Z,2023-12-31T21:14:59+00:00,21,24.76
4733,finetune: fix typo in README.md,danbev,2024-01-02T08:04:23Z,2024-01-02T09:16:56+00:00,1,1.21
4735,llama : auto download HF models if URL provided,ggerganov,2024-01-02T11:23:01Z,2024-01-09T18:50:26+00:00,2,175.46
4736,ggml : include stdlib.h before intrin.h,ggerganov,2024-01-02T12:39:08Z,2024-01-04T08:12:27+00:00,1,43.56
4742,CUDA: faster softmax via shared memory + fp16 math,JohannesGaessler,2024-01-02T22:05:06Z,2024-01-09T07:58:55+00:00,1,153.9
4751,Print backend name on test-backend-ops failure,JohannesGaessler,2024-01-03T10:02:53Z,2024-01-04T08:43:24+00:00,1,22.68
4754,Fix metal backend init failure in swiftui example,singularity-s0,2024-01-03T10:59:24Z,2024-01-04T07:58:16+00:00,2,20.98
4756,finetune: remove unused includes,danbev,2024-01-03T12:41:57Z,2024-01-04T19:45:38+00:00,1,31.06
4758,train: fix typo in overlapping-samples help msg,danbev,2024-01-03T13:43:47Z,2024-01-03T17:53:41+00:00,1,4.17
4761,ggml : do not sched_yield when calling BLAS,ggerganov,2024-01-03T17:17:14Z,2024-01-05T13:18:21+00:00,3,44.02
4766,llama : ggml-backend integration,slaren,2024-01-04T01:55:43Z,2024-01-12T19:07:38+00:00,25,209.2
4767,SwiftUI: Support loading custom model from file picker,singularity-s0,2024-01-04T03:39:51Z,2024-01-04T08:22:38+00:00,2,4.71
4773,SOTA 2-bit quants,ikawrakow,2024-01-04T13:17:46Z,2024-01-08T15:02:32+00:00,12,97.75
4777,updated server readme to reflect the gg/server-token-probs-4088 commit,ibehnam,2024-01-04T18:43:17Z,2024-01-09T10:02:05+00:00,2,111.31
4787,ggml: use __builtin_amdgcn_sudot4 in __dp4a for gfx11,kzhuravl,2024-01-05T17:58:17Z,2024-01-07T06:52:42+00:00,3,36.91
4794,metal : refactor kernel loading code,ggerganov,2024-01-06T12:34:42Z,2024-01-13T16:03:45+00:00,1,171.48
4801,CUDA: int8 tensor core matrix multiplication,JohannesGaessler,2024-01-06T19:10:00Z,2024-06-14T17:07:13+00:00,12,3837.95
4804,Use llama.cpp as SPM package in iOS sample,azarovalex,2024-01-07T04:51:00Z,2024-01-07T08:20:50+00:00,1,3.5
4809,CUDA: fixed redundant value dequantization,JohannesGaessler,2024-01-07T13:17:19Z,2024-01-07T16:24:08+00:00,4,3.11
4818,convert.py: Hot Fix with VocabFactory Integration,teleprint-me,2024-01-08T03:20:46Z,2024-01-09T18:46:47+00:00,4,39.43
4826,llama.swiftui - Updated Models Layout,isaiahbjork,2024-01-08T19:56:32Z,2024-01-12T12:48:01+00:00,1,88.86
4834,llm_load_print_meta: Add additional suffixs for model params,mofosyne,2024-01-09T10:07:32Z,2024-01-10T14:09:54+00:00,2,28.04
4835,llava-cli : don't crash if --image flag is invalid,jart,2024-01-09T11:18:25Z,2024-01-09T17:59:14+00:00,2,6.68
4839,finetune: rename feed-forward tensors (w1/w2/w3),danbev,2024-01-09T14:13:44Z,2024-02-13T13:15:42+00:00,3,839.03
4844,Python script to compare commits with llama-bench,JohannesGaessler,2024-01-09T21:02:02Z,2024-01-10T00:04:33+00:00,1,3.04
4846,clip.cpp quantization update,cmp-nct,2024-01-09T23:14:38Z,2024-01-10T13:37:09+00:00,1,14.38
4848,param.path_model is a Path() not a string,mofosyne,2024-01-09T23:50:15Z,2024-04-06T16:06:40+00:00,1,2104.27
4850,Introduce GGML_CALL function annotation,jart,2024-01-10T00:35:28Z,2024-01-16T11:16:34+00:00,1,154.69
4856,SOTA 2-bit quants - part 2,ikawrakow,2024-01-10T11:47:00Z,2024-01-11T19:39:40+00:00,1,31.88
4858,convert.py: Outfile default name change and additional metadata support,mofosyne,2024-01-10T14:53:01Z,2024-05-13T02:56:47+00:00,3,2964.06
4860,Add a `/health` endpoint to the `server`,ibehnam,2024-01-10T17:16:19Z,2024-01-10T19:56:05+00:00,2,2.66
4861,Importance Matrix calculation,ikawrakow,2024-01-10T17:55:57Z,2024-01-12T05:59:57+00:00,1,36.07
4862,CUDA: fix softmax compile for old CUDA versions,JohannesGaessler,2024-01-10T18:03:49Z,2024-01-12T11:30:41+00:00,4,41.45
4864,"server: support for multiple api keys, added loading api keys from file",m18coppola,2024-01-10T19:21:34Z,2024-01-11T17:51:17+00:00,1,22.5
4866,Update `server` readme to document the new `/health` endpoint,ibehnam,2024-01-11T00:57:29Z,2024-01-11T07:12:06+00:00,1,6.24
4872,Restore intended k-quants quantization mixes for MoE models,ikawrakow,2024-01-11T08:15:51Z,2024-01-11T19:43:15+00:00,1,11.46
4873,Performance: Placing Metal encoder debug group behind a define,ptsochantaris,2024-01-11T09:47:25Z,2024-01-11T14:31:52+00:00,1,4.74
4874,Add total token count and intermittent tokens consumed so far,pudepiedj,2024-01-11T11:20:28Z,2024-01-11T16:14:52+00:00,3,4.91
4879,ci: nix-flake-update: new token with pr permissions,SomeoneSerge,2024-01-11T16:21:47Z,2024-01-11T17:22:34+00:00,1,1.01
4882,Add link to Dart binding for llama.cpp,netdur,2024-01-11T17:53:37Z,2024-01-20T08:05:43+00:00,3,206.2
4883,"Add pydantic models to GBNF grammar generator. For ""Instructor"" library like use of llama.cpp models.",Maximilian-Winter,2024-01-11T21:24:55Z,2024-01-12T19:46:46+00:00,4,22.36
4890,common : streamline the formatting of help,howlger,2024-01-12T09:36:06Z,2024-01-12T11:05:33+00:00,2,1.49
4894,export-lora: use LLAMA_FILE_MAGIC_GGLA,danbev,2024-01-12T13:10:12Z,2024-01-12T17:54:54+00:00,1,4.75
4895,CUDA: faster q8_0 -> f16 dequantization,JohannesGaessler,2024-01-12T14:00:12Z,2024-01-12T19:38:55+00:00,6,5.65
4897,2-bit quantizations,ikawrakow,2024-01-12T14:37:02Z,2024-01-14T07:45:56+00:00,6,41.15
4903,convert : update phi-2 to latest HF repo,ggerganov,2024-01-12T20:51:37Z,2024-01-13T11:44:37+00:00,7,14.88
4906,Make Q3_K_S be the same as old Q3_K_L for Mixtral-8x7B,ikawrakow,2024-01-13T07:25:59Z,2024-01-14T07:44:31+00:00,1,24.31
4908,ggml: cache sin/cos for RoPE,JohannesGaessler,2024-01-13T10:11:10Z,2024-01-13T20:41:37+00:00,1,10.51
4910,compare-llama-bench: tweak output format,JohannesGaessler,2024-01-13T11:13:32Z,2024-01-13T14:52:53+00:00,1,3.66
4917,workflows: nix-ci: further cleanup,SomeoneSerge,2024-01-13T17:19:14Z,2024-01-22T12:19:30+00:00,5,211.0
4920,nix: update flake.lock,ggerganov,2024-01-13T18:20:47Z,2024-01-16T17:13:54+00:00,1,70.89
4923,Correctly set `support_simdgroup_reduction` and `support_simdgroup_mm` on iOS,azarovalex,2024-01-13T23:12:07Z,2024-01-14T08:44:40+00:00,1,9.54
4924,"Metal: Localized logic in `ggml_metal_graph_compute`, minor performance improvement",ptsochantaris,2024-01-13T23:35:20Z,2024-01-16T17:05:19+00:00,5,65.5
4926,Introduce starter project for Android,luciferous,2024-01-14T07:40:06Z,2024-01-16T13:47:34+00:00,3,54.12
4927,Fix ffn_down quantization mix for MoE models,ikawrakow,2024-01-14T07:40:36Z,2024-01-14T08:53:39+00:00,1,1.22
4930,Add ability to use importance matrix for all k-quants,ikawrakow,2024-01-14T10:32:39Z,2024-01-14T14:21:12+00:00,1,3.81
4934,metal: replace loop of dispatch_async with dispatch_apply,azarovalex,2024-01-14T14:18:27Z,2024-01-16T13:41:28+00:00,1,47.38
4935,backend : add eval callback,ggerganov,2024-01-14T14:52:40Z,2024-01-17T16:39:41+00:00,7,73.78
4936,metal: log `recommendedMaxWorkingSetSize` on iOS 16+,azarovalex,2024-01-14T15:09:18Z,2024-01-16T13:33:03+00:00,1,46.4
4938,CUDA: faster dequantize kernels for Q4_0 and Q4_1,ikawrakow,2024-01-14T16:14:46Z,2024-01-15T05:48:06+00:00,4,13.56
4941,[draft] metal: running Metal tests on macOS 13 arm64,azarovalex,2024-01-14T17:36:06Z,2024-01-16T11:52:48+00:00,1,42.28
4943,pass cpu-architecture arguments only to host code (C;C++),ngc92,2024-01-14T19:47:05Z,2024-01-15T18:40:49+00:00,2,22.9
4948,sampling_temperature_fix,cmp-nct,2024-01-15T01:36:30Z,2024-01-15T14:59:02+00:00,1,13.38
4950,"The check for 256 divisibility was missing for IQ2_XS, IQ2_XXS",ikawrakow,2024-01-15T05:59:20Z,2024-01-15T08:09:38+00:00,1,2.17
4951,llama : apply classifier-free guidance to logits directly,dfriehs,2024-01-15T07:10:06Z,2024-01-15T13:06:52+00:00,1,5.95
4954,MobileVLM native implementation,XiaotaoChen,2024-01-15T08:13:34Z,2024-01-22T13:09:36+00:00,12,172.93
4957,imatrix : offload to GPU support,ggerganov,2024-01-15T14:49:08Z,2024-01-17T16:46:30+00:00,1,49.96
4959,Speculative: threading options,stduhpf,2024-01-15T17:30:31Z,2024-01-16T11:04:33+00:00,1,17.57
4966,"ggml: aarch64: implement mmla kernels for q8_0_q8_0, q4_0_q8_0 and q4_1_q8_1 quantized gemm",snadampal,2024-01-16T03:19:30Z,2024-02-11T13:22:34+00:00,23,634.05
4969,Importance matrix support for legacy quants,ikawrakow,2024-01-16T09:11:41Z,2024-01-16T17:51:26+00:00,1,8.66
4970,metal : create autorelease pool during library build,ggerganov,2024-01-16T11:10:58Z,2024-01-17T16:38:40+00:00,1,29.46
4972,Implementation of dynamic temperature sampling as seen in KoboldCpp,l3utterfly,2024-01-16T12:00:04Z,2024-01-25T20:06:22+00:00,13,224.1
4975,cuda: fix compile error in jetson platform,KyL0N,2024-01-16T13:11:16Z,2024-01-20T07:01:46+00:00,4,89.84
4978,falcon arch fix for tied output embeddings,cmp-nct,2024-01-16T14:30:21Z,2024-01-18T22:12:15+00:00,8,55.7
4979,finetune: add training data file to log message,danbev,2024-01-16T14:37:45Z,2024-01-16T17:54:24+00:00,1,3.28
4986,metal: Removing unnecessary nil check,ptsochantaris,2024-01-16T18:14:22Z,2024-01-17T08:07:25+00:00,1,13.88
4990,ggml : add IQ2 to test-backend-ops + refactoring,ggerganov,2024-01-16T21:11:03Z,2024-01-17T16:54:56+00:00,4,19.73
4994,fix copy/paste error in llama_sampling_params doc comment,dwrensha,2024-01-17T04:33:36Z,2024-01-17T07:17:50+00:00,1,2.74
5002,Added support ccache for speedup recompilation,GermanAizek,2024-01-17T16:52:49Z,2024-01-20T08:11:32+00:00,2,63.31
5003,server: allow to specify tokens as strings in logit_bias,z80maniac,2024-01-17T17:32:30Z,2024-02-11T13:38:14+00:00,3,596.1
5007,"Metal memory: Small memory leak on init, dangling pointer, and unused autorelease pool in graph compute",ptsochantaris,2024-01-17T20:47:36Z,2024-01-18T08:47:24+00:00,1,12.0
5008,kompute : fix rope_f32 and scale ops,ggerganov,2024-01-17T20:52:50Z,2024-01-18T16:49:39+00:00,1,19.95
5015,Add Winogrande evaluation,ikawrakow,2024-01-18T08:30:45Z,2024-01-18T11:46:28+00:00,1,3.26
5016,llama: add codeshell support,chiranko,2024-01-18T10:45:47Z,2024-01-19T09:07:28+00:00,2,22.36
5018,"Server: when no slot is available, defer the task instead of returning ""slot unavailable""",ngxson,2024-01-18T13:27:09Z,2024-01-18T20:33:05+00:00,1,7.1
5021,ggml : add Flash Attention,ggerganov,2024-01-18T17:06:57Z,2024-04-30T09:16:09+00:00,11,2464.15
5024,perplexity : faster Winogrande via batching,ggerganov,2024-01-18T19:39:42Z,2024-01-19T08:45:06+00:00,2,13.09
5033,"Fix issue #4791 alloc causes compute_size to be calculated incorrectly in train-text-from-scratch, end result core dump",bzuzo,2024-01-19T02:44:27Z,2024-01-19T18:20:50+00:00,1,15.61
5037,support qwen2,simonJJJ,2024-01-19T09:36:33Z,2024-01-19T11:53:14+00:00,1,2.28
5039,examples : support minLength and maxLength in JSON schema grammar converter,nopperl,2024-01-19T12:58:08Z,2024-02-19T14:14:07+00:00,1,745.27
5041,convert : partially revert PR #4818,cebtenzzre,2024-01-19T20:42:48Z,2024-01-20T23:14:18+00:00,1,26.52
5043,perplexity : fix MSVC build after #5020,cebtenzzre,2024-01-19T21:40:22Z,2024-01-20T15:08:09+00:00,3,17.46
5045,ggml: parallelize dequantization or fp format conversion when using blas,ReinForce-II,2024-01-19T22:34:08Z,2024-01-22T13:15:08+00:00,6,62.68
5047,Add ability to evauate multiple choice tasks ,ikawrakow,2024-01-20T08:47:44Z,2024-01-21T12:42:44+00:00,1,27.92
5049,llama : run all KQV ops on the CPU with no KV offload,slaren,2024-01-20T13:18:58Z,2024-01-20T15:05:50+00:00,1,1.78
5050,Slightly faster imatrix,ikawrakow,2024-01-20T17:23:36Z,2024-01-21T06:01:20+00:00,1,12.63
5051,add `#include <string>` to unicode.h,bobqianic,2024-01-20T22:03:00Z,2024-01-21T15:17:35+00:00,2,17.24
5052,llama : support StableLM 2 1.6B,compilade,2024-01-20T23:41:08Z,2024-01-22T11:21:52+00:00,1,35.68
5053,CI : Fix Windows CI by updating Intel SDE version,bobqianic,2024-01-20T23:49:36Z,2024-01-22T08:55:05+00:00,1,33.09
5054,nix: update flake.lock,ggerganov,2024-01-21T00:18:25Z,2024-01-21T03:17:28+00:00,1,2.98
5056,nix: init images,SomeoneSerge,2024-01-21T03:04:18Z,2024-02-22T19:44:10+00:00,4,784.66
5060,Add Q3_K_XS,ikawrakow,2024-01-21T07:58:20Z,2024-01-22T10:43:33+00:00,1,26.75
5062,add safetensors support to convert-lora-to-ggml.py,kuronekosaiko,2024-01-21T10:38:20Z,2024-01-21T16:28:15+00:00,1,5.83
5065,Server: try to refactor server.cpp,ngxson,2024-01-21T14:11:52Z,2024-01-26T12:42:20+00:00,12,118.51
5066,Revert LLAMA_NATIVE to OFF in flake.nix,iSma,2024-01-21T20:00:31Z,2024-01-21T21:37:13+00:00,2,1.61
5068,devops: add intel oneapi dockerfile,ngxson,2024-01-22T00:27:50Z,2024-01-23T07:11:39+00:00,1,30.73
5072,finetune: print sample-start/include-sample-start,danbev,2024-01-22T07:35:46Z,2024-01-22T11:11:01+00:00,1,3.59
5074,Small sampling optimizations,kalomaze,2024-01-22T09:08:26Z,2024-01-28T07:50:38+00:00,2,142.7
5076,KL-divergence,ikawrakow,2024-01-22T10:40:58Z,2024-01-22T14:10:15+00:00,7,3.49
5078,nix: add cc to devShell LD_LIBRARY_PATH,mhuesch,2024-01-22T11:56:47Z,2024-01-24T12:39:29+00:00,15,48.71
5080,Server enhancements - grammar segfault and helper titles.,jboero,2024-01-22T15:54:26Z,2024-05-28T10:24:26+00:00,1,3042.5
5081,Additional KL-divergence statistics,ikawrakow,2024-01-22T16:08:58Z,2024-01-23T13:17:20+00:00,1,21.14
5084,convert : fix byte tokens for --vocab-type hfft,Artefact2,2024-01-22T18:22:41Z,2024-02-04T11:49:49+00:00,3,305.45
5085,top-k sort speedup,cmp-nct,2024-01-22T19:48:04Z,2024-01-26T16:28:49+00:00,3,92.68
5086,llama : fix not enough space in buffer with Qwen,slaren,2024-01-22T20:15:04Z,2024-01-22T22:42:42+00:00,1,2.46
5088,CUDA: more info when no device code,JohannesGaessler,2024-01-22T23:02:00Z,2024-01-23T12:31:56+00:00,1,13.5
5090,llama.vim: added api key support,m18coppola,2024-01-22T23:35:19Z,2024-01-23T06:51:27+00:00,1,7.27
5093,Support for Yi-VL and a templating addon/fix for mobileVLM,cmp-nct,2024-01-23T04:48:27Z,2024-01-27T15:09:19+00:00,3,106.35
5099,examples : make pydantic scripts pass mypy and support py3.8,cebtenzzre,2024-01-23T19:50:41Z,2024-01-25T19:51:25+00:00,1,48.01
5100,llama : pre-allocate input tensors in a separate buffer,slaren,2024-01-23T22:38:13Z,2024-01-24T11:48:15+00:00,1,13.17
5104,Port of self extension to server,Maximilian-Winter,2024-01-24T00:37:33Z,2024-01-27T13:38:06+00:00,1,85.01
5105,cuda : fix 2-bit quants on amd hip,Engininja2,2024-01-24T02:35:40Z,2024-01-24T22:18:15+00:00,2,19.71
5107,add MobileVLM 1.7B/3B to the supported models list,XiaotaoChen,2024-01-24T07:40:24Z,2024-01-25T20:14:33+00:00,1,36.57
5109,Another bucket sort,ikawrakow,2024-01-24T13:35:09Z,2024-01-26T07:14:39+00:00,1,41.66
5113,Fix Q3_K_XS for MoE models,ikawrakow,2024-01-24T15:05:42Z,2024-01-25T15:58:54+00:00,1,24.89
5115,Apply min_p to unsorted tokens,JohannesGaessler,2024-01-24T16:34:58Z,2024-01-28T08:59:49+00:00,2,88.41
5118,add support for Orion-14B,sharpHL,2024-01-24T18:10:16Z,2024-01-28T08:00:30+00:00,7,85.84
5126,ggml: softmax op: update the n_task calculation,snadampal,2024-01-25T21:51:51Z,2024-01-26T17:17:59+00:00,6,19.44
5129,Removing unused `n_buffers` and `buffers` fields from `ggml_metal_context`,ptsochantaris,2024-01-25T23:33:07Z,2024-01-26T12:16:07+00:00,3,12.72
5132,New Features to run MobileVLM on orin,JidongZhang-THU,2024-01-26T02:28:08Z,2024-01-31T13:10:15+00:00,21,130.7
5136,"fix(gguf_reader.py): the ""general.alignment"" should be UINT32 type",snowyu,2024-01-26T07:53:12Z,2024-01-26T09:10:29+00:00,1,1.29
5145,cuda : fix tensor size calculation for non-split buffer,slaren,2024-01-26T14:48:36Z,2024-01-26T17:59:44+00:00,1,3.19
5146,cmake : pass CPU architecture flags to nvcc,cebtenzzre,2024-01-26T16:20:09Z,2024-01-26T20:34:07+00:00,3,4.23
5147,"Tests for min_p, sampling queue",JohannesGaessler,2024-01-26T16:42:45Z,2024-01-28T08:35:14+00:00,2,39.87
5151,Add OpenCL add kernel,0cc4m,2024-01-26T19:52:52Z,2024-01-26T22:07:32+00:00,1,2.24
5154,Remove unused data and add fixes,MKlimenko,2024-01-26T22:38:35Z,2024-01-27T14:25:56+00:00,6,15.79
5157,"Add Dockerfiles, CI Updates & Documentation for Server-first container images",K-Mistele,2024-01-27T06:18:07Z,2024-01-28T07:55:32+00:00,6,25.62
5160,Additional cpp fixes,MKlimenko,2024-01-27T21:32:08Z,2024-01-29T11:19:15+00:00,27,37.79
5161,metal: Reducing base memory use,ptsochantaris,2024-01-27T22:59:15Z,2024-01-28T19:50:16+00:00,2,20.85
5162,nix: update flake.lock,ggerganov,2024-01-28T00:17:19Z,2024-01-28T14:54:55+00:00,1,14.63
5173,add Vulkan to Nix flake,mschwaig,2024-01-28T12:41:22Z,2024-02-03T19:13:07+00:00,4,150.53
5175,"fix typo ""RLIMIT_MLOCK""",divinity76,2024-01-28T17:00:36Z,2024-01-29T14:45:41+00:00,1,21.75
5176,allow empty --prompt-cache file,divinity76,2024-01-28T19:14:29Z,2024-01-30T09:18:02+00:00,8,38.06
5181,add max buffer sizes to opencl and metal backends,slaren,2024-01-29T02:28:00Z,2024-01-29T08:05:13+00:00,1,5.62
5182,Fix broken Vulkan Cmake,netrunnereve,2024-01-29T02:58:02Z,2024-01-29T08:04:47+00:00,1,5.11
5184,llama : support InternLM2,SolenoidWGT,2024-01-29T04:39:05Z,2024-02-01T09:19:52+00:00,8,76.68
5187,Faster AVX2 dot product for IQ2_XS,ikawrakow,2024-01-29T07:26:06Z,2024-01-30T13:15:08+00:00,1,29.82
5190,embeddings compatibility for OpenAI [examples/server],wujjpp,2024-01-29T09:48:01Z,2024-01-29T13:48:10+00:00,1,4.0
5193,Add documents about Vulkan,lin-calvin,2024-01-29T12:09:42Z,2024-02-19T11:52:26+00:00,4,503.71
5195,server : fix context shift,ggerganov,2024-01-29T13:33:45Z,2024-01-30T18:17:31+00:00,1,28.73
5196,SOTA 3-bit quants ,ikawrakow,2024-01-29T13:52:50Z,2024-01-30T13:14:12+00:00,1,23.36
5199,Vulkan Windows APU Memory Handling,0cc4m,2024-01-29T20:55:10Z,2024-01-30T12:59:30+00:00,1,16.07
5205,Makefile to generate .a library for static linking,nehzata,2024-01-30T00:04:53Z,2024-02-01T15:18:53+00:00,2,63.23
5208,support SYCL backend windows build,NeoZhangJianyu,2024-01-30T08:28:31Z,2024-01-31T02:38:07+00:00,8,18.16
5216,"Add C++ gguf reader into cmake, add python gguf reader example",cmp-nct,2024-01-30T14:11:28Z,2024-02-13T17:56:38+00:00,1,339.75
5218,APIs for rolling back sampler tokens,l3utterfly,2024-01-30T16:04:42Z,2024-03-11T12:20:32+00:00,6,980.26
5223,Vulkan Fixes,0cc4m,2024-01-30T19:01:46Z,2024-01-31T10:44:20+00:00,2,15.71
5226,kompute : llama-bench support and ggml_cpu_has_kompute(),cebtenzzre,2024-01-30T21:23:55Z,2024-01-31T00:04:37+00:00,1,2.68
5228,Add docs for build Vulkan + SYCL in docker,ngxson,2024-01-30T23:14:10Z,2024-02-02T07:56:31+00:00,18,56.71
5230,Fix broken Vulkan Cmake (properly),netrunnereve,2024-01-31T00:37:28Z,2024-01-31T19:21:55+00:00,2,18.74
5233,"format license text, restore apache license by  legal suggestion",NeoZhangJianyu,2024-01-31T03:32:40Z,2024-01-31T13:04:47+00:00,2,9.54
5240,llama : remove LLAMA_MAX_DEVICES and LLAMA_SUPPORTS_GPU_OFFLOAD,ggerganov,2024-01-31T13:52:10Z,2024-01-31T15:30:18+00:00,3,1.64
5244,Wire up graceful server shutdown,dhiltgen,2024-01-31T16:43:40Z,2024-02-18T16:23:16+00:00,8,431.66
5252,bug: Free the allocated tokens in the batch,irbull,2024-02-01T08:03:13Z,2024-02-02T07:20:13+00:00,4,23.28
5254,update guide of SYCL backend,NeoZhangJianyu,2024-02-01T11:34:29Z,2024-02-02T07:53:27+00:00,19,20.32
5257,add --mmap in llama-bench,NeoZhangJianyu,2024-02-01T14:09:32Z,2024-02-01T19:48:53+00:00,12,5.66
5260,Vulkan Phi Fix for AMD Proprietary Drivers,0cc4m,2024-02-01T16:26:53Z,2024-02-01T18:25:24+00:00,1,1.98
5261,Tidy ggml-sycl,AidanBeltonS,2024-02-01T16:40:32Z,2024-02-02T08:39:48+00:00,2,15.99
5267,Llava 1.6 support,cmp-nct,2024-02-01T23:25:15Z,2024-02-14T07:38:35+00:00,5,296.22
5270,[SYCL] get MAX_MEM_ALLOC from device property,airMeng,2024-02-02T01:41:45Z,2024-02-02T07:54:14+00:00,3,6.21
5273,Fix Windows KL divergence calculations,kalomaze,2024-02-02T06:11:51Z,2024-02-02T14:15:30+00:00,1,8.06
5284,docs: add tenere in the ui tools list,pythops,2024-02-02T14:42:16Z,2024-02-03T11:20:26+00:00,1,20.64
5285,YaRN : store rope scaling type as int32_t in memory,cebtenzzre,2024-02-02T15:46:37Z,2024-02-03T11:22:06+00:00,1,19.59
5286,[SYCL] Fix im2col with 32fp,AidanBeltonS,2024-02-02T16:42:42Z,2024-02-03T08:11:37+00:00,2,15.48
5288,convert : fix TypeError on GPT-2 vocab.json,likejazz,2024-02-02T17:11:18Z,2024-02-07T04:28:00+00:00,5,107.28
5289,[SYCL] Fix cpy with dims of 3,AidanBeltonS,2024-02-02T17:49:59Z,2024-02-05T07:08:25+00:00,2,61.31
5291,Switch to emplace_back to avoid extra object,MKlimenko,2024-02-02T18:15:00Z,2024-02-03T11:23:37+00:00,2,17.14
5295,Added dynamic temperature parameters to main example cli,l3utterfly,2024-02-03T01:19:57Z,2024-02-05T08:00:47+00:00,1,54.68
5297,update install make by w64devkit,NeoZhangJianyu,2024-02-03T07:19:27Z,2024-02-07T10:16:55+00:00,4,98.96
5298,"Fix for LLAMA_WIN_VER default value, fixes #5158",crimsonmagick,2024-02-03T07:31:46Z,2024-02-04T04:18:52+00:00,1,20.79
5301,"Vulkan Intel Fixes, Optimizations and Debugging Flags",0cc4m,2024-02-03T11:01:22Z,2024-02-03T17:15:00+00:00,1,6.23
5302,Adding some imatrix tools,ikawrakow,2024-02-03T11:49:24Z,2024-02-04T08:39:58+00:00,1,20.84
5303,scripts: update server-llm.sh: add flag to run script non-interactive,garrnizon,2024-02-03T12:03:28Z,2024-02-05T07:43:57+00:00,2,43.67
5305,py : fix internlm2-hf convert to gguf,SolenoidWGT,2024-02-03T14:21:42Z,2024-02-05T09:04:07+00:00,2,42.71
5307,server: allow to get default generation settings for completion,z80maniac,2024-02-03T15:06:21Z,2024-02-05T08:10:22+00:00,1,41.07
5309,make: fix nvcc optimization flags for host code,JohannesGaessler,2024-02-03T17:14:02Z,2024-02-03T19:15:00+00:00,1,2.02
5310,make: add nvcc info print,JohannesGaessler,2024-02-03T17:43:06Z,2024-02-03T19:15:13+00:00,1,1.54
5311,Vulkan build on MacOS,dokterbob,2024-02-03T18:02:09Z,2024-02-19T22:49:49+00:00,15,388.79
5315,nix: update flake.lock,ggerganov,2024-02-04T00:17:30Z,2024-02-04T16:45:35+00:00,1,16.47
5318,make: Use ccache for faster compilation,JohannesGaessler,2024-02-04T08:07:48Z,2024-02-05T18:33:00+00:00,2,34.42
5320,iq2_xxs: tune quantization,ikawrakow,2024-02-04T09:02:48Z,2024-02-05T08:46:07+00:00,1,23.72
5321,Basic Vulkan Multi-GPU implementation,0cc4m,2024-02-04T11:43:57Z,2024-02-07T06:54:50+00:00,6,67.18
5325,Avoid duplicating function calls when using MIN/MAX macros.,tom7,2024-02-04T20:01:14Z,2024-02-05T11:13:57+00:00,2,15.21
5328,llama : support Mamba Selective State Space Models,compilade,2024-02-05T01:09:14Z,2024-03-08T22:31:00+00:00,12,789.36
5334,iq3_xxs: guards for the no-imatrix situation,ikawrakow,2024-02-05T08:41:05Z,2024-02-05T10:32:27+00:00,1,1.86
5338,Make use of ggml-quants.h possible in C++ code,ikawrakow,2024-02-05T09:27:53Z,2024-02-05T12:09:47+00:00,1,2.7
5341,py : handle byte tokens in `get_token_type`,ggerganov,2024-02-05T11:45:20Z,2024-02-06T05:47:22+00:00,3,18.03
5343,README: updated introduction,JohannesGaessler,2024-02-05T13:23:14Z,2024-02-05T14:55:10+00:00,1,1.53
5346,Support MiniCPM,runfuture,2024-02-05T14:23:25Z,2024-02-07T06:15:56+00:00,12,39.88
5349,"include total ""num_slots"" in default_generation_settings_for_props",jparkerweb,2024-02-05T18:46:13Z,2024-02-06T09:20:59+00:00,1,14.58
5351,CUDA: mul_mat_vec_q for batch sizes > 1,JohannesGaessler,2024-02-05T20:11:40Z,2024-02-06T13:44:06+00:00,1,17.54
5352,server: added `dynatemp_range` and `dynatemp_exponent`,m18coppola,2024-02-05T21:29:22Z,2024-02-06T09:20:00+00:00,1,11.84
5353,Fix possible typo in README-sycl.md,valiray,2024-02-05T21:36:48Z,2024-02-19T10:37:10+00:00,1,325.01
5354,Added Faraday to readme,biw,2024-02-06T01:08:23Z,2024-02-07T06:16:48+00:00,1,29.14
5357,Add Nv/AMD sycl target build cmd,abhilash1910,2024-02-06T08:36:18Z,2024-03-26T07:30:02+00:00,3,1174.9
5358,Added options to --numa flag to add finegrained control over execution,bmtwl,2024-02-06T09:18:34Z,2024-02-06T22:34:29+00:00,1,13.27
5361,Slight quantization improvement for Q4_K and Q5_K,ikawrakow,2024-02-06T12:00:03Z,2024-02-06T15:28:02+00:00,1,3.47
5366,Update README.md,ikawrakow,2024-02-06T15:26:40Z,2024-02-06T17:00:16+00:00,1,1.56
5372,Documentation: Host sample updated for IPv4+IPv6.,jboero,2024-02-06T19:21:16Z,2024-05-17T05:57:52+00:00,2,2410.61
5373,"Update `/props` with ""total_slots"" value",jparkerweb,2024-02-06T20:02:54Z,2024-02-07T06:15:19+00:00,3,10.21
5377,Added numa options to allow finer grained control as well as plumbing for a new mirror mode that will require numa.h,bmtwl,2024-02-06T22:50:30Z,2024-02-16T09:31:07+00:00,47,226.68
5379,Modernize README for 2024,netrunnereve,2024-02-07T02:43:20Z,2024-02-07T06:21:30+00:00,1,3.64
5381,vulkan: Find optimal memory type but with fallback,luciferous,2024-02-07T04:23:07Z,2024-02-15T06:11:15+00:00,4,193.8
5382,llava-cli: tokenize special tokens and remove its own special escape process,jxy,2024-02-07T05:18:01Z,2024-02-07T08:17:25+00:00,1,2.99
5386,"CUDA: fixed mmvq kernel for bs 2,3,4 and -sm row",JohannesGaessler,2024-02-07T09:31:51Z,2024-02-07T11:40:26+00:00,1,2.14
5388,sampling: fix top_k <= 0,JohannesGaessler,2024-02-07T12:24:57Z,2024-02-08T08:46:31+00:00,3,20.36
5392,Bugfix for MiniCPM model support (undo HF model tensor permute),runfuture,2024-02-07T15:07:33Z,2024-02-08T10:36:19+00:00,1,19.48
5393,CMAKE_OSX_ARCHITECTURES for MacOS cross compilation,Xarbirus,2024-02-07T15:20:19Z,2024-02-07T21:39:23+00:00,5,6.32
5394,CUDA: more warps for mmvq on NVIDIA,JohannesGaessler,2024-02-07T16:30:05Z,2024-02-08T20:56:40+00:00,2,28.44
5401,ARM intrinsics detection for MSVC,Xarbirus,2024-02-07T21:30:57Z,2024-02-14T08:49:01+00:00,1,155.3
5404,Fix `error C2078: too many initializers` with uint32x4_t for MSVC ARM64,Xarbirus,2024-02-08T08:41:40Z,2024-02-09T09:56:44+00:00,1,25.25
5409,Using abort_callback from ggml to interrupt llama computation,Xarbirus,2024-02-08T10:52:11Z,2024-03-02T19:52:25+00:00,4,561.0
5411,Fix f16_sycl cpy call from Arc,abhilash1910,2024-02-08T11:26:45Z,2024-02-08T17:09:11+00:00,3,5.71
5412,vulkan: only use M-sized matmul on Apple GPUs,slp,2024-02-08T12:16:05Z,2024-02-11T14:12:00+00:00,6,73.93
5415,README.md: added JavaScript/Wasm (works in browser) tangledgroup/llama-cpp-wasm,mtasic85,2024-02-08T16:09:06Z,2024-02-09T10:17:00+00:00,1,18.13
5416,"llama : do not print ""offloading layers"" message in CPU-only builds",slaren,2024-02-08T16:51:50Z,2024-02-08T20:33:03+00:00,1,3.69
5418,common: use enums for sampler types,z80maniac,2024-02-08T17:44:12Z,2024-02-11T13:43:31+00:00,1,67.99
5419,Not capping thread count when MoE inference is running on CPU,ptsochantaris,2024-02-08T18:23:49Z,2024-02-09T10:48:06+00:00,2,16.4
5420,server: fix prompt caching for repeated prompts,ristew,2024-02-08T19:01:27Z,2024-02-09T10:49:49+00:00,1,15.81
5423,Add support for BERT embedding models,iamlemec,2024-02-08T20:50:48Z,2024-02-11T16:21:38+00:00,17,67.51
5424,Fix Vulkan crash on APUs with very little device memory,0cc4m,2024-02-08T21:23:20Z,2024-02-09T05:52:33+00:00,1,8.49
5425,server: add llama2 chat template,ngxson,2024-02-08T22:25:51Z,2024-02-11T10:16:22+00:00,11,59.84
5427,vulkan: Set limit for task concurrency,luciferous,2024-02-09T01:50:29Z,2024-02-09T18:30:19+00:00,1,16.66
5428,llava: add requirements.txt and update README.md,danbev,2024-02-09T07:52:51Z,2024-02-09T13:01:00+00:00,1,5.14
5434,"CUDA: mul_mat_vec_q tiling, refactor mul mat logic",JohannesGaessler,2024-02-09T21:10:08Z,2024-02-11T18:08:39+00:00,9,44.98
5437,Use @autoreleasepool to avoid memory leaks,irbull,2024-02-10T05:48:40Z,2024-02-10T10:53:28+00:00,1,5.08
5444,make: add error message for bad CUDA version,JohannesGaessler,2024-02-10T20:04:58Z,2024-02-13T11:38:37+00:00,5,63.56
5448,nix: update flake.lock,ggerganov,2024-02-11T00:17:37Z,2024-02-11T15:50:41+00:00,1,15.55
5450,lookup: add print for drafting performance,JohannesGaessler,2024-02-11T10:25:35Z,2024-02-11T11:44:51+00:00,1,1.32
5452,sync : ggml,ggerganov,2024-02-11T12:40:44Z,2024-02-12T07:16:06+00:00,1,18.59
5453,1.5 bit quantization,ikawrakow,2024-02-11T16:29:28Z,2024-02-18T16:16:55+00:00,2,167.79
5457,llava: remove prog parameter from ArgumentParser,danbev,2024-02-12T06:50:05Z,2024-02-12T08:38:44+00:00,1,1.81
5458,ggml-sycl: Replace 3d ops with macro ,abhilash1910,2024-02-12T07:41:37Z,2024-02-12T14:52:06+00:00,1,7.17
5460,Fix persimmon `n_rot` conversion in `convert-persimmon-to-gguf.py`,lx200916,2024-02-12T10:59:36Z,2024-02-12T17:29:57+00:00,1,6.51
5464,Deepseek coder merge,jaggzh,2024-02-12T12:22:53Z,2024-05-07T14:22:12+00:00,5,2041.99
5466,Support batched embeddings,iamlemec,2024-02-12T18:32:55Z,2024-02-13T12:06:58+00:00,1,17.57
5468,Add support for Nomic Embed,cebtenzzre,2024-02-12T21:40:49Z,2024-02-13T17:03:53+00:00,1,19.38
5473,tests : disable moe test,ggerganov,2024-02-13T07:38:44Z,2024-02-13T09:20:25+00:00,1,1.69
5477,common : make load error reporting more granular,akx,2024-02-13T10:21:57Z,2024-02-13T13:24:51+00:00,1,3.05
5478,common : allow raw byte in SPM vocabs; don't crash if newline token is not found,akx,2024-02-13T10:23:15Z,2024-02-13T16:18:16+00:00,5,5.92
5479,lookup: complement data from context with general text statistics,JohannesGaessler,2024-02-13T13:33:50Z,2024-03-23T00:24:36+00:00,15,922.85
5482,Early return for zero size calls to get_tensor.,manyoso,2024-02-13T18:49:35Z,2024-02-13T21:44:25+00:00,4,2.91
5487,fix(gguf-py): special tokens are no longer skipped when add_<token>_token is set to false,vriesdemichael,2024-02-14T13:22:15Z,2024-02-15T13:14:38+00:00,3,23.87
5488,ggml : add ALiBi support for ggml_soft_max_ext,ggerganov,2024-02-14T14:06:26Z,2024-02-17T21:04:16+00:00,10,78.96
5491,Fix memory management bug in llava and server code,Elbios,2024-02-14T16:16:24Z,2024-02-15T08:01:57+00:00,5,15.76
5494,"server: add ""samplers"" param to control the samplers order",z80maniac,2024-02-14T17:09:58Z,2024-02-16T11:33:25+00:00,6,42.39
5495,hotfix for llava-1.6 image number,cmp-nct,2024-02-14T17:13:33Z,2024-02-15T07:59:18+00:00,1,14.76
5500,Use correct type of pooling for embedding models,iamlemec,2024-02-15T07:12:55Z,2024-02-15T17:21:49+00:00,8,10.15
5501,scripts : add hf.sh helper script,ggerganov,2024-02-15T07:56:40Z,2024-02-15T13:41:15+00:00,6,5.74
5502,Update ggml_sycl_op_mul_mat_vec_q,AidanBeltonS,2024-02-15T10:50:52Z,2024-02-20T07:01:25+00:00,9,116.18
5509,llava: fix clip-model-is-vision flag in README.md,danbev,2024-02-15T13:27:12Z,2024-02-16T09:24:39+00:00,4,19.96
5511,examples : constantify lambda variables,GermanAizek,2024-02-15T14:02:01Z,2024-05-20T02:54:07+00:00,1,2268.87
5516,server : fix system prompt cli,An0nie,2024-02-16T00:24:31Z,2024-02-16T10:00:56+00:00,1,9.61
5521,scripts : add helpers script for bench comparing commits,ggerganov,2024-02-16T08:19:52Z,2024-02-16T13:14:40+00:00,1,4.91
5525,cmake : fix VULKAN and ROCm builds,ggerganov,2024-02-16T11:16:33Z,2024-02-16T17:05:56+00:00,1,5.82
5528,"ggml, common, examples, tests : fixed type arguments in printf",GermanAizek,2024-02-16T11:30:46Z,2024-02-18T16:20:12+00:00,1,52.82
5530,common : fixed critical UB (before C++17) inserting map size into himself,GermanAizek,2024-02-16T12:07:16Z,2024-02-18T16:22:11+00:00,4,52.25
5535,"common, examples, llama : optimize using reserve if possible",GermanAizek,2024-02-16T14:01:15Z,2025-01-18T21:54:24+00:00,1,8095.89
5536,llava: update surgery script to not remove tensors,danbev,2024-02-16T14:34:20Z,2024-02-18T16:19:24+00:00,1,49.75
5538,Add llama_chat_apply_template(),ngxson,2024-02-16T15:36:24Z,2024-02-19T08:23:37+00:00,11,64.79
5543,CMAKE CUDA fix: Unknown option '-Wall',clibdev,2024-02-17T09:29:14Z,2024-02-18T21:24:45+00:00,4,35.93
5544,Update gitignore file for CLion IDE,clibdev,2024-02-17T09:35:38Z,2024-02-17T16:28:38+00:00,1,6.88
5548,server: enhanced health endpoint,phymbert,2024-02-17T11:50:17Z,2024-02-18T16:31:28+00:00,4,28.69
5549,server: --n-predict option document and ensure the completion request does not exceed it,phymbert,2024-02-17T13:49:03Z,2024-02-18T16:30:10+00:00,2,26.69
5550,server: slots monitoring endpoint,phymbert,2024-02-17T14:48:45Z,2024-02-18T17:39:58+00:00,2,26.85
5553,support llava 1.6 image embedding dimension in server,cjpais,2024-02-17T20:14:34Z,2024-02-20T19:07:22+00:00,6,70.88
5557,Android and old glibc NUMA incompatibility bugfixes ,bmtwl,2024-02-17T21:47:37Z,2024-02-19T07:38:33+00:00,1,33.85
5558,nix: update flake.lock,ggerganov,2024-02-18T00:17:14Z,2024-02-18T14:39:58+00:00,1,14.38
5566,server: init functional tests,phymbert,2024-02-18T17:39:11Z,2024-02-24T11:28:55+00:00,27,137.83
5568,llama : rename n_ctx to kv_size,ggerganov,2024-02-18T20:15:27Z,2024-07-26T19:24:16+00:00,1,3815.15
5573,Fixed the baby-llama issue (see issue #4830),NawafAlansari,2024-02-18T22:26:34Z,2024-02-19T08:25:38+00:00,1,9.98
5574,cuda : fix nans in soft_max,slaren,2024-02-18T22:33:21Z,2024-02-19T08:04:45+00:00,2,9.52
5576,Fix 2 cuda memory leaks in ggml-cuda.cu,dhiltgen,2024-02-19T01:17:22Z,2024-03-12T20:14:21+00:00,4,546.95
5577,llava: avoid chaging the original BakLLaVA model,danbev,2024-02-19T06:49:06Z,2024-02-19T08:31:59+00:00,1,1.71
5590,IQ4_NL: 4-bit non-linear quants with blocks of 32,ikawrakow,2024-02-19T14:43:12Z,2024-02-21T09:39:52+00:00,1,42.94
5591,[SYCL] Use batched mul_mat pathway,AidanBeltonS,2024-02-19T16:48:34Z,2024-03-01T07:36:47+00:00,3,254.8
5593,Server: use llama_chat_apply_template,ngxson,2024-02-19T18:44:20Z,2024-02-20T14:58:27+00:00,4,20.24
5594,server: health endpoint configurable failure on no slot,phymbert,2024-02-19T19:49:51Z,2024-02-20T07:48:19+00:00,1,11.97
5597,cuda : ignore peer access already enabled errors,slaren,2024-02-19T20:39:01Z,2024-02-19T22:40:26+00:00,1,2.02
5604,metal : add build system support for embedded metal library,tonyfettes,2024-02-20T05:44:25Z,2024-02-20T09:58:36+00:00,1,4.24
5605,Add maid to UI list,danemadsen,2024-02-20T06:11:33Z,2024-02-20T10:00:23+00:00,1,3.81
5606,cabelo@opensuse.org - Build in openSUSE:compatible with gcc7,cabelo,2024-02-20T08:23:29Z,2024-03-05T16:57:07+00:00,4,344.56
5611,llava: add explicit instructions for llava-1.6,danbev,2024-02-20T14:50:50Z,2024-02-20T17:30:27+00:00,1,2.66
5615,"convert : get general.name from model dir, not its parent",cebtenzzre,2024-02-20T18:28:09Z,2024-05-16T06:15:23+00:00,1,2051.79
5618,readme: add Msty to UI list,ashokgelal,2024-02-20T21:27:46Z,2024-02-25T15:57:34+00:00,1,114.5
5622,examples : do not assume BOS when shifting context,cebtenzzre,2024-02-21T03:51:45Z,2024-02-21T15:33:54+00:00,1,11.7
5624,[SYCL] conext add name,airMeng,2024-02-21T06:28:38Z,2024-02-21T09:52:06+00:00,1,3.39
5625,Implement stochastic speculative sampling,mscheong01,2024-02-21T08:15:50Z,2024-03-04T18:24:00+00:00,12,298.14
5628,"Server: fallback to chatml, add AlphaMonarch chat template",ngxson,2024-02-21T10:14:13Z,2024-02-22T08:33:25+00:00,2,22.32
5629,readme: add LocalAI to the availables UI,mudler,2024-02-21T11:21:48Z,2024-02-21T14:39:10+00:00,1,3.29
5634,server: health: fix race condition on slots data using tasks queue,phymbert,2024-02-21T13:02:02Z,2024-02-21T14:47:48+00:00,1,1.76
5638,MPT: add optional bias parameters,datquocnguyen,2024-02-21T15:50:37Z,2024-02-22T08:15:13+00:00,1,16.41
5639,[SYCL] Add support for soft_max ALiBi,AidanBeltonS,2024-02-21T16:19:08Z,2024-02-26T14:02:11+00:00,4,117.72
5640,server: clarify some params in the docs,z80maniac,2024-02-21T17:37:18Z,2024-02-22T08:27:32+00:00,1,14.84
5643,Fix MSVC compile errors,uextm,2024-02-21T18:49:42Z,2024-03-08T09:35:04+00:00,1,374.76
5645,Add docs for llama_chat_apply_template,ngxson,2024-02-21T20:20:55Z,2024-02-21T23:31:00+00:00,1,3.17
5647,py : add Gemma conversion from HF models,ggerganov,2024-02-21T20:52:34Z,2024-02-22T21:22:48+00:00,3,24.5
5663,"workflows: nix: hardcode cachix ids, build unconditionally",SomeoneSerge,2024-02-22T14:49:20Z,2024-02-22T16:32:09+00:00,1,1.71
5664,build(nix): Package gguf-py,ditsuke,2024-02-22T15:37:01Z,2024-09-02T11:21:02+00:00,45,4627.73
5665,Add Gemma chat template,ngxson,2024-02-22T15:50:21Z,2024-02-22T18:10:21+00:00,1,2.33
5666,ggml : always define ggml_fp16_t as uint16_t,ggerganov,2024-02-22T17:05:58Z,2024-02-22T21:21:39+00:00,1,4.26
5676,IQ3_S: a much better alternative to Q3_K,ikawrakow,2024-02-23T07:35:06Z,2024-02-24T14:23:52+00:00,2,30.81
5684,server: add KV cache quantization options,AlpinDale,2024-02-23T16:07:23Z,2024-02-23T19:31:55+00:00,1,3.41
5687,build(nix): Introduce flake.formatter for `nix fmt`,ditsuke,2024-02-23T16:44:56Z,2024-03-01T23:18:27+00:00,6,174.56
5689,Makefile: use variables for cublas,lindeer,2024-02-23T17:29:00Z,2024-02-27T02:03:06+00:00,11,80.57
5691,llama : refactor k-shift implementation + initial defragmentation,ggerganov,2024-02-23T18:34:49Z,2024-02-25T20:12:24+00:00,1,49.63
5697,code : normalize enum names,ggerganov,2024-02-24T09:16:35Z,2024-02-25T10:09:09+00:00,1,24.88
5699,server: continue to update other slots on embedding concurrent request,phymbert,2024-02-24T12:05:55Z,2024-02-24T18:16:04+00:00,2,6.17
5700,server: logs - unified format and --log-format option,phymbert,2024-02-24T12:17:38Z,2024-02-25T12:50:33+00:00,18,24.55
5702,cmake: Add fix compilation for Android armeabi-v7a,rgryta,2024-02-24T18:29:15Z,2024-02-25T10:53:11+00:00,7,16.4
5703,Fix issues during StableLM models conversion due to config.json changes,aahouzi,2024-02-24T18:43:40Z,2024-02-25T09:54:04+00:00,8,15.17
5705,nix: update flake.lock,ggerganov,2024-02-25T00:17:17Z,2024-02-25T22:24:22+00:00,1,22.12
5708,server: monitoring - add /metrics prometheus compatible endpoint,phymbert,2024-02-25T09:44:01Z,2024-02-25T12:49:43+00:00,8,3.1
5710,Server: Improve work queue stability,ngxson,2024-02-25T12:01:46Z,2024-02-28T20:57:20+00:00,6,80.93
5711,ggml-quants: Provide ggml_vqtbl1q_u8 for 64bit compatibility ,rgryta,2024-02-25T12:40:48Z,2024-02-25T18:43:00+00:00,3,6.04
5714,server : fix crash when system prompt is bigger than batch size,compilade,2024-02-25T16:25:44Z,2024-02-25T18:43:51+00:00,1,2.3
5715,server: tests - slow inference causes timeout on the CI,phymbert,2024-02-25T17:23:57Z,2024-02-25T21:48:33+00:00,2,4.41
5718,server: docs - refresh and tease a little bit more the http server,phymbert,2024-02-25T18:38:40Z,2024-02-25T20:46:29+00:00,4,2.13
5721,Adding IQ2_S and IQ2_M to complete coverage of the 2-3 bit quantization range,ikawrakow,2024-02-26T06:44:54Z,2024-02-26T16:28:40+00:00,1,9.73
5722,"Add ""/chat/completions"" as alias for ""/v1/chat/completions""",jorgealias,2024-02-26T06:51:49Z,2024-02-28T08:39:15+00:00,3,49.79
5729,CUDA: fix DEBUG_CUDA_MALLOC,JohannesGaessler,2024-02-26T12:02:30Z,2024-02-26T14:36:38+00:00,2,2.57
5733,Server: fix server hangs on empty prompt,ngxson,2024-02-26T14:38:30Z,2024-02-26T22:15:48+00:00,1,7.62
5734,Server: Hit Ctrl+C twice to exit,ngxson,2024-02-26T14:44:56Z,2024-02-28T08:55:37+00:00,8,42.18
5738,[SYCL] Add support for SYCL Nvidia target,AidanBeltonS,2024-02-26T16:49:54Z,2024-03-11T01:13:57+00:00,2,320.4
5740,Improve BERT tokenization for accented characters and non-latin scripts,iamlemec,2024-02-26T21:04:28Z,2024-02-28T08:51:11+00:00,10,35.78
5742,ggml-quants : fix avx2 iq1_s vec_dot when compiled with gcc,Engininja2,2024-02-26T22:41:50Z,2024-02-27T12:50:18+00:00,1,14.14
5744,cuda : replace remaining shfl_xor with calls to warp_reduce functions,Engininja2,2024-02-27T01:31:04Z,2024-02-27T13:22:45+00:00,2,11.86
5745,build(python): Package scripts with pep-0517 compliance,ditsuke,2024-02-27T06:34:38Z,2024-07-04T15:39:13+00:00,26,3081.08
5746,Update issues.feature,TruongGiangBT,2024-02-27T07:28:50Z,2024-02-29T01:47:24+00:00,1,42.31
5747,IQ4_XS: a 4.25 bpw quantization,ikawrakow,2024-02-27T08:42:49Z,2024-02-27T14:34:24+00:00,1,5.86
5754,llama : fix non-quantization of expert gating tensors,compilade,2024-02-27T20:05:17Z,2024-02-28T08:52:56+00:00,2,12.79
5756,"Enable CORS requests on ""/health"" server endpoint.",StrangeBytesDev,2024-02-28T01:22:33Z,2024-02-28T21:29:47+00:00,1,20.12
5757,Support Vulkan versions older than v1.3.208 (eEnumeratePortabilityKHR),netrunnereve,2024-02-28T01:48:25Z,2024-02-28T19:33:37+00:00,1,17.75
5758,readme: add link to LLaVA 1.6 models,danbev,2024-02-28T05:39:01Z,2024-02-28T08:39:39+00:00,1,3.01
5760,Make i-quants work with super-blocks of 64 (CPU and Metal),ikawrakow,2024-02-28T06:54:38Z,2024-02-28T08:37:02+00:00,1,1.71
5770,llama : remove deprecated API,ggerganov,2024-02-28T15:36:23Z,2024-02-28T16:43:39+00:00,1,1.12
5771,ci : reduce 3b ppl chunks to 1 to avoid timeout,ggerganov,2024-02-28T16:43:15Z,2024-02-28T19:44:21+00:00,1,3.02
5772,"cleanup unused --no-mul-mat-q,-nommq, -mmq, --mul-mat-q, mul_mat_q",phymbert,2024-02-28T17:04:57Z,2024-03-01T11:39:06+00:00,5,42.57
5774,constified `llama_set_state_data`'s `src`,MarcusDunn,2024-02-28T17:59:55Z,2024-02-29T08:17:23+00:00,2,14.29
5776,server: error handling,z80maniac,2024-02-28T18:38:26Z,2024-03-09T10:17:40+00:00,1,231.65
5779,Server: normalize naming,ngxson,2024-02-28T21:00:23Z,2024-02-29T20:42:11+00:00,1,23.7
5780,"Arm AArch64: optimized GEMV and GEMM kernels for q4_0_q8_0, and q8_0_q8_0 quantization",Dibakar,2024-02-28T21:08:47Z,2024-07-10T12:14:51+00:00,40,3183.1
5781,Enable CORS requests on all routes,StrangeBytesDev,2024-02-28T21:38:29Z,2025-02-10T21:47:11+00:00,15,8352.15
5789,Add Ubuntu 22 Vulkan CI run,netrunnereve,2024-02-29T03:13:42Z,2024-03-01T08:54:53+00:00,1,29.69
5792,fix(convert-hf-to-gguf): requires einops for InternLM2ForCausalLM models,Nold360,2024-02-29T09:18:47Z,2024-03-01T21:51:12+00:00,2,36.54
5794,server: allow to override threads server pool with --threads-http,phymbert,2024-02-29T10:25:44Z,2024-03-01T09:08:08+00:00,5,22.71
5795,Add support for StarCoder2,pacman100,2024-02-29T12:02:35Z,2024-03-01T19:30:47+00:00,9,31.47
5796,llama : fix embeddings,ggerganov,2024-02-29T13:46:03Z,2024-03-04T20:31:20+00:00,15,102.75
5798,Models without Vocabulary,Xarbirus,2024-02-29T14:37:52Z,2024-03-14T16:21:57+00:00,15,337.73
5799,Switch to multimap based nfd_map due to compile time issues,iamlemec,2024-02-29T18:18:01Z,2024-03-01T09:15:36+00:00,4,14.96
5805,Fix flag name in help message `--logits-all` to `--all-logits`,ensan-hcl,2024-03-01T07:34:46Z,2024-03-01T13:48:56+00:00,1,6.24
5806,Support multiple GPUs (split mode) on SYCL backend,NeoZhangJianyu,2024-03-01T07:52:58Z,2024-03-02T11:49:31+00:00,7,27.94
5808,server : remove api_like_OAI.py proxy script,ggerganov,2024-03-01T08:20:57Z,2024-03-01T18:00:58+00:00,1,9.67
5810,Fix Gemma parity issue,kunal-vaishnavi,2024-03-01T09:42:01Z,2024-03-01T14:08:08+00:00,1,4.44
5813,"ggml-vulkan: fix VULKAN_CHECK_RESULTS flag, which was previously broken",ddpasa,2024-03-01T12:26:05Z,2024-03-01T17:00:00+00:00,1,4.57
5814,nix: static build,hutli,2024-03-01T14:30:45Z,2024-03-05T01:33:09+00:00,12,83.04
5820,llama : fix segfault from unknown model arch name,compilade,2024-03-01T17:04:37Z,2024-03-02T13:42:56+00:00,3,20.64
5821,convert : automatically fall back to HfVocab if tokenizer.model doesn't exist,cebtenzzre,2024-03-01T17:17:31Z,2024-03-02T17:27:26+00:00,1,24.17
5824,Assume tied weights if lm_head/output weights is missing.,dmahurin,2024-03-01T20:20:30Z,2024-03-08T10:41:50+00:00,1,158.36
5825,convert-hf : make model class definitions self-contained,cebtenzzre,2024-03-01T21:00:49Z,2024-03-02T17:21:47+00:00,1,20.35
5826,workflows : remove nocleanup arg for check-requirements.sh,crasm,2024-03-01T22:11:08Z,2024-03-02T05:11:07+00:00,1,7.0
5829,IQ3_S improvements,ikawrakow,2024-03-02T07:57:01Z,2024-03-02T15:00:51+00:00,1,7.06
5830,Refactor multi-thread quantize,ngxson,2024-03-02T10:15:17Z,2024-03-02T14:19:09+00:00,1,4.06
5832,server: passkey challenge /  self-extend with context shift demo,phymbert,2024-03-02T13:10:29Z,2024-03-02T21:00:14+00:00,2,7.83
5835,Vulkan Improvements,0cc4m,2024-03-02T18:00:18Z,2024-03-05T12:33:42+00:00,1,66.56
5836,server: init server http requests threads pool with --parallel if set,phymbert,2024-03-02T19:54:40Z,2024-03-03T07:48:37+00:00,1,11.9
5839,server: tests: scheduled slow tests step only on Release or on demand,phymbert,2024-03-02T22:23:55Z,2024-03-03T08:35:23+00:00,1,10.19
5840,llama : fix llama_copy_state_data with fragmented KV cache,compilade,2024-03-02T23:17:37Z,2024-03-03T08:41:55+00:00,1,9.4
5841,gguf-dump: support i-quants,Nindaleth,2024-03-02T23:46:44Z,2024-03-03T08:43:42+00:00,1,8.95
5842,nix: update flake.lock,ggerganov,2024-03-03T00:17:09Z,2024-03-03T04:11:31+00:00,1,3.91
5844,Handle cases where git index is not found in .git directory,danemadsen,2024-03-03T01:46:47Z,2024-03-04T18:26:55+00:00,3,40.67
5847,Support special tokens as reverse/anti prompt.,dranger003,2024-03-03T03:02:15Z,2024-03-04T07:57:20+00:00,3,28.92
5849,Allow for user specified embedding pooling type,iamlemec,2024-03-03T05:13:51Z,2024-03-03T10:40:27+00:00,5,5.44
5853,cuda : fix data race in soft max,slaren,2024-03-03T12:20:52Z,2024-03-03T13:26:18+00:00,1,1.09
5855,Use already defined default seed to avoid compiler error C2397,dranger003,2024-03-03T16:53:57Z,2024-03-04T08:08:20+00:00,1,15.24
5858,Add alias for chat template,ngxson,2024-03-03T18:35:18Z,2024-03-04T11:22:09+00:00,1,16.78
5862,[SYCL] fix mul_mat fault in CI/unit-test,NeoZhangJianyu,2024-03-04T00:47:19Z,2024-03-05T08:08:35+00:00,13,31.35
5874,fix speculative decoding build on windows,jquesnelle,2024-03-04T21:33:08Z,2024-03-05T03:23:06+00:00,2,5.83
5880,server: maintain chat completion id for streaming responses,mscheong01,2024-03-05T08:41:41Z,2024-03-08T09:36:14+00:00,4,72.91
5882,server : refactor,ggerganov,2024-03-05T09:25:07Z,2024-03-07T09:41:53+00:00,9,48.28
5885,Don't allow grammar json array to output unescaped new line in string,ExtReMLapin,2024-03-05T10:32:10Z,2024-03-05T13:44:29+00:00,1,3.21
5886,[SYCL] Add q3_s and q1_s,abhilash1910,2024-03-05T12:23:27Z,2024-03-11T04:57:56+00:00,6,136.57
5888,Fixed json strings grammar by blacklisting character control set,ExtReMLapin,2024-03-05T14:47:04Z,2024-03-05T16:33:08+00:00,1,1.77
5892,compare-llama-bench.py : remove mul_mat_q,slaren,2024-03-05T20:06:20Z,2024-03-05T21:27:30+00:00,1,1.35
5894,Using `uint8x16_t` as the return type for `ggml_vqtbl1q_u8`,bobqianic,2024-03-05T22:20:31Z,2024-03-06T07:35:07+00:00,1,9.24
5901,[SYCL] fix error when set main gpu to non-zero,NeoZhangJianyu,2024-03-06T09:58:30Z,2024-03-07T08:34:32+00:00,16,22.6
5906,ggml : use SYS_get_cpu if SYS_getcpu is not defined,cebtenzzre,2024-03-06T16:13:58Z,2024-03-06T20:42:23+00:00,1,4.47
5908,server: tests: embeddings use a real embeddings model,phymbert,2024-03-06T17:27:35Z,2024-03-06T19:24:20+00:00,2,1.95
5914,add `/v1/completions` endpoint,mscheong01,2024-03-07T01:23:30Z,2024-03-07T10:42:39+00:00,1,9.32
5918,"Revert ""[SYCL] fix error when set main gpu to non-zero""",NeoZhangJianyu,2024-03-07T09:12:18Z,2024-03-07T11:14:49+00:00,1,2.04
5926,server: SSL Support,gabe-l-hart,2024-03-07T18:58:15Z,2024-03-09T09:57:09+00:00,6,38.98
5927,Add Unicode model filename support for Windows,BruceMacD,2024-03-07T21:45:06Z,2024-03-14T19:07:56+00:00,4,165.38
5933,"server: tests: add truncated prompt tests, better kv cache size",phymbert,2024-03-08T09:21:32Z,2024-03-09T09:30:04+00:00,1,24.14
5935,examples: fix utf8 decoding error,zhangfuwen,2024-03-08T09:54:04Z,2024-03-10T20:03:18+00:00,1,58.15
5937,"server: metrics: fix bucket reset on /health, add few more metrics",phymbert,2024-03-08T10:02:11Z,2024-03-08T11:25:04+00:00,1,1.38
5939,Server: reorganize some http logic,ngxson,2024-03-08T11:15:04Z,2024-03-09T10:27:54+00:00,3,23.21
5940,ggml : add ggml-common.h to deduplicate shared code,ggerganov,2024-03-08T11:36:56Z,2024-03-09T10:47:57+00:00,3,23.18
5941,server: benchmark: chat/completions scenario and other llm servers comparison,phymbert,2024-03-08T13:01:22Z,2024-03-09T22:41:49+00:00,4,33.67
5942,ggml : remove old quantization functions,ggerganov,2024-03-08T13:22:49Z,2024-03-09T13:54:00+00:00,3,24.52
5943,ggml : reuse quantum structs across backends,ggerganov,2024-03-08T15:17:13Z,2024-03-12T12:27:20+00:00,1,93.17
5946,perplexity : support using multiple sequences to allow larger batch sizes,slaren,2024-03-08T23:48:44Z,2024-03-09T18:55:54+00:00,5,19.12
5948,GBNF Validator Program,HanClinto,2024-03-09T01:32:03Z,2024-04-04T07:44:28+00:00,3,630.21
5950,Removes segfault when parsing grammar with missing symbols. Adds friendly error message.,HanClinto,2024-03-09T04:51:49Z,2024-03-10T15:17:44+00:00,2,34.43
5956,output normalize embedding in '/v1/embeddings',redlion0929,2024-03-09T11:25:02Z,2024-03-09T12:27:58+00:00,1,1.05
5959,Add support for GritLM,dranger003,2024-03-09T12:51:21Z,2024-03-10T15:56:30+00:00,10,27.09
5961,Server: format error to json,ngxson,2024-03-09T14:11:54Z,2024-03-11T09:56:41+00:00,13,43.75
5966,ROCm: use native CMake HIP support,GZGavinZhao,2024-03-09T17:26:53Z,2024-05-17T15:03:03+00:00,3,1653.6
5968,server: ci: windows build and tests,phymbert,2024-03-10T01:04:14Z,2024-03-10T17:17:47+00:00,5,16.23
5969,nix: update flake.lock,ggerganov,2024-03-10T04:33:38Z,2024-03-10T14:43:08+00:00,1,10.16
5970,Add support for control vectors,vgel,2024-03-10T06:57:20Z,2024-03-15T20:43:02+00:00,10,133.76
5971,Better 1.5 bit quantization ,ikawrakow,2024-03-10T08:23:52Z,2024-03-11T06:51:49+00:00,1,22.47
5978,json-schema-to-grammar improvements (+ added to server),ochafik,2024-03-10T17:44:00Z,2024-03-21T11:50:43+00:00,12,258.11
5979,Windows ARM runner and build fixes,Xarbirus,2024-03-10T18:46:58Z,2024-03-11T09:28:51+00:00,4,14.7
5985,fix: make subdirectory llama.cpp work with `LLAMA_METAL_EMBED_LIBRARY`,giladgd,2024-03-10T22:44:18Z,2024-03-11T08:00:08+00:00,1,9.26
5988,server: maintain chat completion id for streaming responses,mscheong01,2024-03-11T02:52:59Z,2024-03-11T08:09:32+00:00,1,5.28
5992,llama : refactor unicode stuff,ggerganov,2024-03-11T09:25:35Z,2024-03-11T15:47:47+00:00,1,6.37
5995,sycl : try to fix SYCL after IQ1_S changes,ggerganov,2024-03-11T11:32:24Z,2024-03-12T09:15:05+00:00,5,21.71
5999,1.5 bit: we can do even better,ikawrakow,2024-03-11T13:59:09Z,2024-03-11T15:53:16+00:00,1,1.9
6001,Server: Use multi-task for embeddings endpoint,ngxson,2024-03-11T17:10:50Z,2024-03-13T10:39:11+00:00,6,41.47
6004,"Fix: GBNF missing ""root"" node crashing server",HanClinto,2024-03-11T21:52:59Z,2024-03-13T18:10:40+00:00,1,44.29
6006,[SYCL] fix error to set main gpu as non-zero ,NeoZhangJianyu,2024-03-12T03:37:17Z,2024-03-12T15:34:38+00:00,4,11.96
6010,fix build break by iq1s,NeoZhangJianyu,2024-03-12T09:17:01Z,2024-03-12T15:37:43+00:00,11,6.34
6012,ggml : fix UB in IQ2_S and IQ3_S,ggerganov,2024-03-12T10:45:09Z,2024-03-12T11:49:55+00:00,1,1.08
6017,llama : add pipeline parallelism support,slaren,2024-03-12T14:22:40Z,2024-03-13T17:54:22+00:00,22,27.53
6022,"[SYCL] fix set main gpu error, support single/mul gpu mode",NeoZhangJianyu,2024-03-12T15:34:18Z,2024-03-15T06:37:57+00:00,7,63.06
6025,[SYCL] Update get version,AidanBeltonS,2024-03-12T17:45:21Z,2024-03-13T13:17:54+00:00,2,19.54
6028,test-backend-ops : skip CPU backend by default,slaren,2024-03-12T22:38:54Z,2024-03-13T13:58:30+00:00,1,15.33
6033,Add Command-R Model,acanis,2024-03-13T06:16:33Z,2024-03-15T20:41:22+00:00,13,62.41
6035,[CANN] Add Ascend NPU backend,hipudding,2024-03-13T07:21:24Z,2024-07-17T11:23:50+00:00,18,3028.04
6037,Fix: attempt to reduce the impact of a worst-case scenario on defragmentation,Xarbirus,2024-03-13T10:33:09Z,2024-03-14T10:56:48+00:00,1,24.39
6039,README.md: Update details about running llama in Termux on Android,wanix1988,2024-03-13T11:42:39Z,2024-03-13T18:34:40+00:00,1,6.87
6042,[SYCL] Fix non-intel device selection,AidanBeltonS,2024-03-13T15:30:56Z,2024-03-15T09:26:21+00:00,5,41.92
6044,readme: improve readme for Llava-1.6 example,jianliao,2024-03-13T18:54:03Z,2024-03-14T11:18:23+00:00,1,16.41
6045,"gguf-py: add support for I8, I16 and I32",certik,2024-03-13T20:19:41Z,2024-03-14T10:40:14+00:00,1,14.34
6047,"server: test: disable debug release type sanitizer, simplify trigger",phymbert,2024-03-13T21:28:15Z,2024-03-14T11:15:39+00:00,1,13.79
6052,[SYCL] iq2_s,abhilash1910,2024-03-14T11:46:06Z,2024-04-07T03:27:22+00:00,9,567.69
6053,issues: ci - close inactive issue with workflow,phymbert,2024-03-14T12:32:34Z,2024-03-16T12:20:53+00:00,3,47.81
6062,gguf : add support for I64 and F64 arrays,certik,2024-03-14T18:58:06Z,2024-03-15T08:46:51+00:00,1,13.81
6066,Add Orion chat template,ngxson,2024-03-14T21:33:31Z,2024-03-15T08:44:58+00:00,1,11.19
6069,llama-bench : use random tokens to improve accuracy with mixtral,slaren,2024-03-14T23:15:31Z,2024-03-15T08:22:24+00:00,1,9.11
6073,"[SYCL] fix set main gpu error, support single/mul gpu mode ",NeoZhangJianyu,2024-03-15T06:46:52Z,2024-03-15T10:53:53+00:00,2,4.12
6074,Add qwen2moe,simonJJJ,2024-03-15T07:28:44Z,2024-04-16T15:40:49+00:00,23,776.2
6078,cuda : disable unused cudaLaunchHostFunc code,slaren,2024-03-15T11:17:52Z,2024-03-15T12:24:03+00:00,1,1.1
6079,llava: change llava API to pure C style for Rust FFI bindgen,tinglou,2024-03-15T11:32:39Z,2024-03-15T14:31:05+00:00,1,2.97
6083,backend : offload large batches to GPU,slaren,2024-03-15T13:44:21Z,2024-03-18T10:03:04+00:00,6,68.31
6086,gritlm: add initial README.md to examples/gritlm,danbev,2024-03-15T14:45:44Z,2024-03-16T15:46:30+00:00,3,25.01
6088,Added AVX512F macros to ggml.c ,amiralimi,2024-03-15T17:37:09Z,2024-03-16T15:52:02+00:00,1,22.25
6094,ggml:fix finding transfer queue family index error,GainLee,2024-03-16T02:38:10Z,2024-03-17T17:12:23+00:00,1,38.57
6098,common: llama_load_model_from_url using --model-url,phymbert,2024-03-16T11:35:46Z,2024-03-17T18:12:38+00:00,12,30.61
6100,Readme: add wllama as a wasm binding,ngxson,2024-03-16T12:33:52Z,2024-03-16T15:42:09+00:00,1,3.14
6101,Refactor nested if causing error C1061 on MSVC.,dranger003,2024-03-16T14:04:31Z,2024-03-16T15:39:15+00:00,4,1.58
6105,Tidy-up argument parsing.,dranger003,2024-03-16T17:03:50Z,2024-03-18T08:27:45+00:00,1,39.4
6106,convert : use f32 outtype for bf16 tensors,Artefact2,2024-03-16T20:42:37Z,2024-03-18T08:04:42+00:00,1,35.37
6109,nix: update flake.lock,ggerganov,2024-03-17T06:37:46Z,2024-03-18T18:51:30+00:00,1,36.23
6118,nix: make `xcrun` visible in Nix sandbox for precompiling Metal shaders,josephst,2024-03-17T19:23:31Z,2024-03-26T00:51:46+00:00,26,197.47
6119,Add support for CamembertModel architecture,Royalphax,2024-03-17T20:16:20Z,2024-03-18T08:17:00+00:00,1,12.01
6122,llama : greatly reduce output buffer memory usage,compilade,2024-03-17T23:41:55Z,2024-03-26T14:46:41+00:00,17,207.08
6126,ci : disable stale issue messages,ggerganov,2024-03-18T07:21:33Z,2024-03-18T11:45:38+00:00,1,4.4
6127,common : disable repeat penalties by default,ggerganov,2024-03-18T08:09:48Z,2024-03-19T08:21:54+00:00,1,24.2
6128,ci : temporary disable sanitizer builds,ggerganov,2024-03-18T08:35:17Z,2024-03-18T11:45:27+00:00,1,3.17
6135,gguf-split: split and merge gguf per batch of tensors,phymbert,2024-03-18T12:15:45Z,2024-03-19T11:05:44+00:00,8,22.83
6137,backend : set max split inputs to GGML_MAX_SRC,slaren,2024-03-18T13:10:31Z,2024-03-18T15:33:45+00:00,1,2.39
6140,ci : exempt some labels from being tagged as stale,slaren,2024-03-18T16:53:00Z,2024-03-19T08:06:54+00:00,1,15.23
6141,[SYCL] Revisited & updated SYCL build documentation,OuadiElfarouki,2024-03-18T17:33:06Z,2024-03-28T16:01:47+00:00,9,238.48
6144,Allow conversion of Llama / Mistral HF models,pcuenca,2024-03-18T17:58:44Z,2024-03-29T07:15:00+00:00,4,253.27
6145,Print usage on '-h' and '--help'.,dranger003,2024-03-18T18:22:21Z,2024-03-19T05:59:36+00:00,1,11.62
6146,server tests : more pythonic process management; fix bare `except:`,cebtenzzre,2024-03-18T18:39:44Z,2024-03-20T05:33:49+00:00,2,34.9
6151,update readme sycl for new update,NeoZhangJianyu,2024-03-19T02:02:39Z,2024-03-20T03:21:42+00:00,14,25.32
6152,Add a MobileVLM_V2-1.7B backup,ZiangWu-77,2024-03-19T04:18:53Z,2024-03-20T11:20:37+00:00,1,31.03
6155,Vulkan k-quant mmq and ggml-backend offload functionality,0cc4m,2024-03-19T09:44:51Z,2024-03-29T16:29:21+00:00,4,246.74
6157,[SYCL] Add nvidia and amd backends index,AidanBeltonS,2024-03-19T11:02:41Z,2024-03-21T06:10:53+00:00,3,43.14
6158,Remove undeed header file.,dranger003,2024-03-19T11:40:08Z,2024-03-19T16:16:10+00:00,2,4.6
6159,[SYCL] increase igpu cluster limit,abhilash1910,2024-03-19T12:27:28Z,2024-03-20T02:58:50+00:00,3,14.52
6164,[SYCL] Fix batched impl for NVidia GPU,AidanBeltonS,2024-03-19T16:11:05Z,2024-03-27T08:16:40+00:00,9,184.09
6167,nix: windows build,hutli,2024-03-19T20:07:15Z,2024-03-28T07:48:28+00:00,36,203.69
6169,Server: version bump for httplib and json,ngxson,2024-03-19T20:35:25Z,2024-03-20T12:30:36+00:00,1,15.92
6170,cuda : refactor to remove global resources,slaren,2024-03-19T21:49:42Z,2024-03-20T13:43:00+00:00,1,15.89
6174,Server: Handle n_keep parameter in the request,jkarthic,2024-03-20T09:59:19Z,2024-03-20T11:02:35+00:00,5,1.05
6175,Add MobileVLM_V2 backup,ZiangWu-77,2024-03-20T13:28:12Z,2024-03-20T15:02:32+00:00,1,1.57
6181,doc: fix typo in MobileVLM-README.md,ZiangWu-77,2024-03-20T15:29:48Z,2024-03-28T04:03:30+00:00,1,180.56
6182,Add mac prebuilds to llama.cpp,Vaibhavs10,2024-03-20T16:55:48Z,2024-03-21T09:13:12+00:00,1,16.29
6183,"Add ability to use Q5_0, Q5_1, and IQ4_NL for quantized K cache",ikawrakow,2024-03-20T17:33:07Z,2024-03-21T07:27:57+00:00,4,13.91
6185,cuda : print the returned error when CUDA initialization fails,slaren,2024-03-20T19:03:15Z,2024-03-20T20:03:26+00:00,1,1.0
6186,cuda : fix conflict with std::swap,slaren,2024-03-20T20:39:32Z,2024-03-21T00:47:47+00:00,1,4.14
6187,llama_model_loader: support multiple split/shard GGUFs,phymbert,2024-03-20T21:31:49Z,2024-03-22T18:00:02+00:00,52,44.47
6188,Make tokenize CLI tool have nicer command line arguments.,Noeda,2024-03-20T23:00:17Z,2024-05-25T01:14:42+00:00,12,1562.24
6192,common: llama_load_model_from_url split support ,phymbert,2024-03-21T07:42:14Z,2024-03-23T17:07:00+00:00,7,57.41
6193,add `retrieval` example,mscheong01,2024-03-21T08:10:47Z,2024-03-25T07:38:22+00:00,22,95.46
6196,Make IQ4_NL quantization be the same on CPU/CUDA/Metal when quantizing K-cache,ikawrakow,2024-03-21T10:23:55Z,2024-03-21T12:59:38+00:00,2,2.6
6199,Corrected typo to wrong file,semidark,2024-03-21T14:23:10Z,2024-03-21T17:52:35+00:00,1,3.49
6203,Fix params underscore convert to dash.,dranger003,2024-03-21T15:17:19Z,2024-03-22T01:32:42+00:00,2,10.26
6204,Add grok-1 support,arki05,2024-03-21T17:38:41Z,2024-03-23T16:41:54+00:00,1,47.05
6207,tests: conditional python & node json schema tests,ochafik,2024-03-21T18:50:33Z,2024-03-22T13:09:08+00:00,1,18.31
6208,cuda : add LLAMA_CUDA_NO_PEER_COPY to workaround broken ROCm p2p copy,slaren,2024-03-21T19:00:17Z,2024-03-22T13:05:32+00:00,1,18.09
6209,correction of the attn.v.weight quantization for IQ3_XS,Nexesenex,2024-03-21T19:01:26Z,2024-03-22T13:32:02+00:00,1,18.51
6210,move BLAS to a separate backend,slaren,2024-03-21T19:07:33Z,2024-06-13T01:11:35+00:00,2,1998.07
6211,server : fix n_keep always showing as 0 in response,kaetemi,2024-03-21T20:08:54Z,2024-03-22T11:12:05+00:00,1,15.05
6213,server : update readme doc from `slot_id` to `id_slot`,kaetemi,2024-03-21T20:59:10Z,2024-03-21T22:41:24+00:00,1,1.7
6214,Add CURL flag for the mac builds.,Vaibhavs10,2024-03-21T21:19:40Z,2024-03-22T07:53:43+00:00,1,10.57
6217,[SYCL] offload op,airMeng,2024-03-22T02:22:40Z,2024-03-24T04:04:25+00:00,2,49.7
6219,readme: add RecurseChat to the list of UIs,xyc,2024-03-22T04:29:02Z,2024-03-22T11:29:49+00:00,1,7.01
6231,server : enable continuous batching by default,ggerganov,2024-03-22T09:17:50Z,2024-03-22T11:08:28+00:00,1,1.84
6232,"json-schema-to-grammar: fix order of props in C++, support non-string const/enum",ochafik,2024-03-22T09:40:43Z,2024-03-22T13:07:44+00:00,1,3.45
6237,convert-llama2c-to-ggml: enable conversion of multiqueries,fraxy-v,2024-03-22T14:06:27Z,2024-03-22T18:49:06+00:00,1,4.71
6239,quantize: be able to explicitly specify quantization type of output and token embedding tensors,ikawrakow,2024-03-22T14:38:24Z,2024-03-22T18:47:14+00:00,2,4.15
6240,sampling: remove duplicated code for probability distribution access,mscheong01,2024-03-22T14:50:45Z,2024-03-24T08:54:07+00:00,6,42.06
6241,Support build win release for SYCL ,NeoZhangJianyu,2024-03-22T15:08:37Z,2024-03-24T01:44:01+00:00,1,34.59
6245,Fallback to tokenizer.json if vocab.json does not exist,CISC,2024-03-22T21:16:38Z,2024-03-28T15:44:38+00:00,4,138.47
6248,use _wfopen instead of fopen on Windows,cebtenzzre,2024-03-23T02:24:28Z,2024-03-23T22:48:02+00:00,1,20.39
6252,llama : add Deepseek support #5981,dragnil1,2024-03-23T08:55:56Z,2024-06-15T21:15:17+00:00,5,2028.32
6253,server: flush stdout after logging in both text and json layout,phymbert,2024-03-23T09:50:35Z,2024-03-23T12:18:45+00:00,1,2.47
6254,"server: docs: `--threads` and `--threads`, `--ubatch-size`, `--log-disable`",phymbert,2024-03-23T10:01:47Z,2024-03-23T17:00:38+00:00,9,6.98
6266,nix: update flake.lock,ggerganov,2024-03-24T00:18:05Z,2024-03-25T15:22:27+00:00,2,39.07
6269,cuda : refactor into multiple files,slaren,2024-03-24T03:09:07Z,2024-03-25T12:50:23+00:00,1,33.69
6270,"ci: close inactive issue, increase operations per run",phymbert,2024-03-24T06:19:27Z,2024-03-24T08:57:07+00:00,1,2.63
6271,imatrix : fix wname for mul_mat_id ops,ggerganov,2024-03-24T07:01:53Z,2024-03-24T14:18:45+00:00,2,7.28
6272,Fix heap corruption from wmode out-of-bound writes on windows,TheFlipbook,2024-03-24T08:34:22Z,2024-03-24T21:45:56+00:00,1,13.19
6273,Fixed lookup compilation issues on Windows,JohannesGaessler,2024-03-24T08:42:13Z,2024-03-24T13:21:17+00:00,1,4.65
6280,Support AVX512VNNI,jart,2024-03-24T15:35:20Z,2024-03-25T05:39:56+00:00,1,14.08
6281,nix: fix blas support,ck3d,2024-03-24T15:36:24Z,2024-03-25T17:52:45+00:00,1,26.27
6283,server: continuous performance monitoring and PR comment,phymbert,2024-03-24T16:34:00Z,2024-03-27T19:26:50+00:00,13,74.88
6284,Server: clean up OAI params parsing function,ngxson,2024-03-24T18:35:00Z,2024-03-25T08:42:17+00:00,2,14.12
6290,[SYCL] fix SYCL backend build on windows is break by LOG() error,NeoZhangJianyu,2024-03-25T02:01:49Z,2024-03-25T07:52:41+00:00,1,5.85
6296,"embedding: adjust `n_ubatch` value, print error on insufficient `n_batch` value",mscheong01,2024-03-25T11:24:45Z,2024-03-26T09:11:47+00:00,4,21.78
6299,cuda : rename build flag to LLAMA_CUDA,slaren,2024-03-25T15:08:18Z,2024-03-26T00:16:01+00:00,1,9.13
6300,server : add `n_discard` parameter to specify the number of tokens to discard when context is shifted,kaetemi,2024-03-25T15:13:24Z,2024-03-26T08:47:43+00:00,1,17.57
6301,[Model] Add support for xverse,hxer7963,2024-03-25T16:13:40Z,2024-03-29T13:37:03+00:00,8,93.39
6302,IQ1_M: 1.75 bpw quantization,ikawrakow,2024-03-25T16:48:49Z,2024-03-26T14:21:27+00:00,2,21.54
6305,wpm : portable unicode tolower,cebtenzzre,2024-03-25T20:16:53Z,2024-03-26T21:46:21+00:00,1,25.49
6307,Update llava-cli.cpp to support comma-delimited image lists,cpumaxx,2024-03-25T23:51:46Z,2024-04-29T03:47:34+00:00,6,819.93
6310,IQ1_XS FTYPE quant strategy,Nexesenex,2024-03-26T02:01:53Z,2024-08-11T01:12:08+00:00,5,3311.17
6314,fix no file in win rel for sycl,NeoZhangJianyu,2024-03-26T03:32:38Z,2024-03-27T01:47:06+00:00,1,22.24
6320,[convert-hf] Fix exception in sentencepiece with added tokens,pcuenca,2024-03-26T09:41:33Z,2024-03-26T12:32:19+00:00,1,2.85
6321,quantize: be able to override metadata by key,ikawrakow,2024-03-26T10:04:44Z,2024-03-26T12:09:31+00:00,1,2.08
6325,server: support reverse proxy setup with custom route,EZForever,2024-03-26T13:46:28Z,2024-03-27T05:55:30+00:00,1,16.15
6326,add php api bindings to readme,mcharytoniuk,2024-03-26T16:41:41Z,2024-03-27T07:08:59+00:00,1,14.46
6327,Make IQ1_M work for QK_K = 64,ikawrakow,2024-03-26T18:22:43Z,2024-03-27T07:44:27+00:00,1,13.36
6334,Change --no-penalize-nl to --penalize-nl,CISC,2024-03-26T23:14:14Z,2024-03-27T07:23:10+00:00,1,8.15
6336,doc: fix outdated default value of batch size,Sunt-ing,2024-03-27T03:47:34Z,2024-03-28T08:51:06+00:00,1,29.06
6339,fix crash when set main gpu,NeoZhangJianyu,2024-03-27T08:11:15Z,2024-03-28T00:55:24+00:00,1,16.74
6341,llama : save and restore kv cache for single seq id,kaetemi,2024-03-27T09:16:23Z,2024-04-08T12:43:31+00:00,31,291.45
6343,split: allow --split-max-size option,ngxson,2024-03-27T11:58:30Z,2024-03-29T21:34:44+00:00,9,57.6
6348,server : stop gracefully on SIGTERM,EZForever,2024-03-27T16:17:31Z,2024-03-28T08:50:48+00:00,1,16.55
6349,nix: ci: dont test cuda and rocm (for now),SomeoneSerge,2024-03-27T16:19:22Z,2024-03-27T19:18:55+00:00,2,2.99
6351,sync : ggml,ggerganov,2024-03-27T17:06:10Z,2024-03-29T15:45:46+00:00,1,46.66
6354,Create Security Policy,joycebrum,2024-03-27T21:11:46Z,2024-04-03T17:48:07+00:00,4,164.61
6355,convert : refactor vocab selection logic,cebtenzzre,2024-03-27T22:20:12Z,2024-03-28T15:44:36+00:00,1,17.41
6363,[SYCL] Iq4 nl,abhilash1910,2024-03-28T08:34:38Z,2024-04-08T03:07:50+00:00,1,258.55
6364,Fixed some MobileVLM's inference bugs. Added more tests on different devices.,ZiangWu-77,2024-03-28T09:35:00Z,2024-03-28T14:33:10+00:00,1,4.97
6367,llama : fix command-r inference when omitting outputs,compilade,2024-03-28T10:32:45Z,2024-03-28T12:05:54+00:00,2,1.55
6369,llama: remove redundant reshape in build_kv_store,danbev,2024-03-28T14:34:07Z,2024-03-29T07:23:22+00:00,1,16.82
6370,cmake: add explicit metal version options,mattjcly,2024-03-28T14:38:59Z,2024-03-29T07:27:43+00:00,1,16.81
6371,"Consider adding Layla, an android app that allows loading any GGUF models on Android phones to the list of supported UIs?",l3utterfly,2024-03-28T15:07:31Z,2024-05-09T13:32:41+00:00,3,1006.42
6374,CUDA: Faster FlashAttention kernel,JohannesGaessler,2024-03-28T19:22:14Z,2024-04-02T10:48:13+00:00,1,111.43
6378,Add llama_chat_apply_antiprompt,danemadsen,2024-03-29T01:47:57Z,2024-04-01T10:22:34+00:00,1,80.58
6387,ggml : update mul_mat_id to use the same tensor for all the experts,slaren,2024-03-29T18:16:42Z,2024-04-03T13:07:05+00:00,7,114.84
6388,Fedora build update,Man2Dev,2024-03-29T19:00:57Z,2024-03-29T21:59:56+00:00,1,2.98
6393,ci: bench: fix Resource not accessible by integration on PR event,phymbert,2024-03-30T06:43:21Z,2024-03-30T10:36:07+00:00,1,3.88
6397,"Add OpenChat, Alpaca, Vicuna chat templates",kaizau,2024-03-30T09:41:40Z,2024-04-03T15:24:31+00:00,10,101.71
6402,nix: update flake.lock,ggerganov,2024-03-31T00:18:12Z,2024-04-01T16:05:57+00:00,1,39.8
6403,kompute: implement op_getrows_f32,woachk,2024-03-31T06:21:49Z,2024-06-03T05:32:17+00:00,1,1535.17
6405,license : add AUTHORS,ggerganov,2024-03-31T07:39:49Z,2024-04-09T06:23:19+00:00,3,214.72
6408,[SYCL] refactor,airMeng,2024-03-31T10:54:09Z,2024-06-19T01:11:51+00:00,9,1910.3
6409,ci: server: verify deps are coherent with the commit,phymbert,2024-03-31T11:21:30Z,2024-04-01T10:36:40+00:00,1,23.25
6412,Introduce bfloat16 support,jart,2024-03-31T14:45:56Z,2024-05-08T06:30:09+00:00,5,903.74
6413,Server: Unix Socket Support,adrianliechti,2024-03-31T22:15:51Z,2024-06-30T15:46:35+00:00,1,2177.51
6414,Improve cpu prompt eval speed,jart,2024-04-01T00:55:47Z,2024-04-16T18:55:31+00:00,12,378.0
6428,A few small fixes to server's README docs,fat-tire,2024-04-02T00:09:34Z,2024-04-03T20:22:57+00:00,4,44.22
6431,server: allow setting penalize-nl on server webpage,sha224,2024-04-02T03:12:27Z,2024-04-04T15:03:00+00:00,7,59.84
6435,[SYCL] Disable iqx on windows as WA,airMeng,2024-04-02T07:13:52Z,2024-04-03T02:34:40+00:00,1,19.35
6443,Missing tokenizer.model error during gguf conversion,overtunned,2024-04-02T22:10:27Z,2024-04-03T15:42:52+00:00,3,17.54
6448,add SEA-LION support,bryanSwk,2024-04-03T03:10:23Z,2024-04-03T18:05:10+00:00,6,14.91
6452,Handle server exception on wrong type in request,JH23X,2024-04-03T06:20:17Z,2024-04-03T18:09:53+00:00,3,11.83
6454,add loongarch lsx and lasx optimize code,junchao-loongson,2024-04-03T07:54:35Z,2024-05-20T07:19:21+00:00,9,1127.41
6456,"CI maintenance: Update checkout, setup-python and upload-artifact to latest versions",EwoutH,2024-04-03T13:15:27Z,2024-04-03T18:01:13+00:00,2,4.76
6457,Run make to build the project,limitedAtonement,2024-04-03T13:22:44Z,2024-04-07T11:05:40+00:00,5,93.72
6458,Correct README link,limitedAtonement,2024-04-03T13:27:29Z,2024-04-04T14:30:02+00:00,1,25.04
6461,server: add cURL support to `server.Dockerfile`,elepedus,2024-04-03T15:46:47Z,2024-04-03T17:56:38+00:00,1,2.16
6464,[SYCL] Fixed minor bug when enabling FP16 for non intel targets,OuadiElfarouki,2024-04-03T16:56:22Z,2024-04-05T13:35:06+00:00,2,44.65
6466,"ci: bench: add more ftype, fix triggers and bot comment",phymbert,2024-04-03T19:53:18Z,2024-04-04T09:57:58+00:00,1,14.08
6467,Added support for . (any character) token in grammar engine.,HanClinto,2024-04-04T02:51:53Z,2024-06-06T13:08:52+00:00,6,1522.28
6468,server: add option to disable KV offload,jxy,2024-04-04T03:25:10Z,2024-04-04T06:33:48+00:00,1,3.14
6470,Fix for lint error complaining of bare except during CI builds.,HanClinto,2024-04-04T04:37:32Z,2024-04-04T06:32:53+00:00,1,1.92
6471,common: remove duplicate check for curl,danbev,2024-04-04T05:23:26Z,2024-04-04T07:49:22+00:00,4,2.43
6472,Tests: Added integration tests for GBNF parser ,HanClinto,2024-04-04T06:34:38Z,2024-04-06T14:31:33+00:00,1,55.95
6474,server: add cURL support to server Dockerfiles,elepedus,2024-04-04T09:41:51Z,2024-04-04T16:31:22+00:00,2,6.83
6478,ci: bench fix concurrency for workflow trigger dispatch with sha1,phymbert,2024-04-04T11:43:14Z,2024-04-04T14:59:04+00:00,1,3.26
6481,typo error in README file,junnjiee,2024-04-04T14:36:02Z,2024-04-04T17:16:37+00:00,2,2.68
6487,Add project to README.md,alexpinel,2024-04-04T16:08:59Z,2024-04-04T17:22:51+00:00,1,1.23
6491,Add Command R Plus support,RefractAI,2024-04-04T17:29:31Z,2024-04-09T08:16:13+00:00,17,110.78
6495,ci: bench: support sse and fix prompt processing time / server: add tokens usage in stream OAI response,phymbert,2024-04-04T22:12:21Z,2024-04-06T03:40:47+00:00,1,29.47
6498,BERT tokenizer fixes,cebtenzzre,2024-04-04T23:01:57Z,2024-04-09T17:44:08+00:00,3,114.7
6500,bench: make n_batch and n_ubatch configurable in Batched bench,Sunt-ing,2024-04-05T02:16:20Z,2024-04-05T18:34:53+00:00,1,16.31
6503,Add MindMac to UI list,hugo53,2024-04-05T11:21:16Z,2024-04-05T18:39:43+00:00,3,7.31
6504,gguf.py: add licence and version to gguf writer,mofosyne,2024-04-05T12:37:16Z,2024-04-05T18:41:38+00:00,1,6.07
6505,ggml : group all experts in a single ggml_mul_mat_id,slaren,2024-04-05T13:23:20Z,2024-04-18T13:18:49+00:00,10,311.92
6511,convert.py: add python logging instead of print(),mofosyne,2024-04-06T13:50:45Z,2024-05-03T19:36:41+00:00,14,653.77
6513,Add GritLM as supported models.,dranger003,2024-04-06T15:33:48Z,2024-04-07T17:33:59+00:00,1,26.0
6515,model: support arch `DbrxForCausalLM`,phymbert,2024-04-06T21:45:24Z,2024-04-13T09:33:52+00:00,15,155.81
6517,nix: update flake.lock,ggerganov,2024-04-07T00:18:24Z,2024-04-07T18:25:30+00:00,1,18.12
6519,llama_sampling_sample with default args is more naively usable ,TheFlipbook,2024-04-07T01:41:13Z,2024-04-08T13:02:30+00:00,5,35.35
6531,Comment explaining a decision,kunnis,2024-04-08T00:24:14Z,2024-04-08T15:44:19+00:00,2,15.33
6532,[SYCL] Remove condition in mmvq,abhilash1910,2024-04-08T03:27:59Z,2024-04-08T08:26:01+00:00,1,4.97
6535,Adding KodiBot to UI list,firatkiral,2024-04-08T06:35:37Z,2024-04-08T07:48:29+00:00,1,1.21
6539,The MLX Challenge,ggerganov,2024-04-08T09:55:49Z,2024-11-17T09:29:55+00:00,1,5351.57
6541,quantize : fix precedence of cli args,ggerganov,2024-04-08T11:26:51Z,2024-04-08T13:23:01+00:00,1,1.94
6550,llama : fix attention layer count sanity check,ggerganov,2024-04-08T18:18:50Z,2024-04-08T19:25:50+00:00,1,1.12
6554,server : detect search query to start webchat,Mardak,2024-04-08T22:42:52Z,2024-04-09T08:31:47+00:00,1,9.82
6555,"JSON schema conversion: ⚡️ faster repetitions, min/maxLength for strings, cap number length",ochafik,2024-04-08T22:45:23Z,2024-04-12T18:43:38+00:00,13,91.97
6556,metal : try to unify mul_mv_id kernels,slaren,2024-04-08T23:27:29Z,2024-04-12T16:13:20+00:00,2,88.76
6560,Adding eva to UI list,ylsdamxssjxxdd,2024-04-09T10:51:05Z,2024-04-10T06:34:00+00:00,1,19.72
6563,Fix more int overflow during quant (PPL/CUDA).,dranger003,2024-04-09T13:21:44Z,2024-04-28T22:38:44+00:00,15,465.28
6565,docs: how to add a model,phymbert,2024-04-09T14:27:39Z,2024-04-10T06:58:48+00:00,3,16.52
6572,minor layout improvements,rsoika,2024-04-09T19:00:54Z,2024-04-10T17:18:25+00:00,1,22.29
6575,Refactor Error Handling for CUDA,nneubacher,2024-04-10T01:52:31Z,2024-04-11T19:56:29+00:00,2,42.07
6576,eval-callback: Example how to use eval callback for debugging,phymbert,2024-04-10T01:57:22Z,2024-04-11T12:51:08+00:00,12,34.9
6578,Fix Qwen2-0.5B in convert-hf-to-gguf.py,jklj077,2024-04-10T03:11:01Z,2024-04-18T04:42:02+00:00,1,193.52
6579,Fix ROCm link 404 in README,artem-zinnatullin,2024-04-10T03:48:31Z,2024-04-10T06:49:12+00:00,1,3.01
6582,gguf: add option to not check tensor data,danbev,2024-04-10T07:11:57Z,2024-04-10T18:16:49+00:00,1,11.08
6588,Support converting models with multiple chat templates,CISC,2024-04-10T13:35:49Z,2024-04-18T11:49:01+00:00,1,190.22
6589,llama : add model types for mixtral,slaren,2024-04-10T14:24:00Z,2024-04-10T15:24:14+00:00,2,1.0
6590,Server Side Swift Support,spprichard,2024-04-10T16:33:02Z,2024-04-15T10:14:47+00:00,2,113.7
6591,Remove split metadata when quantize model shards,zj040045,2024-04-10T16:44:51Z,2024-04-12T10:45:06+00:00,11,42.0
6600,scripts : add --outdir option to hf.sh,danbev,2024-04-11T06:20:56Z,2024-04-11T13:22:47+00:00,2,7.03
6602,Support MiniCPM-2B-128k,zkh2016,2024-04-11T07:47:39Z,2024-05-13T10:46:12+00:00,10,770.98
6609,grammars: 1.5x faster inference w/ complex grammars (vector reserves / reuses),ochafik,2024-04-11T14:29:37Z,2024-04-11T18:47:34+00:00,7,4.3
6616,Grammar optimization: eliminate redundant grammar trees (~4x faster grammar sampling),HanClinto,2024-04-11T18:10:10Z,2024-04-12T01:44:50+00:00,4,7.58
6622,"fix memcpy() crash, add missed cmd in guide, fix softmax",NeoZhangJianyu,2024-04-12T01:49:05Z,2024-04-14T02:42:29+00:00,5,48.89
6625,chore: Fix markdown warnings,reneleonhardt,2024-04-12T04:26:57Z,2024-04-12T08:52:36+00:00,1,4.43
6626,infill : add download instructions for model,danbev,2024-04-12T05:59:52Z,2024-04-12T12:11:46+00:00,9,6.2
6628,ci : disable Metal for macOS-latest-cmake-x64,ggerganov,2024-04-12T07:12:54Z,2024-04-12T08:15:05+00:00,1,1.04
6635,Support StableLM2 12B,ashishdatta,2024-04-12T09:46:53Z,2024-04-16T15:48:35+00:00,41,102.03
6636,Fix cuda mul mat for pascal cc==610,xcnick,2024-04-12T10:31:06Z,2024-04-13T22:22:19+00:00,1,35.85
6638,server: stop generation at `n_ctx_train` if `n_predict` is not set,phymbert,2024-04-12T11:45:03Z,2024-04-26T10:15:30+00:00,3,334.51
6640,"grammars: x{min,max} repetition operator",ochafik,2024-04-12T14:18:54Z,2024-06-06T09:07:07+00:00,40,1314.8
6644,Extending grammar integration tests,HanClinto,2024-04-12T20:39:44Z,2024-04-29T18:40:14+00:00,8,406.01
6645,Enable the `--use-temp-file` cli flag,jac-jim,2024-04-12T22:53:28Z,2024-04-14T08:40:19+00:00,2,33.78
6646,CUDA: faster FlashAttention for batch sizes > 1,JohannesGaessler,2024-04-12T22:54:33Z,2024-04-18T11:15:32+00:00,3,132.35
6648,CPU F16->F32 conversion speed improvement,kunnis,2024-04-13T02:09:23Z,2024-04-23T06:18:41+00:00,2,244.16
6650,Add Command R chat template,jc19chaoj,2024-04-13T06:56:22Z,2024-04-14T16:16:34+00:00,1,33.34
6655,Fix --split-max-size,CISC,2024-04-13T10:10:17Z,2024-04-14T11:12:59+00:00,4,25.05
6658,`quantize`: add imatrix and dataset metadata in GGUF,phymbert,2024-04-13T13:00:04Z,2024-04-26T18:06:33+00:00,17,317.11
6659,`main`: add --json-schema / -j flag,ochafik,2024-04-13T13:00:38Z,2024-04-15T17:35:21+00:00,1,52.58
6661,`build`: generate hex dump of server assets during build,ochafik,2024-04-13T15:39:43Z,2024-04-21T17:48:53+00:00,4,194.15
6662,Added support for GGML_OP_CLAMP in Metal,dave-fl,2024-04-13T16:25:42Z,2024-04-14T11:14:19+00:00,1,18.81
6664,add missing kv clear in llama_beam_search,dwrensha,2024-04-13T18:46:05Z,2024-04-14T19:24:15+00:00,1,24.64
6669,nix: update flake.lock,ggerganov,2024-04-14T00:19:49Z,2024-04-14T13:55:30+00:00,1,13.59
6682,"fix mul_mat_id() for new input, make the ut pass",NeoZhangJianyu,2024-04-15T06:15:30Z,2024-04-15T09:12:26+00:00,1,2.95
6688,Implement '--keep-split' to quantize model into several shards,zj040045,2024-04-15T14:38:10Z,2024-04-25T10:29:35+00:00,5,235.86
6689,gguf : add special tokens metadata for FIM/Infill,danbev,2024-04-15T14:52:45Z,2024-04-16T06:13:13+00:00,6,15.34
6695,perplexity : require positive --ctx-size arg,ggerganov,2024-04-15T18:56:25Z,2024-04-16T06:28:33+00:00,1,11.54
6699,gritlm : add --outdir option to hf.sh script,danbev,2024-04-16T04:21:09Z,2024-04-16T06:34:06+00:00,1,2.22
6704,Fix autoawq gemma convert,dengzheng-cloud,2024-04-16T13:07:49Z,2024-04-16T20:51:08+00:00,2,7.72
6707,add support of codeqwen due to tokenizer,JustinLin610,2024-04-16T16:46:38Z,2024-04-24T07:16:21+00:00,8,182.5
6709,llama : make general.name optional,ggerganov,2024-04-16T19:11:38Z,2024-04-16T20:50:39+00:00,2,1.65
6735,llama : fix compatibility with old 2 expert models,slaren,2024-04-18T01:54:49Z,2024-04-18T07:04:47+00:00,2,5.17
6738,Qwen2: assume tied weights if lm_head/output weights is missing,jklj077,2024-04-18T09:55:54Z,2024-04-18T11:38:04+00:00,1,1.7
6741,Implement the OLMo architecture,nopperl,2024-04-18T12:49:59Z,2024-04-19T09:35:54+00:00,10,20.77
6745,Support Llama 3 conversion,pcuenca,2024-04-18T16:39:12Z,2024-04-21T11:50:42+00:00,7,67.19
6748,ci: add ubuntu latest release and fix missing build number (mac & ubuntu),loonerin,2024-04-18T17:10:34Z,2024-04-19T17:03:35+00:00,2,23.88
6751,Added llama-3 chat template,DifferentialityDevelopment,2024-04-18T21:53:08Z,2024-04-21T13:03:39+00:00,13,63.18
6752,Add general name to train,teleprint-me,2024-04-18T23:45:13Z,2024-04-19T07:16:46+00:00,1,7.53
6757,Adding support to take in a Hugging Face user token to pull models directly with authorization.,sourabratabose,2024-04-19T06:04:10Z,2024-06-19T07:11:30+00:00,5,1465.12
6761,gguf-py: Add IQ1_M to GGML_QUANT_SIZES,pmysl,2024-04-19T07:52:28Z,2024-04-21T12:49:30+00:00,3,52.95
6765,server: static: upstream upgrade,phymbert,2024-04-19T10:08:04Z,2024-04-19T11:19:01+00:00,1,1.18
6766,Introduction of CUDA Graphs to LLama.cpp,agray3,2024-04-19T12:26:22Z,2024-05-08T20:55:50+00:00,25,464.49
6767,[SYCL] Windows default build instructions without -DLLAMA_SYCL_F16 flag activated,aahouzi,2024-04-19T13:04:59Z,2024-04-23T00:53:18+00:00,2,83.81
6773,Fix flash-attn for AMD,JohannesGaessler,2024-04-19T18:18:11Z,2024-04-30T10:32:56+00:00,1,256.25
6780,common : try to fix Android CI,ggerganov,2024-04-20T08:21:34Z,2024-04-20T10:27:13+00:00,1,2.09
6781,ci: fix job are cancelling each other,phymbert,2024-04-20T08:32:37Z,2024-04-22T11:22:55+00:00,1,50.84
6783,Fedora hardware acceleration packages,Man2Dev,2024-04-20T13:17:16Z,2024-04-21T12:32:05+00:00,5,23.25
6789,doc : add link to falcon,kaetemi,2024-04-20T16:27:38Z,2024-04-21T12:35:40+00:00,1,20.13
6796,llamafile : improve sgemm.cpp,jart,2024-04-20T20:17:31Z,2024-04-22T19:00:36+00:00,2,46.72
6797,llava : use logger in llava-cli,jart,2024-04-20T20:37:59Z,2024-04-21T12:19:04+00:00,1,15.68
6799,nix: update flake.lock,ggerganov,2024-04-21T00:17:49Z,2024-04-22T10:42:43+00:00,1,34.41
6807,llama : add option to render special/control tokens,ggerganov,2024-04-21T12:16:34Z,2024-04-21T15:36:45+00:00,1,3.34
6811,`grammars`: cache decoded token codepoints for faster sampling,ochafik,2024-04-21T16:55:32Z,2024-06-10T01:15:16+00:00,19,1184.33
6820,ggml : fix calloc argument ordering.,airlied,2024-04-22T05:31:09Z,2024-04-22T14:05:06+00:00,5,8.57
6826,feat: add potential to run Jina Embeddings architecture,JoanFM,2024-04-22T11:27:36Z,2024-05-11T07:46:10+00:00,1,452.31
6829,ggml : add RPC backend,rgerganov,2024-04-22T14:51:11Z,2024-05-14T11:27:19+00:00,25,524.6
6835,Server: fix seed for multiple slots,JohannesGaessler,2024-04-22T21:58:50Z,2024-04-24T09:08:36+00:00,12,35.16
6839,added implementation of DRY sampler,l3utterfly,2024-04-23T02:51:16Z,2024-11-05T07:35:43+00:00,12,4708.74
6844,Custom quantization schemes,jubruckne,2024-04-23T12:53:33Z,2025-09-17T14:20:07+00:00,4,12289.44
6852,add phi3 support,liuwei-git,2024-04-23T17:51:19Z,2024-04-24T07:00:37+00:00,1,13.15
6857,Add phi 3 chat template,tristandruyen,2024-04-23T23:28:56Z,2024-04-24T08:52:37+00:00,12,9.39
6860,Fix: Revert showing control tokens by default for server OpenAI Chat completions,K-Mistele,2024-04-24T03:35:06Z,2024-04-24T10:15:29+00:00,1,6.67
6862,add llama_get_pooling_type function,iamlemec,2024-04-24T04:44:58Z,2024-04-24T13:10:07+00:00,2,8.42
6866,convert : fix set_vocab_sentencepiece,ggerganov,2024-04-24T07:21:07Z,2024-05-18T05:46:21+00:00,2,574.42
6869,"ggml-qnn: add Qualcomm QNN(Qualcomm Neural Network,aka Qualcomm AI Engine Direct) backend",jeffzhou2000,2024-04-24T08:29:34Z,2024-07-19T08:22:51+00:00,73,2063.89
6881,README: add graphic for matrix multiplication,JohannesGaessler,2024-04-24T18:07:14Z,2024-04-24T19:29:13+00:00,1,1.37
6884,add basic tensor data validation function,slaren,2024-04-24T19:58:27Z,2024-04-26T16:39:58+00:00,2,44.69
6885,llama : check that all the tensor data is in the model file,slaren,2024-04-24T20:23:10Z,2024-04-25T13:23:47+00:00,1,17.01
6891,AVX Q4_0 and Q8_0 sgemm,netrunnereve,2024-04-25T04:01:13Z,2024-05-08T14:29:23+00:00,8,322.47
6894,clip : rename lerp function to avoid conflict,danbev,2024-04-25T06:31:28Z,2024-04-25T12:38:14+00:00,1,6.11
6899,add support for moondream vision language model,vikhyat,2024-04-25T08:15:25Z,2024-04-25T19:38:31+00:00,1,11.38
6905,gguf : enforce that tensor names are unique,ngxson,2024-04-25T11:43:44Z,2024-04-28T15:36:18+00:00,4,75.88
6910,Fix OLMo HF to GGUF conversion,nopperl,2024-04-25T15:40:54Z,2024-05-07T19:39:43+00:00,2,291.98
6915,ggml : use dynamic thread scheduling for matrix multiplication,kunnis,2024-04-26T00:15:00Z,2024-05-15T17:59:12+00:00,3,473.74
6920,llama : improve BPE pre-processing + LLaMA 3 and Deepseek support,ggerganov,2024-04-26T08:46:07Z,2024-04-29T13:58:42+00:00,18,77.21
6921,fixed off by one error when context shifting in main.cpp example,l3utterfly,2024-04-26T08:54:28Z,2024-05-01T19:27:41+00:00,1,130.55
6923,main : don't print special tokens with --grammar,jart,2024-04-26T09:16:12Z,2024-05-25T09:04:03+00:00,6,695.8
6930,Improve usability of --model-url & related flags,ochafik,2024-04-26T14:26:35Z,2024-04-29T23:52:50+00:00,18,81.44
6933,Reset schedule earlier to allow overlap with ggml graph computation on device,agray3,2024-04-26T15:55:03Z,2024-04-26T18:08:31+00:00,2,2.22
6935,ci: server: tests python env on github container ubuntu latest / fix n_predict,phymbert,2024-04-26T19:50:09Z,2024-04-27T15:50:49+00:00,1,20.01
6936,"perplexity: more statistics, added documentation",JohannesGaessler,2024-04-26T21:38:31Z,2024-04-30T21:36:27+00:00,1,95.97
6937,Fix conversion of some BERT embedding models,christianazinn,2024-04-26T22:33:43Z,2024-04-29T13:34:41+00:00,1,63.02
6940,"Implemented basic interface for llamacheck and link to weights, adapt…",Ferruolo,2024-04-27T02:30:33Z,2024-05-31T02:35:22+00:00,2,816.08
6942,Option to split during conversion,christianazinn,2024-04-27T04:23:58Z,2024-06-24T09:42:03+00:00,66,1397.3
6949,"Replace ""alternative"" boolean operator in conditional compilation directive",mgroeber9110,2024-04-27T17:19:10Z,2024-04-27T19:02:06+00:00,1,1.72
6950,"Server: add test for num slots, fails on master",JohannesGaessler,2024-04-27T21:48:54Z,2024-05-01T15:52:55+00:00,6,90.07
6951,move ndk code to a new library,eltonkola,2024-04-27T23:23:23Z,2024-05-14T07:30:30+00:00,5,392.12
6952,nix: update flake.lock,ggerganov,2024-04-28T00:18:31Z,2024-04-28T11:12:50+00:00,1,10.91
6958,server: avoid breaking KV cache when prompt >= n_ctx,prfd,2024-04-28T03:46:16Z,2024-07-08T02:06:05+00:00,2,1702.33
6959,[SYCL] add device version in SYCL device list,arthw,2024-04-28T08:13:55Z,2024-04-28T14:40:31+00:00,1,6.44
6962,use std::random_device{}() for default random seed,dwrensha,2024-04-28T12:49:45Z,2024-04-29T13:35:45+00:00,1,24.77
6964,build(cmake): simplify instructions (`cmake -B build && cmake --build build ...`),ochafik,2024-04-28T15:48:18Z,2024-04-29T16:02:45+00:00,11,24.24
6965,llama3 custom regex split,jaime-m-p,2024-04-28T17:52:29Z,2024-05-09T13:30:44+00:00,5,259.64
6967,ci : add building in MSYS2 environments (Windows),przemoc,2024-04-29T00:23:55Z,2024-04-29T12:59:48+00:00,1,12.6
6969,llava-cli: Add ability to analyze multiple images on a single command line without having to the reload the model,cpumaxx,2024-04-29T03:50:04Z,2024-04-29T14:34:24+00:00,1,10.74
6977,ggml : fix __MSC_VER -> _MSC_VER,ggerganov,2024-04-29T13:03:05Z,2024-04-29T14:55:02+00:00,1,1.87
6986,Attempt at OpenElm,joshcarp,2024-04-29T18:28:03Z,2024-05-07T19:04:43+00:00,5,192.61
6987,log more info when metal fails,bakkot,2024-04-29T19:58:22Z,2024-04-30T09:34:50+00:00,1,13.61
6999,add chatglm3-6b and glm-4-9b-chat model support,mnlife,2024-04-30T06:16:18Z,2024-06-27T05:49:50+00:00,10,1391.56
7013,"Update Server's README with undocumented options for RoPE, YaRN, and KV cache quantization",K-Mistele,2024-04-30T21:10:24Z,2024-05-07T18:44:29+00:00,4,165.57
7014,ci : exempt confirmed bugs from being tagged as stale,slaren,2024-04-30T21:49:46Z,2024-05-01T05:14:00+00:00,1,7.4
7016,Tidy Android Instructions README.md,Jeximo,2024-04-30T23:40:29Z,2024-05-04T16:10:16+00:00,10,88.5
7019,CUDA: implement __hmax and __hmax2 for CUDA < 11.7,JohannesGaessler,2024-05-01T08:07:07Z,2024-05-01T12:46:38+00:00,3,4.66
7020,Added support for the ArcticForCausalLM.,fairydreaming,2024-05-01T08:10:11Z,2024-05-24T12:31:14+00:00,15,556.35
7026,docs: Fix typo and update description for --embeddings flag,louixs,2024-05-01T14:54:17Z,2024-05-14T05:20:47+00:00,6,302.44
7027,"convert.py: When --vocab-only is passed, generate false but valid params",20kdc,2024-05-01T15:43:13Z,2024-05-08T12:22:32+00:00,1,164.66
7029,Update LOG_IMPL and LOG_TEE_IMPL,a-downing,2024-05-01T19:50:10Z,2024-05-01T21:31:30+00:00,1,1.69
7031,convert-hf : reduce repeated boilerplate from write_tensors,compilade,2024-05-01T23:39:15Z,2024-05-06T12:14:04+00:00,7,108.58
7032,chore: fix typo in llama.cpp,alwqx,2024-05-02T01:12:22Z,2024-05-02T15:56:41+00:00,3,14.74
7033,Add BPE pre-tokenization for Command-R.,dranger003,2024-05-02T01:21:22Z,2024-05-03T19:51:51+00:00,2,42.51
7034,Disable benchmark on forked repo,CISC,2024-05-02T03:14:10Z,2024-05-05T11:38:56+00:00,11,80.41
7038,Bug fix for server crash if first token is the stop word and asking for logprobs,maor-ps,2024-05-02T07:56:08Z,2024-05-04T09:06:40+00:00,1,49.18
7041,BPE pretokenizer - add support for command-r-plus and command-r models,sealad886,2024-05-02T10:56:10Z,2024-05-06T05:08:56+00:00,2,90.21
7045,llama : rename ctx to user_data in progress_callback,danbev,2024-05-02T14:49:33Z,2024-05-03T13:24:30+00:00,3,22.58
7051,Remove .attention from skipped tensors to match more accurately,bartowski1182,2024-05-02T20:36:57Z,2024-05-02T23:49:09+00:00,1,3.2
7059,add_special option for server tokenize endpoint,JohanAR,2024-05-03T12:47:40Z,2024-05-08T12:27:58+00:00,2,119.67
7061,CUDA: generalize FP16 fattn vec kernel,JohannesGaessler,2024-05-03T18:16:50Z,2024-05-09T12:32:02+00:00,1,138.25
7064,Fix Linux /sys cpu path to guess number of cores,viric,2024-05-03T20:32:26Z,2024-05-04T13:26:53+00:00,1,16.91
7065,Add Note to Readme that LLaMA 3 is Not Supported for convert.py ,lyledean1,2024-05-03T20:38:52Z,2024-05-05T05:21:46+00:00,1,32.72
7067,Add an option to build without CUDA VMM,WilliamTambellini,2024-05-04T00:12:08Z,2024-05-06T18:12:15+00:00,3,66.0
7072,gguf-split: add `--no-tensor-first-split` option,ngxson,2024-05-04T13:03:28Z,2024-05-04T16:56:22+00:00,2,3.88
7075,convert-hf : save memory with lazy evaluation,compilade,2024-05-04T14:41:41Z,2024-05-08T22:16:38+00:00,2,103.58
7077,Further tidy on Android instructions README.md,Jeximo,2024-05-04T18:24:08Z,2024-05-08T00:26:43+00:00,4,78.04
7078,fix: use `vm_allocate` instead of `posix_memalign` for Metal on macOS,giladgd,2024-05-04T21:36:05Z,2024-05-08T19:08:11+00:00,1,93.53
7079,nix: update flake.lock,ggerganov,2024-05-05T00:17:58Z,2024-05-06T15:36:06+00:00,1,39.3
7080,Adding support for the --numa argument for benchmarking.,kunnis,2024-05-05T03:12:46Z,2024-05-05T12:17:47+00:00,2,9.08
7083,Add left recursion check: quit early instead of going into an infinite loop,nuchi,2024-05-05T05:45:14Z,2024-05-14T05:25:56+00:00,4,215.68
7084,Vulkan Bugfixes and Improvements,0cc4m,2024-05-05T05:57:27Z,2024-05-09T18:39:54+00:00,2,108.71
7090,opencl alignment size should be converted from bits to bytes,albertjin,2024-05-05T15:17:41Z,2024-05-09T09:34:37+00:00,2,90.28
7096,Scripting & documenting debugging one test without anything else in the loop.,josh-ramer,2024-05-06T05:40:22Z,2024-05-11T17:26:35+00:00,1,131.77
7097,main-interactive-mode: optionally allow for special tokens from user in interactive mode for fill-in-middle etal,hanishkvc,2024-05-06T06:11:02Z,2024-05-10T10:21:58+00:00,1,100.18
7098,ci : add GG_BUILD_EXTRA_TESTS_0 env,ggerganov,2024-05-06T07:13:16Z,2024-05-07T08:08:49+00:00,2,24.93
7099,Fixed save_imatrix to match old behaviour for MoE,jukofyork,2024-05-06T10:01:10Z,2024-05-08T00:24:16+00:00,15,38.38
7104,Update log text (EOS to EOG),RhinoDevel,2024-05-06T15:07:52Z,2024-05-07T17:51:31+00:00,1,26.73
7108,main : add --conversation / -cnv flag,dawidpotocki,2024-05-06T22:53:36Z,2024-05-08T14:32:32+00:00,1,39.65
7114,Add BPE pre-tokenization for Qwen2.,jklj077,2024-05-07T04:40:02Z,2024-05-08T12:06:43+00:00,1,31.44
7117,chore: Add model vocab support,teleprint-me,2024-05-07T05:52:34Z,2024-05-19T02:57:00+00:00,1,285.07
7122,Fix NFD computation,JoanFM,2024-05-07T12:14:50Z,2024-05-08T19:00:31+00:00,1,30.76
7124,docs: fix typos,omahs,2024-05-07T14:01:28Z,2024-05-07T15:20:33+00:00,1,1.32
7125,server: fix incorrectly reported token probabilities,JohannesGaessler,2024-05-07T14:23:29Z,2024-05-07T21:07:58+00:00,2,6.74
7132,Add BPE pre-tokenization for DBRX.,dranger003,2024-05-07T23:21:01Z,2024-05-08T10:43:24+00:00,2,11.37
7138,compare-llama-bench.py: add missing basicConfig,mofosyne,2024-05-08T05:34:01Z,2024-05-08T08:54:39+00:00,2,3.34
7142,Server: clean up json_value() function,ngxson,2024-05-08T09:48:06Z,2024-05-08T11:24:14+00:00,2,1.6
7143,"JSON: [key] -> .at(key), assert() -> GGML_ASSERT",JohannesGaessler,2024-05-08T09:48:35Z,2024-05-08T19:53:08+00:00,6,10.08
7150,perplexity: add BF16 vs. FP16 results,JohannesGaessler,2024-05-08T20:06:38Z,2024-05-13T11:03:27+00:00,1,110.95
7151,cmake : fix typo,cebtenzzre,2024-05-08T20:24:58Z,2024-05-08T23:55:32+00:00,1,3.51
7153,Server: Fix system_prompt handling,ngxson,2024-05-08T21:51:23Z,2024-05-11T15:28:10+00:00,2,65.61
7154,ggml : rewrite silu and softmax for cpu,jart,2024-05-09T00:26:51Z,2024-05-17T06:58:52+00:00,1,198.53
7158,convert-hf : support bfloat16 conversion,compilade,2024-05-09T04:06:10Z,2024-05-11T15:06:27+00:00,18,59.0
7160,llama : update llama_timings.n_p_eval setting,danbev,2024-05-09T04:43:53Z,2024-05-09T11:03:29+00:00,1,6.33
7162,TypoFix,AhmedZeer,2024-05-09T05:24:42Z,2024-05-09T08:16:45+00:00,1,2.87
7163,Fix moondream support,abetlen,2024-05-09T05:35:37Z,2024-05-10T06:41:10+00:00,1,25.09
7166,Add special token modification capability,CISC,2024-05-09T06:36:33Z,2024-05-09T10:56:00+00:00,1,4.32
7169,metal : fix flash attention kernel requirements,ggerganov,2024-05-09T08:20:08Z,2024-05-10T15:20:10+00:00,1,31.0
7172,[SYCL] Minor arithmetic improvement to MMVQ wrapper kernel,OuadiElfarouki,2024-05-09T09:33:54Z,2024-05-10T00:32:15+00:00,2,14.97
7184,eval-callback : fix conversion to float,slaren,2024-05-09T19:42:20Z,2024-05-09T23:04:13+00:00,2,3.36
7188,CUDA: add FP32 FlashAttention vector kernel,JohannesGaessler,2024-05-09T22:08:40Z,2024-05-12T17:40:45+00:00,2,67.53
7191,Add support for properly optimized Windows ARM64 builds with LLVM and MSVC,max-krasnyansky,2024-05-10T02:55:00Z,2024-05-16T02:47:36+00:00,11,143.88
7192,ggml : full ALiBi support,ggerganov,2024-05-10T07:49:36Z,2024-05-11T07:32:41+00:00,9,23.72
7193,fix : lookup word in vocab before doing BPE merges,tonyfettes,2024-05-10T08:04:27Z,2024-05-11T08:12:06+00:00,3,24.13
7194,Fix memory bug in grammar parser,jart,2024-05-10T08:59:26Z,2024-05-10T11:01:08+00:00,2,2.03
7199,llama-bench : add pp+tg test type,slaren,2024-05-10T11:53:27Z,2024-05-10T16:03:54+00:00,1,4.17
7203,server: fix reported top tokens for temperature 0,JohannesGaessler,2024-05-10T15:55:38Z,2024-05-11T08:11:28+00:00,1,16.26
7204,remove convert-lora-to-ggml.py,slaren,2024-05-10T16:25:12Z,2024-05-12T00:29:33+00:00,2,32.07
7210,error capturing in convert-hf-to-gguf-update if repo not reachable due to lack of licensed access,CrispStrobe,2024-05-10T21:13:00Z,2024-05-11T08:18:35+00:00,1,11.09
7212,[server] Cleanup a memory leak on exit,stevegrubb,2024-05-11T02:42:46Z,2024-05-11T08:13:02+00:00,1,5.5
7225,Add phi3 128K model support,liuwei-git,2024-05-11T19:03:00Z,2024-05-21T20:28:32+00:00,4,241.43
7226,change default temperature of OAI compat API from 0 to 1,Kartoffelsaft,2024-05-11T19:19:09Z,2024-05-13T02:40:08+00:00,1,31.35
7229,Better ccache guide,ibehnam,2024-05-11T23:55:14Z,2024-05-22T07:09:40+00:00,1,247.24
7232,nix: update flake.lock,ggerganov,2024-05-12T00:42:00Z,2024-05-24T15:59:06+00:00,1,303.29
7233,[SYCL]rm wait() to improve the performance,arthw,2024-05-12T03:21:41Z,2024-05-13T10:11:27+00:00,1,30.83
7234,convert-hf : support direct Q8_0 conversion,compilade,2024-05-12T03:39:13Z,2024-05-13T18:10:51+00:00,4,38.53
7235,[SYCL]update CI with oneapi 2024.1,arthw,2024-05-12T05:13:31Z,2024-05-13T00:02:55+00:00,1,18.82
7237,Update and fix Vulkan soft_max and argsort implementations,0cc4m,2024-05-12T07:00:10Z,2024-05-18T06:10:58+00:00,1,143.18
7241,[SYCL] Add oneapi runtime dll files to win release package,arthw,2024-05-12T13:22:37Z,2024-05-13T00:04:29+00:00,1,10.7
7245,Unicode codepoint flags for custom regexs,jaime-m-p,2024-05-12T23:45:46Z,2024-05-17T23:09:13+00:00,1,119.39
7246,Add example script for rendering jinja2 templates,teleprint-me,2024-05-13T00:52:45Z,2025-01-20T23:48:35+00:00,37,6070.93
7248,llava-cli: fix base64 prompt,Adriankhl,2024-05-13T03:36:27Z,2024-05-13T14:02:37+00:00,1,10.44
7255,llava-cli: fix process prompt bug,M3Dade,2024-05-13T11:34:15Z,2024-05-14T02:24:50+00:00,1,14.84
7258,Windows support for AVX512_BF16 and associated bug fixes for BF16 model,Srihari-mcw,2024-05-13T13:56:43Z,2024-05-20T02:18:39+00:00,1,156.37
7263,cuda : add half2 __shfl_xor() for ROCm 5.5,Engininja2,2024-05-13T18:48:31Z,2024-05-18T08:05:17+00:00,1,109.28
7264,server: free sampling contexts on exit,stevegrubb,2024-05-13T19:09:23Z,2024-05-14T14:11:24+00:00,2,19.03
7265,llama : disable pipeline parallelism with nkvo,slaren,2024-05-13T21:19:05Z,2024-05-14T07:33:42+00:00,2,10.24
7267,ggml llama: align structs for memory optimization on 64-bit platforms,GermanAizek,2024-05-13T23:39:18Z,2024-07-24T15:11:02+00:00,1,1719.53
7272,"ggml-opencl, llama: using reserve() if count already known",GermanAizek,2024-05-14T01:35:01Z,2024-05-20T07:33:21+00:00,5,149.97
7273,"grammar, json, llama: replace push on emplace if it possible",GermanAizek,2024-05-14T01:55:45Z,2024-05-16T06:14:24+00:00,1,52.31
7274,"ggml-quants, llama: removed excess checks, it has already been checked before",GermanAizek,2024-05-14T02:26:38Z,2024-05-17T07:08:50+00:00,2,76.7
7279,Added a single test function script and fix debug-test.sh to be more robust,mofosyne,2024-05-14T11:56:46Z,2024-05-17T12:40:14+00:00,8,72.72
7284,server bench: fix bench not waiting for model load,JohannesGaessler,2024-05-14T13:34:52Z,2024-05-15T06:44:16+00:00,1,17.16
7286,avoid to get prompt in infill mode and embedding mode,woodx9,2024-05-14T18:01:49Z,2024-06-07T07:09:45+00:00,1,565.13
7288,doc: add references to hugging face GGUF-my-repo quantisation web tool.,Vaibhavs10,2024-05-14T21:02:04Z,2024-05-16T05:38:43+00:00,2,32.61
7290,ggml : tag ggml_tensor::backend as deprecated,slaren,2024-05-14T23:45:03Z,2024-05-15T13:08:48+00:00,1,13.4
7297,embedding: free the batch after execution,dm4,2024-05-15T09:17:19Z,2024-05-15T12:01:12+00:00,1,2.73
7298,Capture CUDA logging output,fraxy-v,2024-05-15T09:28:58Z,2024-05-18T22:44:42+00:00,5,85.26
7300,Add phi-2 tokenizer,BramVanroy,2024-05-15T10:56:29Z,2024-08-27T14:31:38+00:00,2,2499.59
7302,Avoid unnecessarily disabling CUDA graphs,agray3,2024-05-15T12:19:20Z,2024-05-15T13:44:49+00:00,1,1.42
7304,rpc : add command line arg for specifying backend memory,rgerganov,2024-05-15T12:33:10Z,2024-05-16T06:58:30+00:00,1,18.42
7305,server : add support for the RPC backend,rgerganov,2024-05-15T13:17:26Z,2024-05-17T07:00:17+00:00,1,41.71
7308,Qwen/Qwen-7b HF to GGUF Conversion Support Fix,amd-lalithnc,2024-05-15T17:19:56Z,2024-05-17T07:01:58+00:00,2,37.7
7310,readme : remove stray double quote,danbev,2024-05-15T18:51:15Z,2024-05-15T21:41:03+00:00,1,2.83
7313,ggml : fix quants nans when all the group weights are very close to zero,slaren,2024-05-15T21:34:23Z,2024-05-18T00:39:54+00:00,2,51.09
7314,CUDA: faster large batch FA without tensor cores,JohannesGaessler,2024-05-15T22:04:52Z,2024-05-17T16:54:53+00:00,1,42.83
7319,rpc : get available mem for the CPU backend,rgerganov,2024-05-16T07:27:04Z,2024-05-16T09:04:08+00:00,1,1.62
7320,rpc : set SO_REUSEADDR for the server socket,rgerganov,2024-05-16T10:09:35Z,2024-05-17T14:25:44+00:00,1,28.27
7321,[SYCL] Update SYCL upscale operation,AidanBeltonS,2024-05-16T10:12:14Z,2024-05-20T11:08:23+00:00,6,96.94
7324,"Add support for larger Granite Code Models (20B, 34B)",sroecker,2024-05-16T11:14:39Z,2024-05-18T08:04:55+00:00,1,44.84
7326,Fixed painfully slow single process builds.,jboero,2024-05-16T11:47:54Z,2024-05-30T20:32:39+00:00,1,344.75
7327,llama : use n_embd_head_v instead of n_embd_head_k when reshaping kqv,fairydreaming,2024-05-16T12:37:17Z,2024-05-17T11:24:38+00:00,1,22.79
7328,Viking tokenizer support,akx,2024-05-16T13:22:11Z,2024-07-01T08:17:31+00:00,3,1098.92
7330,github-actions-labeler: initial commit,mofosyne,2024-05-16T13:43:34Z,2024-05-18T06:04:23+00:00,1,40.35
7332,tokenization: add warning for double BOS,JohannesGaessler,2024-05-16T15:51:17Z,2024-05-17T07:59:57+00:00,1,16.14
7335,[Server] Added --verbose option to README,reuank,2024-05-16T20:48:37Z,2024-05-17T00:11:03+00:00,1,3.37
7341,"android : use ""ci-android"" branch for CI",ggerganov,2024-05-17T12:49:02Z,2024-05-18T10:40:39+00:00,1,21.86
7342,Another threadpool: Avoid creating hundreds of threads in GGML,besnardjb,2024-05-17T14:01:55Z,2024-06-04T08:44:04+00:00,2,426.7
7347,server: add test for token probs,JohannesGaessler,2024-05-17T17:15:53Z,2024-05-19T14:26:02+00:00,1,45.17
7348,Fix floating point error with ndot progress printing in Perplexity and show stats with < 100 tasks,strawberrymelonpanda,2024-05-17T17:37:39Z,2024-05-18T07:57:08+00:00,1,14.32
7349,Add StableLM2 pre-tokenizer,aahouzi,2024-05-17T19:19:04Z,2024-05-19T12:46:46+00:00,1,41.46
7350,SimpleChat: a simple and dumb web front end for testing /chat/completions and /completions end points and try chat,hanishkvc,2024-05-17T19:21:10Z,2024-05-22T17:53:21+00:00,2,118.54
7352,CUDA: deduplicate FlashAttention code,JohannesGaessler,2024-05-17T22:03:24Z,2024-05-18T10:36:25+00:00,2,12.55
7353,examples: cache hf model when --model not provided,amirzia,2024-05-17T22:31:23Z,2024-05-21T14:13:12+00:00,17,87.7
7359,OpenELM support,icecream95,2024-05-18T07:53:25Z,2024-07-04T17:14:21+00:00,7,1137.35
7360,Vulkan Embedding Fix,0cc4m,2024-05-18T08:21:47Z,2024-05-19T15:19:53+00:00,1,30.97
7363,labeler.yml: Use settings from ggerganov/llama.cpp [no ci],mofosyne,2024-05-18T10:19:42Z,2024-05-19T10:51:03+00:00,1,24.52
7372,ggml: implement quantized KV cache for FA,JohannesGaessler,2024-05-18T20:34:22Z,2024-05-19T14:46:13+00:00,1,18.2
7374,fix inverted strcmp checking for `quantize --keep-split`,fredlas,2024-05-18T21:07:19Z,2024-05-19T16:37:04+00:00,1,19.5
7375,Tokenizer SPM fixes for phi-3 and llama-spm,jaime-m-p,2024-05-18T22:34:17Z,2024-05-20T18:15:57+00:00,1,43.69
7376,cuda : clear error after buffer allocation failure,slaren,2024-05-18T22:41:20Z,2024-05-19T12:19:37+00:00,1,13.64
7378,rpc: free buffer after client disconnect,chraac,2024-05-19T02:56:24Z,2024-05-20T08:04:57+00:00,3,29.14
7379,Automate vocab support and model conversion,teleprint-me,2024-05-19T02:56:24Z,2025-01-20T23:48:44+00:00,24,5924.87
7382,server: fix seed being reported back,JohannesGaessler,2024-05-19T12:23:31Z,2024-05-19T14:06:33+00:00,1,1.72
7388,server : tuning tests,ggerganov,2024-05-19T15:48:58Z,2024-05-20T07:16:41+00:00,3,15.46
7389,server : return error on too large embedding input,ggerganov,2024-05-19T16:03:00Z,2024-05-20T05:56:05+00:00,1,13.88
7395,llama : remove MPI backend,slaren,2024-05-19T17:45:05Z,2024-05-19T23:17:04+00:00,1,5.53
7397,CUDA: deduplicate mmq code,JohannesGaessler,2024-05-19T20:19:07Z,2024-05-21T14:02:12+00:00,2,41.72
7402,Add Smaug 70B support to conversion,bartowski1182,2024-05-20T00:44:17Z,2024-05-26T12:28:36+00:00,1,155.74
7408,llama : remove Persimmon,ggerganov,2024-05-20T07:54:43Z,2024-05-20T16:35:28+00:00,1,8.68
7409,server : fix temperature + disable some tests,ggerganov,2024-05-20T08:18:50Z,2024-05-20T12:10:03+00:00,1,3.85
7410,Update README.md,binganao,2024-05-20T08:36:23Z,2024-05-20T09:55:35+00:00,1,1.32
7411,rpc : track allocated buffers,rgerganov,2024-05-20T09:39:02Z,2024-05-20T13:36:56+00:00,12,3.96
7413,perplexity: update README FP16 results [no ci],JohannesGaessler,2024-05-20T10:58:17Z,2024-05-20T16:15:38+00:00,1,5.29
7414,update HIP_UMA #7399,Djip007,2024-05-20T11:44:04Z,2024-05-27T23:40:47+00:00,5,179.95
7424,`grammars`: fix resampling logic regression,ochafik,2024-05-21T00:28:53Z,2024-05-21T19:40:00+00:00,1,19.19
7425,Tokenizer SPM fixes for phi-3 and llama-spm (bugfix),jaime-m-p,2024-05-21T00:35:13Z,2024-05-21T12:39:49+00:00,1,12.08
7426,vulkan: fix clang-cl debug build,Adriankhl,2024-05-21T01:59:27Z,2024-05-22T12:53:21+00:00,7,34.9
7430,Move convert.py to examples/convert-legacy-llama.py,Galunid,2024-05-21T04:36:50Z,2024-05-30T11:40:00+00:00,11,223.05
7433,"ggml: aarch64: implement SVE kernels for q8_0_q8_0, q4_0_q8_0 vector dot",msy-kato,2024-05-21T07:57:34Z,2024-05-25T08:42:31+00:00,1,96.75
7435,llama-bench : add support for the RPC backend,rgerganov,2024-05-21T10:59:42Z,2024-05-29T11:45:44+00:00,1,192.77
7436,[SYCL]fix ggml_sycl_mul_mat_id() to match the change of api,arthw,2024-05-21T13:58:12Z,2024-05-28T09:53:37+00:00,10,163.92
7438,[SYCL ]add build shared lib in win release package,arthw,2024-05-21T14:31:52Z,2024-05-24T02:06:56+00:00,2,59.58
7445,Add missing model type names,jart,2024-05-21T20:41:52Z,2024-05-22T11:08:18+00:00,1,14.44
7447,phi3 : duplicate rope factors in each layer,slaren,2024-05-21T21:17:36Z,2024-05-22T14:10:46+00:00,3,16.89
7449,Fix phi3 chat template confusion with zephyr,tristandruyen,2024-05-22T01:22:38Z,2024-05-23T14:15:15+00:00,12,36.88
7458,Embedding parameters,YannFollet,2024-05-22T09:55:13Z,2024-06-24T05:30:24+00:00,2,787.59
7459,[SYCL] Prevent q_xxs using mul_mat_q,AidanBeltonS,2024-05-22T10:50:34Z,2024-05-27T16:34:51+00:00,1,125.74
7461,Add missing inference support for GPTNeoXForCausalLM (Pythia and GPT-NeoX base models),fairydreaming,2024-05-22T12:24:01Z,2024-05-23T09:49:53+00:00,1,21.43
7464,llama : add getters for n_threads/n_threads_batch,danbev,2024-05-22T13:21:34Z,2024-05-23T12:29:26+00:00,3,23.13
7465,CUDA: fix FA out-of-bounds writes,JohannesGaessler,2024-05-22T14:12:23Z,2024-05-22T15:58:25+00:00,1,1.77
7472,gguf-py : do not use internal numpy types,compilade,2024-05-22T18:35:50Z,2024-07-09T05:04:49+00:00,1,1138.48
7473,ggml : drop support for QK_K=64,ggerganov,2024-05-22T18:58:00Z,2024-05-23T07:00:21+00:00,1,12.04
7475,Vulkan Rope Frequency Factor Update,0cc4m,2024-05-22T20:17:41Z,2024-05-23T07:00:00+00:00,1,10.71
7477,Allow pooled embeddings on any model,iamlemec,2024-05-22T21:30:07Z,2024-06-21T05:38:22+00:00,23,704.14
7480,"SimpleChat Completion Mode flexibility and cleanup, Settings gMe, Optional sliding window",hanishkvc,2024-05-22T23:31:07Z,2024-05-26T00:56:34+00:00,2,73.42
7481,llama: extend for small granite models,giuseppe,2024-05-22T23:49:21Z,2024-05-28T18:49:49+00:00,19,139.01
7482,labeler.yml: add embedding label detector,mofosyne,2024-05-23T02:21:23Z,2024-05-23T07:40:43+00:00,1,5.32
7483,gguf-py : fix and simplify quantized shape round-trip,compilade,2024-05-23T04:06:50Z,2024-05-25T01:11:48+00:00,2,45.08
7495,[SYCL] Add freq factors support to sycl rope operation,AidanBeltonS,2024-05-23T13:06:29Z,2024-05-27T12:34:09+00:00,2,95.46
7499,convert-*.py: GGUF Naming Convention Refactor and Metadata Override Refactor,mofosyne,2024-05-23T17:51:33Z,2024-07-18T10:40:15+00:00,40,1336.81
7500,Tokenizer WPM fixes for bert-bge and jina-v2-en,jaime-m-p,2024-05-23T18:26:20Z,2024-05-28T19:46:34+00:00,1,121.34
7502,Android module,eltonkola,2024-05-23T22:01:29Z,2024-05-25T08:11:33+00:00,1,34.17
7503,fix missing slash in `fs_get_cache_directory()`,ngxson,2024-05-23T22:28:06Z,2024-05-25T03:30:59+00:00,1,29.05
7514,Add `cvector-generator` example,ngxson,2024-05-24T09:53:42Z,2024-06-15T16:53:40+00:00,61,535.0
7515,docker.yml: disable light-intel and server-intel test,mofosyne,2024-05-24T10:17:50Z,2024-05-24T13:47:56+00:00,1,3.5
7517,rpc: remove backend handle from global map when freed,chraac,2024-05-24T11:05:07Z,2024-05-27T11:48:17+00:00,1,72.72
7519,Add support for DeepseekV2ForCausalLM,fairydreaming,2024-05-24T16:18:40Z,2024-05-28T15:07:05+00:00,23,94.81
7524,server: do not remove whitespace at the start of a completion chunk,mgroeber9110,2024-05-24T20:28:40Z,2024-05-28T04:55:51+00:00,1,80.45
7526,Introduce ggml_threadpool,fmz,2024-05-24T22:30:10Z,2024-06-05T15:58:33+00:00,1,281.47
7527,CUDA: quantized KV support for FA vec,JohannesGaessler,2024-05-24T22:54:05Z,2024-06-01T06:44:15+00:00,1,175.84
7529,labeler: added Apple Metal detector (+Kompute),mofosyne,2024-05-25T01:09:00Z,2024-05-25T09:30:42+00:00,1,8.36
7530,Tokenizer BPE fixes,jaime-m-p,2024-05-25T02:45:33Z,2024-06-18T16:40:53+00:00,21,589.92
7531,llama : support Jamba hybrid Transformer-Mamba models,compilade,2024-05-25T03:38:16Z,2025-07-09T18:59:57+00:00,33,9855.36
7534,main: replace --no-special with --special (and also set control token output to stdout to off by default),mofosyne,2024-05-25T13:13:14Z,2024-05-26T14:10:17+00:00,1,24.95
7539,Fix aya-23 conversion script,Galunid,2024-05-25T22:09:43Z,2024-05-26T14:02:34+00:00,1,15.88
7540,nix: update flake.lock,ggerganov,2024-05-26T00:18:42Z,2024-05-26T15:54:56+00:00,1,15.6
7542,make: add --device-debug to NVCC debug flags,JohannesGaessler,2024-05-26T10:01:56Z,2024-05-27T17:34:40+00:00,1,31.55
7543,github: add self sorted issue ticket forms,mofosyne,2024-05-26T12:15:16Z,2024-05-27T00:54:30+00:00,5,12.65
7546,main: replace --no-special with --special (#7534),Bryan-Roe,2024-05-26T16:58:49Z,2024-05-26T22:00:38+00:00,1,5.03
7547,Feat: Support of converting local models added to `convert-hf-to-gguf-update.py`,EvilFreelancer,2024-05-26T19:29:48Z,2024-09-11T12:29:51+00:00,4,2585.0
7548,"SimpleChat: Simple histogram/repeatMatching driven garbageTrimming, Settings UI, Streaming mode, OpenAi Compat (Model, Authorization Bearer), Save/Restore session, Auto Settings UI",hanishkvc,2024-05-26T22:13:59Z,2024-06-01T16:20:18+00:00,1,138.11
7551,faster avx512 exp implementation,chriselrod,2024-05-27T01:16:57Z,2024-05-30T11:32:56+00:00,1,82.27
7552,vulkan: initialize devices properly for LLAMA_SPLIT_MODE_NONE,Adriankhl,2024-05-27T02:38:37Z,2024-05-28T17:25:08+00:00,1,38.78
7561,github: add refactor to issue template,mofosyne,2024-05-27T10:48:38Z,2024-05-28T10:27:27+00:00,2,23.65
7562,rpc : resource management rework,rgerganov,2024-05-27T11:10:01Z,2024-05-28T15:13:36+00:00,15,28.06
7563,ggml : generalize GGML_OP_CONCAT,ggerganov,2024-05-27T11:15:32Z,2024-05-28T08:04:19+00:00,1,20.81
7565,Allow multiple copy function pointers for CUDA graph kernel updates,agray3,2024-05-27T13:07:24Z,2024-05-27T17:33:43+00:00,1,4.44
7566,[SYCL] Align GEMM dispatch,airMeng,2024-05-27T13:13:31Z,2024-05-28T23:00:24+00:00,12,33.78
7570,distributed evaluation with speculation demo v0,okuvshynov,2024-05-27T16:51:30Z,2024-05-31T12:58:02+00:00,8,92.11
7571,Markdownish code block fix,nathan-sixnines,2024-05-27T17:46:46Z,2024-05-28T04:41:14+00:00,1,10.91
7574,Fix missing option in visual studio.,kunnis,2024-05-27T21:12:15Z,2024-05-27T23:40:12+00:00,1,2.47
7582,vulkan: select only one device for single gpu with multiple drivers,Adriankhl,2024-05-28T02:53:47Z,2024-06-11T19:26:05+00:00,1,352.54
7587,llama : cache llama_token_to_piece,ggerganov,2024-05-28T10:16:50Z,2024-05-30T16:01:41+00:00,14,53.75
7596,feat: add changes to handle jina v2 base code,JoanFM,2024-05-28T18:45:18Z,2024-06-06T07:22:41+00:00,3,204.62
7598,ggml : use atomic_flag for critical section,slaren,2024-05-28T20:13:41Z,2024-05-29T11:36:39+00:00,1,15.38
7599,support MiniCPM-V-2.5,tc-mb,2024-05-28T20:39:01Z,2024-08-09T10:33:54+00:00,63,1741.91
7605,[Readme-SYCL][no ci]Added Arc A750 and Arch linux to readme-sycl.md as verified ,qnixsynapse,2024-05-29T05:32:19Z,2024-05-29T06:53:47+00:00,1,1.36
7606,ggml: Support OpenMP for multi-thread processing,msy-kato,2024-05-29T05:59:58Z,2024-06-03T15:14:15+00:00,2,129.24
7612,github: add contact links to issues and convert question into researc…,mofosyne,2024-05-29T09:05:34Z,2024-05-30T11:55:36+00:00,1,26.83
7616,Extend README with brew installation instruction [no ci],makuche,2024-05-29T11:09:48Z,2024-05-30T14:58:15+00:00,1,27.81
7617,ggml : fix YARN + add tests + add asserts,ggerganov,2024-05-29T11:33:26Z,2024-05-29T17:17:31+00:00,2,5.73
7618,README: explain parallel build [no ci],JohannesGaessler,2024-05-29T12:59:38Z,2024-05-30T07:52:39+00:00,3,18.88
7627,Add tokenizer.ggml.pre to gguf-new-metadata.py,Galunid,2024-05-29T18:37:00Z,2024-05-30T00:10:40+00:00,1,5.56
7628,Vulkan Mixture of Experts (MoE) support,0cc4m,2024-05-29T20:50:23Z,2024-06-03T08:59:14+00:00,1,108.15
7630,[SYCL] fix intel docker,airMeng,2024-05-30T02:55:07Z,2024-05-30T06:19:09+00:00,1,3.4
7633,Server UI Improvement - New Try,mounta11n,2024-05-30T06:38:00Z,2024-06-01T19:31:48+00:00,4,60.9
7634,ggml : unify rope norm/neox,ggerganov,2024-05-30T08:36:59Z,2024-06-05T08:29:20+00:00,3,143.87
7640,llama : offload to RPC in addition to other backends,rgerganov,2024-05-30T13:33:49Z,2024-06-03T17:03:26+00:00,9,99.49
7642,Catch exceptions correctly in server.cpp,0wwafa,2024-05-30T14:10:01Z,2024-06-05T11:09:54+00:00,7,141.0
7644,More checks before assuming FIM tokens,CISC,2024-05-30T14:49:10Z,2024-06-14T10:20:04+00:00,4,355.51
7647,`llama_supports_rpc()` function,martindevans,2024-05-30T15:04:02Z,2024-06-10T13:50:00+00:00,2,262.77
7648,Only use FIM middle token if it exists,CISC,2024-05-30T15:15:41Z,2024-06-18T12:19:45+00:00,1,453.07
7650,docs: add aikit to readme,sozercan,2024-05-30T17:51:30Z,2024-05-30T23:57:16+00:00,1,6.1
7660,Handle NotImplementedError in convert-hf-to-gguf,Galunid,2024-05-31T06:59:09Z,2024-05-31T15:42:33+00:00,1,8.72
7664,MiniCPM Support lm_head,zkh2016,2024-05-31T08:16:04Z,2024-06-03T07:49:30+00:00,3,71.56
7669,docs: repeat-penalty 1.0 = disabled,brandon-lockaby,2024-05-31T12:26:36Z,2024-06-03T17:13:26+00:00,2,76.78
7670,server : update js,ggerganov,2024-05-31T12:49:24Z,2024-05-31T19:23:04+00:00,1,6.56
7676,CUDA: use tensor cores for MMQ,JohannesGaessler,2024-05-31T14:31:15Z,2024-06-10T09:45:13+00:00,1,235.23
7681,"CUDA: fix Pascal FA, deq. KV to FP16 for batch > 8",JohannesGaessler,2024-06-01T10:02:00Z,2024-06-01T13:47:04+00:00,1,3.75
7682,[SYCL] Update rpc-server.cpp to include SYCL backend,nickp27,2024-06-01T10:58:20Z,2024-06-02T09:13:54+00:00,3,22.26
7685,Per token attributes,jaime-m-p,2024-06-01T21:04:29Z,2024-06-04T07:17:17+00:00,5,58.21
7686,nix: update flake.lock,ggerganov,2024-06-02T00:33:42Z,2024-06-02T21:13:12+00:00,1,20.66
7687,convert-hf : match model part name prefix and suffix,compilade,2024-06-02T00:57:51Z,2024-06-09T02:47:25+00:00,1,169.83
7688,refine .gitignore,jeffzhou2000,2024-06-02T01:23:21Z,2024-06-04T11:21:27+00:00,2,57.97
7689,chore: Add ignore rule for generated server themes,teleprint-me,2024-06-02T02:25:55Z,2024-06-02T17:39:08+00:00,1,15.22
7693,"convert-hf : set the model name based on cli arg, if present",sasha0552,2024-06-02T08:42:11Z,2024-06-09T06:39:25+00:00,1,165.95
7696,Improve hipBLAS support in CMake,daniandtheweb,2024-06-02T16:06:24Z,2024-06-04T12:09:15+00:00,4,44.05
7700,docs: Added initial PR template with directions for doc only changes and squash merges [no ci],nicolasperez19,2024-06-02T20:14:01Z,2024-06-09T15:24:29+00:00,10,163.17
7702,add pkg-config spec file for llama.cpp,andy-tai,2024-06-02T23:04:05Z,2024-06-03T08:06:24+00:00,1,9.04
7707,Add Intel Advanced Matrix Extensions (AMX) support to ggml,mingfeima,2024-06-03T02:29:16Z,2025-03-06T05:23:39+00:00,4,6626.91
7710,[SYCL] remove global variables,airMeng,2024-06-03T08:07:10Z,2024-06-15T06:05:11+00:00,4,285.97
7713,Poro-34B-chat tokenizer support,ezosa,2024-06-03T12:10:06Z,2024-06-14T10:16:49+00:00,4,262.11
7716,"CUDA: refactor mmq, dmmv, mmvq",JohannesGaessler,2024-06-03T16:24:42Z,2024-06-05T14:53:00+00:00,10,46.47
7722,llama-bench : allow using a different printer for stderr with -oe,slaren,2024-06-03T23:43:56Z,2024-06-04T12:32:42+00:00,1,12.81
7726,ggml : prevent builds with -ffinite-math-only,ggerganov,2024-06-04T05:29:02Z,2024-06-04T07:01:09+00:00,1,1.54
7728,server : Smart selection of available slot using Longest Common Prefix,sasha0552,2024-06-04T06:18:05Z,2024-06-08T07:50:31+00:00,2,97.54
7733,Fix encoding in python scripts,Galunid,2024-06-04T09:44:33Z,2024-06-05T17:07:24+00:00,1,31.38
7738,Allow number of nodes in CUDA graph to change,agray3,2024-06-04T12:32:27Z,2024-06-04T20:06:50+00:00,2,7.57
7745,server: update cache_prompt documentation [no ci],JohannesGaessler,2024-06-04T15:17:10Z,2024-06-07T09:15:49+00:00,1,65.98
7749,Fix per token atrributes bits,jaime-m-p,2024-06-04T17:08:53Z,2024-06-04T23:26:14+00:00,1,6.29
7751,Fix no gcc pragma on Windows,jojorne,2024-06-04T19:13:19Z,2024-06-18T12:18:32+00:00,1,329.09
7754,Enable stream updating in the SwiftUI example,shu223,2024-06-05T00:22:57Z,2024-06-21T05:30:58+00:00,1,389.13
7759,Removing -ins from README.md,arch-btw,2024-06-05T04:55:29Z,2024-06-05T06:40:49+00:00,1,1.76
7771,imatrix : migrate to gpt_params,ggerganov,2024-06-05T13:36:33Z,2024-06-06T13:30:58+00:00,1,23.91
7777,use the correct SYCL context for host USM allocations,bashbaug,2024-06-05T19:17:02Z,2024-06-10T09:21:31+00:00,2,110.07
7780,add openmp lib to dockerfiles,slaren,2024-06-05T22:08:41Z,2024-06-06T05:17:21+00:00,1,7.14
7782,build only main and server in their docker images,slaren,2024-06-05T22:15:45Z,2024-06-06T05:19:49+00:00,1,7.07
7784,build: Fix BUILD_SHARED_LIBS=ON build,intelmatt,2024-06-05T23:32:37Z,2024-06-07T12:15:07+00:00,1,36.71
7790,JSON Schema to GBNF integration tests,HanClinto,2024-06-06T05:36:20Z,2024-06-22T03:18:36+00:00,1,381.7
7794,Fix a typo + add Fedora packages for Vulkan,metal3d,2024-06-06T08:08:00Z,2024-06-12T01:18:16+00:00,1,137.17
7797,"`json`: support integer minimum, maximum, exclusiveMinimum, exclusiveMaximum",ochafik,2024-06-06T09:30:10Z,2024-06-25T19:06:20+00:00,9,465.6
7801,server : fix --threads-http arg,ggerganov,2024-06-06T13:37:45Z,2024-06-06T16:19:59+00:00,1,2.7
7803,examples/gguf-split: Change binary multi-byte units to decimal,christianazinn,2024-06-06T14:49:49Z,2024-06-07T12:56:01+00:00,1,22.1
7806,vulkan : reuse parent extra for views,slaren,2024-06-06T18:14:22Z,2024-06-07T17:47:49+00:00,1,23.56
7807,check for nans in imatrix and quantize,slaren,2024-06-06T20:47:54Z,2024-06-07T06:01:29+00:00,1,9.23
7808,"Revert ""[SYCL] Update rpc-server.cpp to include SYCL backend""",slaren,2024-06-06T21:22:34Z,2024-06-08T23:43:39+00:00,1,50.35
7809,"`build`: rename main → llama-cli, server → llama-server, llava-cli → llama-llava-cli, etc...",ochafik,2024-06-06T23:31:00Z,2024-06-12T23:41:53+00:00,15,144.18
7811,[SYCL] fix softmax r2r result wrong issue,pengxin99,2024-06-07T04:27:28Z,2024-06-07T06:28:27+00:00,1,2.02
7818,Update Vulkan RoPE implementation,0cc4m,2024-06-07T17:51:57Z,2024-06-11T19:20:29+00:00,3,97.48
7821,cmake : fix CMake requirement for CUDA,cebtenzzre,2024-06-07T19:31:23Z,2024-06-10T22:32:10+00:00,1,75.01
7824,CUDA: revise q8_1 data layout for mul_mat_q,JohannesGaessler,2024-06-07T21:53:06Z,2024-06-09T07:42:25+00:00,1,33.82
7825,Avoid division-by-zero on 0-weights,CISC,2024-06-07T22:04:34Z,2025-08-17T22:38:27+00:00,2,10464.56
7826,url: save -mu downloads to new cache location,ochafik,2024-06-08T00:04:42Z,2024-06-08T19:21:08+00:00,2,19.27
7827,gguf-py : decouple adding metadata from writing in GGUFWriter,compilade,2024-06-08T02:11:05Z,2024-06-09T02:34:29+00:00,2,24.39
7830,server: do not remove whitespace at the start of a completion chunk (reapply to new UI),mgroeber9110,2024-06-08T08:53:36Z,2024-06-09T10:50:35+00:00,1,25.95
7835,update: support Qwen2-57B-A14B,legraphista,2024-06-08T11:19:38Z,2024-06-17T19:08:46+00:00,38,223.82
7838,nix: update flake.lock,ggerganov,2024-06-09T00:20:06Z,2024-06-09T23:04:50+00:00,1,22.75
7840,"`json`: fix additionalProperties, allow space after enum/const",ochafik,2024-06-09T21:56:23Z,2024-06-26T00:45:58+00:00,3,386.83
7841,"`json`: document schema conversion in GBNF readme, align manual grammar examples & converters",ochafik,2024-06-10T00:44:11Z,2024-06-11T00:00:30+00:00,5,23.27
7843,Fix conversion of unnormalized BF16->BF16 weights,CISC,2024-06-10T02:44:41Z,2024-08-02T19:11:39+00:00,4,1288.45
7844,examples: refine tensor_sum_elements(tensor dump) in examples/benchmark/benchmark-matmult.cpp,jeffzhou2000,2024-06-10T03:47:03Z,2024-07-19T08:24:53+00:00,10,940.63
7845,AVX IQ Quants,netrunnereve,2024-06-10T04:28:08Z,2024-06-21T05:57:36+00:00,3,265.49
7849,ci : disable server-windows workflow,ggerganov,2024-06-10T09:32:39Z,2024-06-10T12:59:40+00:00,1,3.45
7851,Type cast AVX512_BF16 data types based on compiler instead of the OS platform,Srihari-mcw,2024-06-10T09:44:29Z,2024-06-17T18:23:17+00:00,1,176.65
7852,try win-2019 on server windows test,slaren,2024-06-10T10:05:37Z,2024-06-10T11:36:27+00:00,1,1.51
7853,gguf-dump.py: add --markdown dump output,mofosyne,2024-06-10T10:06:07Z,2024-06-17T05:25:20+00:00,13,163.32
7856,ggml : improve ggml_is_contiguous logic,ggerganov,2024-06-10T14:10:07Z,2024-06-12T12:24:20+00:00,1,46.24
7857,tests : add non-cont unary tests,ggerganov,2024-06-10T14:13:52Z,2024-06-12T13:00:23+00:00,1,46.78
7858,"[SYCL] Revert ""use the correct SYCL context for host USM allocations""",AidanBeltonS,2024-06-10T16:31:38Z,2024-06-17T09:15:20+00:00,2,160.73
7860,"CUDA: int8 tensor cores for MMQ (q4_K, q5_K, q6_K)",JohannesGaessler,2024-06-10T19:02:45Z,2024-06-11T06:26:07+00:00,3,11.39
7861,fix CUDA CI by using a windows-2019 image,slaren,2024-06-10T19:07:35Z,2024-06-11T05:59:20+00:00,1,10.86
7863,"`json`: better support for ""type"" unions (e.g. nullable arrays w/ typed items)",ochafik,2024-06-10T22:00:10Z,2024-06-26T00:46:35+00:00,1,362.77
7868,github: move PR template to .github/ root,mofosyne,2024-06-11T01:57:28Z,2024-06-11T14:43:41+00:00,1,12.77
7888,convert-hf-to-gguf-update.py: Added Ukrainian tokens into string,AragonerUA,2024-06-11T20:40:17Z,2024-07-11T10:14:51+00:00,2,709.58
7894,"Revert ""[SYCL] fix intel docker""",airMeng,2024-06-12T07:34:51Z,2024-06-12T09:05:35+00:00,1,1.51
7896,Implement non-mapped async IO for CUDA on Windows. ,mtavenrath,2024-06-12T08:55:24Z,2024-06-17T14:10:15+00:00,6,125.25
7899,[no ci] Add Nix and Flox install instructions,bryanhonof,2024-06-12T11:59:53Z,2024-06-17T15:37:55+00:00,1,123.63
7909,sycl: always set the main device after initialization,bashbaug,2024-06-12T21:52:44Z,2024-06-17T09:14:35+00:00,7,107.36
7910,Save partial imatrix data,CISC,2024-06-12T23:20:04Z,2025-07-19T21:05:03+00:00,1,9645.75
7914,Remove outdated instructions from README.md,Galunid,2024-06-13T06:05:51Z,2024-06-13T07:42:42+00:00,1,1.61
7919,sycl-exp : unify rope neox/norm,joeatodd,2024-06-13T13:24:26Z,2024-06-14T14:08:23+00:00,6,24.73
7920,"Revert ""use the correct SYCL context for host USM allocations""",joeatodd,2024-06-13T13:32:06Z,2024-06-14T14:10:33+00:00,1,24.64
7921,"CUDA: faster q2_K, q3_K MMQ + int8 tensor cores",JohannesGaessler,2024-06-13T14:58:48Z,2024-06-14T16:41:49+00:00,1,25.72
7931,Add support for BitnetForCausalLM (new model / new datatype),Eddie-Wang1120,2024-06-14T03:32:42Z,2024-06-23T18:27:57+00:00,58,230.92
7932,fix: divide 0 exception in mamba,thxCode,2024-06-14T03:40:59Z,2024-06-17T14:11:08+00:00,1,82.5
7935,metal : utilize max shared memory for mul_mat_id,ggerganov,2024-06-14T10:06:34Z,2024-06-14T14:14:09+00:00,1,4.13
7936,llama-bench : fix RPC indication,rgerganov,2024-06-14T10:12:19Z,2024-06-14T13:47:42+00:00,1,3.59
7940,Fix macos x86 build,olexiyb,2024-06-14T15:58:39Z,2024-06-14T17:28:34+00:00,1,1.5
7943,[no ci] Add LARS to the UI list in README,abgulati,2024-06-14T21:07:40Z,2024-06-18T06:57:41+00:00,1,81.83
7946,"[SYCL] Update README-sycl.md for Chapter ""Recommended release"" and ""News""",arthw,2024-06-15T02:25:52Z,2024-06-17T03:17:07+00:00,6,48.85
7947,"Vulkan Shader Refactor, Memory Debugging Option",0cc4m,2024-06-15T09:40:54Z,2024-06-16T05:17:31+00:00,1,19.61
7948,rpc : fix load/store misaligned addresses,ggerganov,2024-06-15T11:42:24Z,2024-06-17T08:09:20+00:00,1,44.45
7950,"SimpleChat v3.1: Boolean chat request options in Settings UI, cache_prompt",hanishkvc,2024-06-15T19:02:22Z,2024-06-25T11:27:35+00:00,2,232.42
7951,nix: update flake.lock,ggerganov,2024-06-16T00:19:26Z,2024-06-16T16:16:21+00:00,1,15.95
7953,Add support for sqrt on CUDA,calvin-laurenson,2024-06-16T02:56:42Z,2024-06-16T22:23:04+00:00,3,19.44
7955,ggml : fix handling of zero blocks in IQ quants,ggerganov,2024-06-16T07:43:34Z,2024-06-16T11:50:12+00:00,2,4.11
7959,Add --pre-tokenizer option to convert,Galunid,2024-06-16T18:21:26Z,2025-09-23T09:34:57+00:00,1,11127.23
7961,Refactor Vulkan backend to allow multiple contexts,0cc4m,2024-06-16T18:44:31Z,2024-06-23T08:21:25+00:00,1,157.62
7972,  sycl-exp : dequant q4 k improvements,AidanBeltonS,2024-06-17T09:58:09Z,2024-06-18T15:20:39+00:00,1,29.38
7975,server : fix JSON-Scheme typo,akx,2024-06-17T12:20:39Z,2024-06-23T15:03:08+00:00,1,146.71
7980,"sycl-exp : Revert ""Minor arithmetic improvement to mmvq wrapper kernel (#7172)""",joeatodd,2024-06-17T16:03:46Z,2024-06-18T12:09:52+00:00,3,20.1
7981,sycl-exp : Temporarily revert RPC offload (#7640),joeatodd,2024-06-17T16:12:17Z,2024-06-18T08:54:47+00:00,1,16.71
7985,chore: clean useless beam search param,thxCode,2024-06-18T01:13:35Z,2024-06-18T07:11:41+00:00,2,5.97
7993,ggml : synchronize threads using barriers,slaren,2024-06-18T17:09:56Z,2024-06-19T13:04:15+00:00,1,19.91
7996,un-ignore `build-info.cmake` and `build-info.sh`,mdegans,2024-06-18T21:54:31Z,2024-06-19T20:10:42+00:00,1,22.27
8003,[SYCL] Fix windows build and inference,luoyu-intel,2024-06-19T07:44:54Z,2024-06-20T13:19:06+00:00,5,29.57
8006,llama : reorganize source code + improve CMake ,ggerganov,2024-06-19T11:26:49Z,2024-06-26T15:33:03+00:00,7,172.1
8014,"[SYCL] fix to support multiple GPUs, set main device",arthw,2024-06-19T15:15:09Z,2024-07-02T08:08:53+00:00,21,304.9
8015,Update convert-hf-to-gguf.py,0xspringtime,2024-06-19T18:13:17Z,2024-06-22T13:37:42+00:00,2,67.41
8016,Add SPM infill support,CISC,2024-06-19T20:05:33Z,2024-06-28T10:53:43+00:00,1,206.8
8017,ggml : remove ggml_task_type and GGML_PERF,slaren,2024-06-19T20:16:43Z,2024-06-24T01:07:59+00:00,1,100.85
8018,CUDA: stream-k decomposition for MMQ,JohannesGaessler,2024-06-19T20:26:57Z,2024-06-20T12:39:21+00:00,1,16.21
8021,Fix `ggml_metal_supports_op`,mdegans,2024-06-19T23:32:46Z,2024-06-20T05:32:01+00:00,1,5.99
8022,vulkan: detect multiple devices by deviceUUID instead of deviceID,Adriankhl,2024-06-20T00:48:21Z,2024-06-21T08:28:21+00:00,4,31.67
8031,Support glm3 and glm4.,youth123,2024-06-20T09:06:39Z,2024-07-07T12:52:10+00:00,35,411.76
8035,ggml-cuda: Adding support for unified memory,matteoserva,2024-06-20T11:56:17Z,2024-08-01T21:28:28+00:00,4,1017.54
8039,Detokenizer fixes,jaime-m-p,2024-06-20T16:58:18Z,2024-07-05T17:01:35+00:00,1,360.05
8040,Fix the encoding in the convert-hf-to-gguf-update.py file,hamdoudhakem,2024-06-20T17:05:50Z,2024-06-20T20:00:00+00:00,1,2.9
8041,Fixed packages versions,hamdoudhakem,2024-06-20T17:16:52Z,2024-06-20T20:01:16+00:00,1,2.74
8043,enable curl in nix build,edude03,2024-06-20T19:31:08Z,2024-07-01T11:47:05+00:00,3,256.27
8048,gguf-hash: model wide and per tensor hashing using xxhash and sha1,mofosyne,2024-06-21T04:02:10Z,2024-07-07T12:58:43+00:00,5,392.94
8052,cvector-generator: Moe Moe Fixie-Fixie for Lots of Formats~! ♡(ᐢ ᴥ ᐢ)♡,HatsuneMikuUwU33,2024-06-21T09:25:03Z,2024-06-22T15:19:37+00:00,1,29.91
8054,Gguf dump start data offset via --data-offset and some extra refactor,mofosyne,2024-06-21T11:26:11Z,2024-06-25T12:03:25+00:00,3,96.62
8055,Model conversion support for T5 and FLAN-T5 model variants,fairydreaming,2024-06-21T13:58:00Z,2024-06-24T05:06:05+00:00,8,63.13
8056,[WIP] Hot swap for LoRA,ltoniazzi,2024-06-21T16:51:40Z,2024-07-21T15:22:35+00:00,9,718.52
8057,sycl-exp : Re-enabled mul_mat_batched_sycl Path for batched Q*K & KQ*V,OuadiElfarouki,2024-06-21T16:57:03Z,2024-06-24T08:57:12+00:00,1,64.0
8058,Update llama-quantize ppl/file size output from LLaMA-v1 to Llama-3 values,ddh0,2024-06-21T19:59:59Z,2024-06-22T13:16:10+00:00,1,17.27
8060,fixes #7999 (adds control vectors to all `build_XXX()` functions in `llama.cpp` [needs testing],jukofyork,2024-06-22T09:41:55Z,2024-06-25T20:47:40+00:00,1,83.1
8062,CUDA: optimize MMQ int8 tensor core performance,JohannesGaessler,2024-06-22T12:49:26Z,2024-06-24T10:41:23+00:00,4,45.87
8066,fix CI failures,slaren,2024-06-22T15:40:53Z,2024-06-23T11:14:45+00:00,1,19.56
8068,Add chat template support for llama-cli,ngxson,2024-06-22T18:39:30Z,2024-06-25T11:56:50+00:00,9,65.29
8069,"cvector: better prompt handling, add ""mean vector"" method",ngxson,2024-06-22T22:23:56Z,2024-06-25T11:59:55+00:00,1,61.6
8071,nix: update flake.lock,ggerganov,2024-06-23T00:19:13Z,2024-06-27T15:37:29+00:00,1,111.3
8075,CUDA: use MMQ instead of cuBLAS by default,JohannesGaessler,2024-06-23T10:47:48Z,2024-06-24T15:43:43+00:00,3,28.93
8081,Add healthchecks to llama-server containers,codearranger,2024-06-23T16:49:09Z,2024-06-25T15:13:27+00:00,1,46.41
8083,disable publishing the full-rocm docker image,slaren,2024-06-23T18:34:14Z,2024-06-24T05:36:11+00:00,1,11.03
8085,gfx1010 optimizations,daniandtheweb,2024-06-24T00:13:16Z,2024-07-03T23:02:58+00:00,3,238.83
8087,"Streamline embeddings from ""non-embedding"" models",iamlemec,2024-06-24T06:00:04Z,2024-07-05T07:05:57+00:00,6,265.1
8089,Add Unigram tokenizer needed by T5 and FLAN-T5 model families,fairydreaming,2024-06-24T07:23:50Z,2024-06-25T19:14:35+00:00,4,35.85
8090,Fix tensor groups for encoder-decoder models in gguf-dump.py,fairydreaming,2024-06-24T08:38:10Z,2024-06-24T12:13:39+00:00,1,3.59
8093,llama : return nullptr from llama_grammar_init,danbev,2024-06-24T11:25:16Z,2024-06-25T19:07:28+00:00,2,31.7
8095,[SYCL] Re-enabled mul_mat_batched_sycl,airMeng,2024-06-24T12:57:53Z,2024-06-25T02:19:20+00:00,1,13.36
8100,CUDA: fix MMQ writeback for int8 tensor cores,JohannesGaessler,2024-06-24T18:58:52Z,2024-06-24T20:15:33+00:00,1,1.28
8102,CUDA: fix matrix multiplication algorithm choice,JohannesGaessler,2024-06-24T20:39:38Z,2024-06-24T23:22:33+00:00,1,2.72
8103,Extend llm_build_ffn() to support _scale tensors ,Eddie-Wang1120,2024-06-25T01:42:33Z,2024-06-26T06:27:46+00:00,2,28.75
8104,Update control vector help,HatsuneMikuUwU33,2024-06-25T05:08:56Z,2024-06-25T08:44:48+00:00,2,3.6
8105,clip : suppress unused variable warnings,danbev,2024-06-25T05:35:47Z,2024-06-26T23:50:10+00:00,4,42.24
8106,[SYCL] Fix the sub group size of Intel,luoyu-intel,2024-06-25T09:09:54Z,2024-07-02T02:16:01+00:00,16,161.1
8110,disable docker CI on pull requests,slaren,2024-06-25T13:25:48Z,2024-06-25T17:20:06+00:00,2,3.9
8115,Clarify default MMQ for CUDA and LLAMA_CUDA_FORCE_MMQ flag,isaac-mcfadyen,2024-06-25T16:58:55Z,2024-06-26T06:29:28+00:00,3,13.51
8118,Add `JAIS` model(s),fmz,2024-06-25T18:39:55Z,2024-07-02T14:36:00+00:00,11,163.93
8119,vulkan : cmake integration,bandoti,2024-06-25T19:37:07Z,2024-07-13T16:12:40+00:00,22,428.59
8122,move public backend headers to the public include directory,slaren,2024-06-25T20:38:50Z,2024-06-26T06:50:42+00:00,1,10.2
8123,CUDA: fix misaligned shared memory read,JohannesGaessler,2024-06-25T22:28:01Z,2024-06-26T06:28:03+00:00,1,8.0
8132,`json`: update grammars/README w/ examples & note about additionalProperties,ochafik,2024-06-26T08:32:20Z,2024-06-27T21:08:42+00:00,1,36.61
8135,Added support for Viking pre-tokenizer,kustaaya,2024-06-26T12:29:07Z,2024-06-27T08:58:54+00:00,2,20.5
8137,Control vector loading fixes,jukofyork,2024-06-26T13:48:49Z,2024-06-27T14:48:07+00:00,3,24.99
8140,"ggml : add GGML_CUDA_USE_GRAPHS option, restore GGML_CUDA_FORCE_CUBLAS",slaren,2024-06-26T17:16:05Z,2024-06-26T19:34:14+00:00,2,2.3
8141,Inference support for T5 and FLAN-T5 model families,fairydreaming,2024-06-26T17:25:31Z,2024-07-04T13:46:11+00:00,11,188.34
8142,ci : publish new docker images only when the files change,slaren,2024-06-26T17:28:02Z,2024-06-26T19:59:28+00:00,1,2.52
8144,Fix CodeLlama FIM token checks,CISC,2024-06-26T21:27:22Z,2024-06-27T07:46:42+00:00,3,10.32
8145,"Fixing llama-android.cpp for error - ""common/common.h not found""",criminact,2024-06-26T21:28:35Z,2024-06-27T01:57:58+00:00,1,4.49
8150,llama : suppress unref var in Windows MSVC,danbev,2024-06-27T06:11:51Z,2024-07-04T10:50:57+00:00,2,172.65
8151,ggml-quants : ternary packing for TriLMs and BitNet b1.58,compilade,2024-06-27T06:13:41Z,2024-09-06T01:48:47+00:00,5,1699.59
8156,Add support for Gemma2ForCausalLM,pculliton,2024-06-27T07:53:11Z,2024-06-28T04:00:43+00:00,6,20.13
8157,[SYCL] Update SYCL-Rope op and Refactor,zhentaoyu,2024-06-27T08:11:32Z,2024-07-01T11:39:06+00:00,2,99.46
8158,Add Qwen2MoE 57B-A14B model identifier,CISC,2024-06-27T08:31:46Z,2024-06-27T14:27:41+00:00,1,5.93
8159,Add alpaca chat template (repush of #7383),jukofyork,2024-06-27T09:02:42Z,2024-10-27T12:10:29+00:00,5,2931.13
8160,Add chatml fallback for cpp `llama_chat_apply_template`,ngxson,2024-06-27T11:02:42Z,2024-06-27T16:14:19+00:00,3,5.19
8165,Delete examples/llama.android/llama/CMakeLists.txt,criminact,2024-06-27T12:21:01Z,2024-06-27T14:39:29+00:00,1,2.31
8167,CUDA: fix MMQ stream-k for --split-mode row,JohannesGaessler,2024-06-27T13:22:56Z,2024-06-27T14:26:05+00:00,1,1.05
8170,CI: fix release build (Ubuntu+Mac),loonerin,2024-06-27T15:28:13Z,2024-06-27T19:01:23+00:00,1,3.55
8171,cmake : fix deprecated option names not working,slaren,2024-06-27T15:46:55Z,2024-06-27T18:04:39+00:00,1,2.3
8172,"Add MiniCPM, Deepseek V2 chat template + clean up `llama_chat_apply_template_internal`",ngxson,2024-06-27T16:49:55Z,2024-06-28T13:11:45+00:00,9,20.36
8177,Add missing items in makefile,ngxson,2024-06-27T22:19:33Z,2024-06-28T00:19:12+00:00,1,1.99
8178,cmake : allow user to override default options,slaren,2024-06-27T23:58:11Z,2024-06-28T10:37:45+00:00,2,10.66
8180,"`json`: restore default additionalProperties to false, fix some pattern escapes",ochafik,2024-06-28T01:18:30Z,2024-06-28T08:26:45+00:00,1,7.14
8181,convert-hf : print output file name when completed,danbev,2024-06-28T07:08:25Z,2024-07-02T06:40:49+00:00,7,95.54
8182,sycl-exp : Enabled more data types for oneMKL's gemm_batch API,OuadiElfarouki,2024-06-28T07:09:50Z,2024-06-28T11:12:35+00:00,1,4.05
8189,json: skip slow tests when running under emulator,ochafik,2024-06-28T14:32:50Z,2024-06-28T17:02:05+00:00,1,2.49
8197,"Add attention and final logit soft-capping, update scaling factor to Gemma2",abetlen,2024-06-28T19:45:44Z,2024-06-30T03:44:08+00:00,15,31.97
8200,"Added checks for cmake,make and ctest in ci->run.sh",AlexsCode,2024-06-28T22:40:15Z,2024-07-07T14:59:14+00:00,1,208.32
8203,"Fix new line issue with chat template, disable template when in-prefix/suffix is set",ngxson,2024-06-29T09:05:28Z,2024-06-30T18:27:13+00:00,1,33.36
8205,Document BERT support.,iacore,2024-06-29T11:00:45Z,2024-07-01T11:40:58+00:00,1,48.67
8209,[no ci] Added gppm to Tool list in README,crashr,2024-06-29T14:25:04Z,2024-07-01T12:48:16+00:00,2,46.39
8215,CUDA: refactor and optimize IQ MMVQ,JohannesGaessler,2024-06-29T20:56:42Z,2024-07-01T18:39:06+00:00,1,45.71
8218,nix: update flake.lock,ggerganov,2024-06-30T00:19:52Z,2024-06-30T23:09:34+00:00,1,22.83
8227,gemma2: add sliding window mask,ngxson,2024-06-30T17:33:35Z,2024-07-01T16:48:35+00:00,24,23.25
8228,llama : fix pre-tokenization of non-special added tokens,compilade,2024-06-30T18:59:16Z,2024-07-14T03:35:10+00:00,17,320.6
8230,[SYCL] Fix win build conflict of math library,luoyu-intel,2024-07-01T07:12:12Z,2024-07-02T04:50:07+00:00,2,21.63
8231,tests : add _CRT_SECURE_NO_WARNINGS for WIN32,danbev,2024-07-01T08:49:51Z,2024-07-04T10:53:42+00:00,1,74.06
8233,Stylistic adjustments for python scripts,jpodivin,2024-07-01T10:07:52Z,2024-07-22T13:44:54+00:00,1,507.62
8236,[SYCL] Enabled more data types for oneMKL's gemm_batch API,OuadiElfarouki,2024-07-01T12:53:07Z,2024-07-05T12:23:25+00:00,4,95.5
8237,Removing fsep token from GPTRefactForCausalLM,jpodivin,2024-07-01T12:58:12Z,2024-07-12T08:06:33+00:00,2,259.14
8239,readme: add Paddler to the list of projects,mcharytoniuk,2024-07-01T15:54:51Z,2024-07-01T17:13:23+00:00,1,1.31
8244,Fix gemma2 tokenizer convert,ngxson,2024-07-01T21:11:42Z,2024-07-01T23:07:23+00:00,3,1.93
8245,cuda : update supports_op for matrix multiplication,slaren,2024-07-01T21:56:41Z,2024-07-02T06:39:38+00:00,1,8.72
8255,Dequant improvements rebase,AidanBeltonS,2024-07-02T11:45:58Z,2024-07-03T01:55:34+00:00,2,14.16
8256,[SYCL] Use multi_ptr to clean up deprecated warnings,AidanBeltonS,2024-07-02T13:52:09Z,2024-07-10T15:10:49+00:00,3,193.31
8257,Target `clean` now removes legacy binary names,HanClinto,2024-07-02T15:46:27Z,2024-07-02T17:19:56+00:00,1,1.56
8262,Fix phi 3 conversion,ngxson,2024-07-02T21:18:41Z,2024-07-03T14:01:54+00:00,1,16.72
8266,[SYCL] Fix WARP_SIZE=16 bug of Intel GPU,luoyu-intel,2024-07-03T03:06:32Z,2024-07-05T05:06:13+00:00,7,49.99
8267,fix typo in comments of `ggml_mul_mat_id`,foldl,2024-07-03T05:58:58Z,2024-07-03T12:40:16+00:00,1,6.69
8268,llama.swiftui: Fix a small bug ,ho2103,2024-07-03T06:12:24Z,2024-07-20T13:09:37+00:00,7,414.95
8277,ppl : fix n_seq_max for perplexity,slaren,2024-07-03T14:13:18Z,2024-07-03T17:33:31+00:00,2,3.34
8278,"CUDA: MMQ support for iq4_nl, iq4_xs",JohannesGaessler,2024-07-03T14:26:31Z,2024-07-05T07:06:31+00:00,1,40.67
8279,Add option to ignore tokens with 2+ English characters,hopto-dot,2024-07-03T15:23:08Z,2024-07-10T19:05:34+00:00,1,171.71
8280,[SYCL] Remove unneeded semicolons in DPCT headers,AidanBeltonS,2024-07-03T16:06:16Z,2024-07-04T01:07:19+00:00,1,9.02
8283,Deprecation warning to assist with migration to new binary names,HanClinto,2024-07-03T18:33:03Z,2024-07-09T15:54:43+00:00,1,141.36
8286,[SYCL] replace get_work_group_size() by local cache for performance,NeoZhangJianyu,2024-07-04T01:51:31Z,2024-07-05T02:32:29+00:00,1,24.68
8295,SYCL : Improved implementation of dp4a for the Nvidia backend,Alcpz,2024-07-04T12:22:03Z,2024-07-15T12:36:37+00:00,11,264.24
8296,cli: add EOT when user hit Ctrl+C,ngxson,2024-07-04T12:30:49Z,2024-07-04T18:55:04+00:00,1,6.4
8299,tokenize : add --show-count (token) option,danbev,2024-07-04T14:42:52Z,2024-07-04T16:38:58+00:00,1,1.94
8305,py : switch to snake_case,ggerganov,2024-07-04T17:45:28Z,2024-07-05T04:53:33+00:00,5,11.13
8307,added support for Authorization Bearer tokens when downloading model,dwoolworth,2024-07-04T18:50:54Z,2024-07-06T20:32:04+00:00,8,49.69
8309,CUDA: revert part of the RDNA1 optimizations,daniandtheweb,2024-07-04T19:40:59Z,2024-07-05T07:06:09+00:00,1,11.42
8311,CUDA: fix MMQ stream-k rounding if ne00 % 128 != 0,JohannesGaessler,2024-07-04T22:34:44Z,2024-07-05T07:05:34+00:00,1,8.51
8315,Update llama-cli documentation,dspasyuk,2024-07-05T04:04:55Z,2024-07-07T15:08:28+00:00,5,59.06
8321,Fix converter for internlm2,RunningLeon,2024-07-05T08:19:52Z,2024-07-10T11:26:40+00:00,5,123.11
8325,Reorganize documentation pages,ngxson,2024-07-05T10:06:43Z,2024-07-05T16:08:32+00:00,1,6.03
8327,llama : add early return for empty range,danbev,2024-07-05T12:21:09Z,2024-07-06T07:22:16+00:00,4,19.02
8332,Refactor lora adapter support,ngxson,2024-07-06T12:41:25Z,2024-07-15T18:50:47+00:00,55,222.16
8333,[no ci] add Build & Supported backends section to README,ngxson,2024-07-06T13:37:32Z,2024-07-06T17:01:23+00:00,1,3.4
8335,py : use cpu-only torch in requirements.txt,compilade,2024-07-06T15:27:46Z,2024-07-07T11:23:38+00:00,1,19.93
8337,server: Retrieve prompt template in /props,bviksoe,2024-07-06T15:30:39Z,2024-07-07T09:10:38+00:00,4,17.67
8341,py : type-check all Python scripts with Pyright,compilade,2024-07-06T21:57:13Z,2024-07-07T19:04:40+00:00,1,21.12
8342,nix: update flake.lock,ggerganov,2024-07-07T00:19:47Z,2024-07-08T22:36:38+00:00,2,46.28
8347,fix web link error [no ci],b4b4o,2024-07-07T04:58:12Z,2024-07-08T14:19:24+00:00,1,33.35
8357,Adding models to the list in convert-hf-to-gguf-update.py,thisisnotthat,2024-07-08T00:00:31Z,2025-09-03T01:55:22+00:00,1,10129.91
8358,Avoid unnecessary logits fetch,kevmo314,2024-07-08T00:44:00Z,2024-07-08T06:31:55+00:00,1,5.8
8359,server : avoid breaking KV cache when prompt >= n_ctx (#6855),prfd,2024-07-08T02:56:56Z,2025-04-04T21:29:52+00:00,2,6498.55
8366,ggml: avoid rebuild of GGML graph for each token (#7456),agray3,2024-07-08T10:06:13Z,2025-07-22T16:25:12+00:00,10,9102.32
8368,sycl : fix powf call in device code,Alcpz,2024-07-08T10:20:57Z,2024-07-08T13:22:41+00:00,2,3.03
8370,cmake : allow external ggml,iboB,2024-07-08T12:49:08Z,2024-07-09T08:38:00+00:00,1,19.81
8372,sycl : Reenabled mmvq path for the SYCL Nvidia Backend,Alcpz,2024-07-08T14:48:16Z,2024-07-09T14:03:15+00:00,5,23.25
8373,labeler : updated sycl backend to match new structure,Alcpz,2024-07-08T15:12:52Z,2024-07-08T20:35:17+00:00,1,5.37
8382,ggml : reading the runtime sve config of the cpu,jdomke,2024-07-09T01:18:27Z,2024-07-26T10:00:51+00:00,1,416.71
8383,feat: Support Moore Threads GPU ,yeahdongcn,2024-07-09T01:24:38Z,2024-07-27T23:41:25+00:00,37,454.28
8392,make/cmake: LLAMA_NO_CCACHE -> GGML_NO_CCACHE,JohannesGaessler,2024-07-09T11:36:10Z,2024-07-09T15:11:07+00:00,1,3.58
8399,Update README.md to fix broken link to docs,andysalerno,2024-07-09T17:35:09Z,2024-07-09T18:58:45+00:00,1,1.39
8400,Add assertion informing the API user about missing llama_encode() call,fairydreaming,2024-07-09T20:17:42Z,2024-07-10T11:38:58+00:00,1,15.35
8404,Name Migration: Build the deprecation-warning 'main' binary every time,HanClinto,2024-07-10T03:46:43Z,2024-07-10T16:35:18+00:00,1,12.81
8408,llama : C++20 compatibility for u8 strings,iboB,2024-07-10T06:56:04Z,2024-07-10T11:45:44+00:00,5,4.83
8412,llama : use F32 precision in Qwen2 attention and no FA,ggerganov,2024-07-10T14:34:11Z,2024-07-11T07:21:30+00:00,1,16.79
8414,cuda : suppress 'noreturn' warn in no_device_code,danbev,2024-07-10T18:35:25Z,2024-07-11T15:53:42+00:00,1,21.3
8416,CUDA: optimize and refactor MMQ,JohannesGaessler,2024-07-10T19:18:14Z,2024-07-11T14:47:47+00:00,1,19.49
8418,Server: Update /props endpoint to correctly return default server parameters,HanClinto,2024-07-10T19:37:07Z,2024-07-11T00:08:17+00:00,1,4.52
8420,server: Ensure batches are either all embed or all completion (#8076),iamlemec,2024-07-10T20:02:25Z,2024-07-12T08:14:12+00:00,5,36.2
8423,tokenize : add --no-parse-special option,compilade,2024-07-10T22:20:47Z,2024-07-11T07:41:49+00:00,1,9.35
8425,ggml : add NVPL BLAS support (#8329),nicholaiTukanov,2024-07-10T23:00:49Z,2024-07-11T16:49:15+00:00,2,17.81
8427,[SYCL] fix the mul_mat_id ut issues,ClarkChin08,2024-07-11T03:09:50Z,2024-07-12T00:52:04+00:00,2,21.7
8433,ggml : minor naming changes,ggerganov,2024-07-11T09:09:35Z,2024-07-12T07:46:02+00:00,2,22.61
8434,examples : sprintf -> snprintf,ggerganov,2024-07-11T10:02:16Z,2024-07-12T07:46:14+00:00,6,21.73
8441,Docker: Fix filename for convert-hf-to-gguf.py in tools.sh,kriation,2024-07-11T19:42:49Z,2024-07-12T08:08:19+00:00,2,12.43
8449,server : handle content array in chat API,ggerganov,2024-07-12T09:04:30Z,2024-07-12T11:48:15+00:00,4,2.73
8460,ggml : suppress unknown pragma 'GCC' on windows,danbev,2024-07-12T13:14:47Z,2024-07-15T12:48:17+00:00,1,71.56
8470,gguf_hash.py: Add sha256,mofosyne,2024-07-13T14:25:42Z,2024-07-14T06:47:14+00:00,2,16.36
8472,server: update README.md with llama-server --help's output [no ci],maruel,2024-07-13T15:42:50Z,2024-07-15T12:04:56+00:00,1,44.37
8474,pydantic : replace uses of __annotations__ with get_type_hints,compilade,2024-07-13T21:10:30Z,2024-07-14T23:51:21+00:00,2,26.68
8475,nix: update flake.lock,ggerganov,2024-07-14T00:20:29Z,2024-07-14T15:54:02+00:00,1,15.56
8477,"chore : Fix vulkan related compiler warnings, add help text, improve CLI options",teleprint-me,2024-07-14T03:37:04Z,2024-07-28T07:52:42+00:00,5,340.26
8479,Vulkan MMQ Fix,0cc4m,2024-07-14T11:01:52Z,2024-07-15T07:38:52+00:00,2,20.62
8480,ggml: Install all public headers in the ggml build regardless of build settings,65a,2024-07-14T19:55:16Z,2024-07-18T14:47:12+00:00,2,90.87
8481,docs: fix links in development docs [no ci],NikolaiLyssogor,2024-07-14T22:48:59Z,2024-07-15T11:46:40+00:00,1,12.96
8482,convert_hf : faster lazy safetensors,compilade,2024-07-14T23:45:32Z,2024-07-16T03:13:10+00:00,1,27.46
8483,[SYCL] add concat through dim 1/2,airMeng,2024-07-15T02:41:05Z,2024-07-15T11:32:15+00:00,2,8.85
8489,llama : change fallback type IQ4_NL -> Q4_0,ggerganov,2024-07-15T07:28:17Z,2024-07-26T19:23:23+00:00,1,275.92
8491,Update clib.json to point to Cyan4973 original xxhash repo,mofosyne,2024-07-15T11:49:35Z,2024-07-16T07:14:17+00:00,1,19.41
8493,examples : Rewrite pydantic_models_to_grammar_examples.py,maruel,2024-07-15T14:19:43Z,2024-07-21T02:09:17+00:00,3,131.83
8494,fix ci,ngxson,2024-07-15T15:20:44Z,2024-07-15T17:23:10+00:00,1,2.04
8495,CUDA: MMQ code deduplication + iquant support,JohannesGaessler,2024-07-15T17:16:14Z,2024-07-20T20:25:26+00:00,1,123.15
8497,handle export-lora help argument,sbonds,2024-07-16T00:21:02Z,2024-07-16T07:04:45+00:00,1,6.73
8505,docs: added AI Studio to the list of UIs [no ci],SommerEngineering,2024-07-16T09:08:38Z,2024-07-24T12:52:30+00:00,1,195.73
8506,Fix func call tokens for internlm2,RunningLeon,2024-07-16T09:12:19Z,2024-07-18T08:07:07+00:00,6,46.91
8508,"llama : move vocab, grammar and sampling into separate files ",ggerganov,2024-07-16T11:41:16Z,2024-07-23T10:10:17+00:00,8,166.48
8515,make/cmake: add missing force MMQ/cuBLAS for HIP,JohannesGaessler,2024-07-16T14:04:07Z,2024-07-16T19:20:59+00:00,1,5.28
8526,llama : simplify Mamba with advanced batch splits,compilade,2024-07-17T01:51:37Z,2024-08-21T21:58:11+00:00,3,860.11
8527,batched: fix n_predict parameter,msy-kato,2024-07-17T03:47:33Z,2024-07-17T07:34:28+00:00,1,3.78
8530,llama : bump max layers from 256 to 512,ggerganov,2024-07-17T07:02:33Z,2024-07-19T13:50:47+00:00,1,54.8
8531,Improvements for running on Windows with Snapdragon X,AndreasKunar,2024-07-17T07:10:24Z,2024-07-25T16:01:00+00:00,2,200.84
8537,build : Fix docker build warnings (#8535),amochkin,2024-07-17T09:17:08Z,2024-07-17T18:21:55+00:00,1,9.08
8541,Update CONTRIBUTING.md to remove mention of noci,mofosyne,2024-07-17T13:43:49Z,2024-07-17T14:57:06+00:00,1,1.22
8542,CPU/CUDA: Gemma 2 FlashAttention support,JohannesGaessler,2024-07-17T14:27:22Z,2024-08-24T19:35:00+00:00,11,917.13
8543,Add support for Chameleon,nopperl,2024-07-17T14:46:15Z,2024-09-28T12:08:44+00:00,3,1749.37
8548,"lookup: fibonacci hashing, fix crashes",JohannesGaessler,2024-07-17T20:16:41Z,2024-07-17T21:35:44+00:00,1,1.32
8549,ggml : fix iq4_nl dot product with odd number of blocks,slaren,2024-07-17T21:14:00Z,2024-07-19T15:17:27+00:00,2,42.06
8550,[SYCL] Improve the next-token speed of Q4_0,luoyu-intel,2024-07-18T01:39:14Z,2024-07-18T08:00:57+00:00,6,6.36
8552,server: use relative routes for static files in new UI,EZForever,2024-07-18T05:53:24Z,2024-07-18T10:43:50+00:00,2,4.84
8553,fix special not work for llama-server,RunningLeon,2024-07-18T06:34:53Z,2024-07-18T08:06:22+00:00,1,1.52
8554,[SYCL] fix multi-gpu issue on sycl,ClarkChin08,2024-07-18T07:02:07Z,2024-07-25T11:45:18+00:00,9,172.72
8556,ggml : fix odd blocks for ARM_NEON,ggerganov,2024-07-18T07:55:36Z,2024-07-19T14:13:18+00:00,4,30.3
8565,convert-*.py: autogenerate general.uuid if missing,mofosyne,2024-07-18T11:09:39Z,2024-11-09T10:18:16+00:00,4,2735.14
8571,convert-*.py: add general.name kv override,mofosyne,2024-07-18T14:14:42Z,2024-07-19T07:51:51+00:00,1,17.62
8572,CUDA: fix partial offloading for ne0 % 256 != 0,JohannesGaessler,2024-07-18T15:41:07Z,2024-07-18T21:48:47+00:00,3,6.13
8573,cmake: fix paths for vulkan shaders compilation on Windows,stduhpf,2024-07-18T15:42:17Z,2024-08-05T06:18:27+00:00,5,422.6
8575,Add friendlier error message to fopen errors,HanClinto,2024-07-18T17:29:53Z,2024-07-19T11:05:45+00:00,1,17.6
8579,llama : Added support for Tekken pre-tokenizer (#8577),m18coppola,2024-07-18T22:08:01Z,2024-07-20T13:43:51+00:00,5,39.6
8587,gguf : handle null name during init,ggerganov,2024-07-19T10:46:25Z,2024-07-20T14:15:43+00:00,1,27.49
8588,gguf_dump.py: fix markddown kv array print,mofosyne,2024-07-19T13:27:54Z,2024-07-20T07:35:25+00:00,10,18.13
8591,gguf-py : fix some metadata name extraction edge cases,compilade,2024-07-19T16:47:10Z,2024-07-21T01:58:49+00:00,4,33.19
8597,convert_hf : fix Gemma v1 conversion,compilade,2024-07-19T21:10:29Z,2024-07-21T01:53:01+00:00,2,28.71
8599,llama : fix codeshell support (#8250),hankeke303,2024-07-20T00:52:37Z,2024-07-22T16:43:43+00:00,2,63.85
8604,Mistral Nemo inference support (#8577),iamlemec,2024-07-20T19:17:36Z,2024-07-22T08:06:17+00:00,2,36.81
8607,examples : Fix `llama-export-lora` example,ngxson,2024-07-20T22:48:08Z,2024-07-23T21:48:37+00:00,10,71.01
8609,llama : Added support for SmolLm pre-tokenizer (#8608),Stillerman,2024-07-20T23:01:32Z,2024-07-22T14:43:02+00:00,5,39.69
8610,nix: update flake.lock,ggerganov,2024-07-21T00:20:11Z,2024-07-21T13:45:10+00:00,1,13.42
8613,Vulkan IQ4_NL Support,0cc4m,2024-07-21T09:02:38Z,2024-07-23T08:56:49+00:00,4,47.9
8619,server : update doc to clarify n_keep count when there is bos token,kaetemi,2024-07-21T20:33:42Z,2024-07-22T08:02:09+00:00,1,11.47
8622,llama : model-based max number of graph nodes,ggerganov,2024-07-22T06:52:22Z,2024-07-27T11:59:29+00:00,1,125.12
8624,"CMake: CMakePresets fix, host for msvc compiler can only be x86 or x64",Xarbirus,2024-07-22T07:33:40Z,2024-09-05T22:14:13+00:00,2,1094.68
8627,llama: use sliding window for phi3,FanShupei,2024-07-22T09:19:46Z,2024-07-25T07:21:09+00:00,7,70.02
8629,Allow all RDNA2 archs to use sdot4 intrinsic,jeroen-mostert,2024-07-22T10:13:34Z,2024-07-23T08:50:40+00:00,1,22.62
8630,contrib : clarify PR squashing + module names,ggerganov,2024-07-22T11:38:04Z,2024-07-23T08:28:38+00:00,4,20.84
8636,add alias for lora adaptors,zhipenghan,2024-07-22T23:31:28Z,2024-07-24T10:19:19+00:00,8,34.8
8642,[SYCL] Fix the accuracy bug of long context length,luoyu-intel,2024-07-23T05:46:12Z,2024-07-23T07:43:28+00:00,1,1.95
8643,llama : refactor sampling,ggerganov,2024-07-23T07:49:57Z,2024-09-07T09:34:57+00:00,5,1105.75
8644,sycl : Add support for non-release DPC++ & oneMKL,joeatodd,2024-07-23T12:03:38Z,2024-07-23T13:58:37+00:00,2,1.92
8645,convert: add tensor hash general.hash.sha256 to kv store,mofosyne,2024-07-23T12:13:38Z,2024-07-26T15:26:19+00:00,1,75.21
8646,Fix Safari Compatibility Issue: Replace URL.parse Method in llama-server UI,0x4139,2024-07-23T12:21:05Z,2024-07-23T14:37:43+00:00,1,2.28
8651,llama : update Swift and Android bindings for refactor sampling,HanClinto,2024-07-23T17:03:18Z,2024-07-24T10:40:30+00:00,1,17.62
8653,llama : add `llama_lora_adapter_clear`,ngxson,2024-07-23T17:45:06Z,2024-07-24T09:25:19+00:00,2,15.67
8657,llama : fix `llama_chat_format_single` for mistral,ngxson,2024-07-23T19:45:27Z,2024-07-24T11:48:47+00:00,3,16.06
8658,Add support for XLMRoberta embedding models,iamlemec,2024-07-23T20:40:57Z,2024-08-06T07:20:54+00:00,6,322.67
8664,ggml : add and use ggml_cpu_has_llamafile(),ggerganov,2024-07-24T08:33:12Z,2024-07-25T09:37:42+00:00,1,25.07
8666,Documentation fix: Quantum -> Quantized.,Ujjawal-K-Panchal,2024-07-24T09:01:38Z,2024-07-25T08:13:28+00:00,1,23.2
8667,sycl : Re-add erroneously removed -fsycl from GGML_EXTRA_LIBS,joeatodd,2024-07-24T09:16:22Z,2024-07-24T10:55:26+00:00,2,1.65
8669,examples : remove `finetune` and `train-text-from-scratch`,ngxson,2024-07-24T14:15:43Z,2024-07-25T08:39:04+00:00,2,18.39
8672,Threadpool: take 2,fmz,2024-07-24T15:13:16Z,2024-08-29T23:20:54+00:00,38,872.13
8676,Add llama 3.1 rope scaling factors to llama conversion and inference,jmorganca,2024-07-24T19:52:17Z,2024-07-27T12:03:45+00:00,12,64.19
8679,server : add Speech Recognition & Synthesis to UI,ElYaiko,2024-07-25T01:49:40Z,2024-07-25T22:10:16+00:00,2,20.34
8683,llama : fix build + fix fabs compile warnings,ggerganov,2024-07-25T07:26:57Z,2024-07-25T16:57:31+00:00,1,9.51
8687,export-lora : fix issue with quantized base models,ngxson,2024-07-25T12:07:46Z,2024-07-25T21:49:39+00:00,1,9.7
8688,[SYCL] add conv support,airMeng,2024-07-25T12:16:41Z,2024-07-29T02:50:27+00:00,2,86.56
8693,docker : Install curl in runtime layer in llama-server.Dockerfile,bsquizz,2024-07-25T20:45:05Z,2024-08-04T18:17:16+00:00,1,237.54
8698,ggml : reduce hash table reset cost,slaren,2024-07-26T01:03:08Z,2024-07-27T02:41:55+00:00,1,25.65
8699,llama : refactor session file management,compilade,2024-07-26T01:55:37Z,2024-07-28T04:42:05+00:00,1,50.77
8707,[SYCL] Add `TIMESTEP_EMBEDDING` OP,zhentaoyu,2024-07-26T08:40:14Z,2024-07-30T06:56:51+00:00,4,94.28
8709,ggml : reading the runtime sve config of the cpu,jdomke,2024-07-26T09:57:08Z,2024-08-03T16:34:41+00:00,7,198.63
8710,[CANN] Fix Multi-NPU execution error,wangshuai09,2024-07-26T10:04:34Z,2024-07-27T08:36:44+00:00,3,22.54
8712,common : add --no-warmup option for main/llama-cli,danbev,2024-07-26T13:16:41Z,2024-07-27T10:45:02+00:00,1,21.47
8713,Implementations for Q4_0_8_8 quantization based functions in AVX2 SIMD architecture,Srihari-mcw,2024-07-26T15:29:59Z,2024-09-04T16:51:22+00:00,2,961.36
8715,Add option to keep output and embed tensors at f16,0wwafa,2024-07-26T20:44:59Z,2024-10-13T10:33:26+00:00,6,1885.81
8716,abandoned,wilsonccccc,2024-07-27T01:10:45Z,2024-07-27T02:11:32+00:00,1,1.01
8729,nix: update flake.lock,ggerganov,2024-07-28T00:20:38Z,2024-07-30T12:58:57+00:00,1,60.64
8740,cmake: use 1 more thread for non-ggml in CI,JohannesGaessler,2024-07-28T18:11:58Z,2024-07-28T20:32:44+00:00,1,2.35
8746,refactor: Organize vendor-specific headers into vendors directory,yeahdongcn,2024-07-29T06:10:05Z,2024-07-29T12:56:12+00:00,1,6.77
8748,ggml: bugfix: fix the inactive elements is agnostic for risc-v vector,CarterLi999,2024-07-29T06:51:02Z,2024-07-29T16:38:34+00:00,1,9.79
8751,added android implementation of ggml_print_backtrace_symbols,l3utterfly,2024-07-29T12:04:18Z,2024-07-30T14:40:18+00:00,2,26.6
8754,Server: Don't ignore llama.cpp params,ardfork,2024-07-29T15:16:38Z,2024-08-04T18:16:23+00:00,3,147.0
8765,[CANN] Update Cmake,wangshuai09,2024-07-30T07:36:56Z,2024-07-30T10:37:35+00:00,1,3.01
8771,Add support for cpu_get_num_physical_cores() on Windows,Septa2112,2024-07-30T11:39:49Z,2024-08-16T06:23:13+00:00,3,402.72
8772,"nix: rely on propagatedBuild{Inputs,Outputs} for cuda",SomeoneSerge,2024-07-30T13:19:33Z,2024-07-30T20:35:30+00:00,1,7.27
8778,src: remove duplicate function llama_should_add_bos_token,kylo5aby,2024-07-30T16:28:03Z,2024-08-15T07:23:23+00:00,1,374.92
8779,server: update llama-server embedding flag documentation (#8763),okigan,2024-07-30T16:28:47Z,2024-07-31T23:59:10+00:00,2,31.51
8781,Build: Fix potential race condition,HanClinto,2024-07-30T17:21:04Z,2024-07-31T19:51:06+00:00,9,26.5
8783,Build: Only include execinfo.h on linux systems that support it,acon96,2024-07-31T00:53:19Z,2024-08-01T16:53:46+00:00,4,40.01
8784,Adding Gemma 2 2B configs,pculliton,2024-07-31T01:20:44Z,2024-07-31T15:12:10+00:00,5,13.86
8787,cmake : fix use of external ggml,iboB,2024-07-31T07:43:30Z,2024-07-31T13:40:08+00:00,1,5.94
8800,cuda : fix dmmv cols requirement to 2*GGML_CUDA_DMMV_X,slaren,2024-07-31T20:28:33Z,2024-08-01T13:26:22+00:00,2,16.96
8804,Added mpi-cli example for running load across nodes,arjn2,2024-08-01T00:46:04Z,2024-08-01T12:12:14+00:00,1,11.44
8805,[CANN] Support Q8_0,wangshuai09,2024-08-01T01:16:10Z,2024-08-01T02:39:05+00:00,1,1.38
8810,py: Add more authorship metadata from model card,mofosyne,2024-08-01T12:05:36Z,2024-08-05T11:15:28+00:00,7,95.16
8812,[SYCL] Fixing IQ4_NL failing tests,OuadiElfarouki,2024-08-01T18:15:40Z,2024-08-02T00:55:17+00:00,1,6.66
8818,ggml: Add epsilon as a parameter for group_norm,MollySophia,2024-08-02T06:47:44Z,2024-08-06T07:26:46+00:00,6,96.65
8819,cann: Fix ggml_cann_im2col for 1D im2col,MengqingCao,2024-08-02T07:12:28Z,2024-08-02T08:50:53+00:00,2,1.64
8822,[CANN] Support Q4_0 for Ascend NPU,wangshuai09,2024-08-02T08:27:29Z,2024-08-05T04:22:31+00:00,3,67.92
8823,common : Changed tuple to struct (TODO fix),Septa2112,2024-08-02T08:28:06Z,2024-08-05T16:14:10+00:00,4,79.77
8824,Add support for getting cpu info on Windows for llama_bench,kylo5aby,2024-08-02T09:19:32Z,2024-08-07T01:01:06+00:00,2,111.69
8835,cmake : Link vulkan-shaders-gen with pthreads,Patater,2024-08-02T18:03:36Z,2024-08-06T13:21:47+00:00,1,91.3
8838,gguf-py : simplify support for quant types,compilade,2024-08-02T21:55:42Z,2024-08-08T17:33:09+00:00,4,139.62
8839,"[example] batched-bench ""segmentation fault""",cunnie,2024-08-03T01:07:57Z,2024-08-04T10:55:03+00:00,2,33.78
8847,nix: update flake.lock,ggerganov,2024-08-04T00:20:07Z,2024-08-04T02:53:20+00:00,1,2.55
8850,Add pre-tokenizer regexes for BLOOM and gpt3-finnish,Exploder98,2024-08-04T09:02:50Z,2024-08-15T07:17:13+00:00,1,262.24
8851,readme: Update model list,BarfingLemurs,2024-08-04T10:29:05Z,2024-08-05T05:54:10+00:00,1,19.42
8852,llama : better replace_all,ggerganov,2024-08-04T10:45:45Z,2024-08-05T05:53:39+00:00,1,19.13
8855,Fix Vulkan Quantized Matrix Vector Multiplication on AMD GPUs when ncols < 64,0cc4m,2024-08-04T15:57:30Z,2024-08-05T05:52:55+00:00,1,13.92
8857,server : add lora hotswap endpoint,ngxson,2024-08-04T17:52:19Z,2024-08-06T15:33:39+00:00,2,45.69
8858,Stop the generation when <|eom_id|> token is encountered (needed for llama 3.1 tool call support),fairydreaming,2024-08-04T19:18:01Z,2024-08-05T07:38:01+00:00,1,12.33
8865,[CANN] Fix buffer_num and runtime speed slowly error,wangshuai09,2024-08-05T09:38:53Z,2024-08-05T13:10:37+00:00,4,3.53
8866,Fix overflows in elu function,jart,2024-08-05T09:39:15Z,2024-08-05T12:43:40+00:00,1,3.07
8867,[CANN] Add doc and docker image,wangshuai09,2024-08-05T09:42:39Z,2024-08-19T08:46:38+00:00,3,335.07
8871,[CANN]: Fix ggml_backend_cann_buffer_get_tensor,MengqingCao,2024-08-05T12:42:33Z,2024-08-06T04:42:42+00:00,1,16.0
8873,docs: introduce gpustack and gguf-parser,thxCode,2024-08-05T13:20:18Z,2024-08-12T12:45:50+00:00,3,167.43
8875,metadata: Detailed Dataset Authorship Metadata,mofosyne,2024-08-05T15:26:39Z,2024-11-13T10:10:38+00:00,8,2394.73
8877,[SYCL] correct cmd name in scirpt,arthw,2024-08-05T16:18:14Z,2024-08-06T01:09:12+00:00,1,8.85
8880,[Vulkan] Fix compilation of `vulkan-shaders-gen` on w64devkit after `e31a4f6`,MaggotHATE,2024-08-06T04:46:54Z,2024-08-06T11:32:03+00:00,1,6.75
8881,llama : rename batch_all to batch,danbev,2024-08-06T07:33:10Z,2024-10-17T23:41:51+00:00,4,1744.14
8884,CUDA: fix padding logic for FP16/FP32,JohannesGaessler,2024-08-06T13:37:22Z,2024-08-06T15:13:55+00:00,2,1.61
8889,quantize : update usage comment in quantize.cpp [no ci],danbev,2024-08-06T16:35:06Z,2024-08-06T23:43:01+00:00,1,7.13
8890,Add ring buffer to store prev tokens in sampling,kylo5aby,2024-08-06T17:05:15Z,2024-08-13T08:13:00+00:00,8,159.13
8891,typo correction,Nexesenex,2024-08-06T17:13:10Z,2024-08-06T23:41:54+00:00,1,6.48
8896,CUDA/HIP: fix tests/test-backend-ops,JohannesGaessler,2024-08-06T21:57:24Z,2024-08-07T07:07:52+00:00,2,9.17
8897,ggml-backend : fix async copy from CPU,slaren,2024-08-06T22:04:36Z,2024-08-07T11:29:03+00:00,8,13.41
8899,make : use C compiler to build metal embed object,slaren,2024-08-07T01:35:05Z,2024-08-07T16:24:05+00:00,4,14.82
8901,[SYCL] Updated SYCL device filtering ,OuadiElfarouki,2024-08-07T05:39:28Z,2024-08-07T10:25:36+00:00,1,4.77
8908,Introduction of gemm4xN and gemmMx4 for Q4_0 and Q8_0 for better performance results,Srihari-mcw,2024-08-07T13:40:15Z,2024-08-31T08:20:35+00:00,2,570.67
8909,[SYCL] update guide,arthw,2024-08-07T14:19:31Z,2024-08-11T08:37:43+00:00,1,90.3
8916,llama : reduce useless copies when saving session,compilade,2024-08-07T20:11:05Z,2024-08-09T03:54:00+00:00,2,31.72
8922,Add Nemotron/Minitron GGUF Conversion & Inference Support,suhara,2024-08-08T07:17:31Z,2024-08-16T02:23:33+00:00,15,187.1
8926,llama : better replace_all (cont),ggerganov,2024-08-08T09:37:15Z,2024-08-09T15:23:52+00:00,2,29.78
8928,fix: Fixes wrong input type for raw_dtype in ggml to gguf scripts,farbodbj,2024-08-08T10:06:13Z,2024-08-16T10:36:30+00:00,1,192.5
8931,llama : default n_swa for phi-3,ngxson,2024-08-08T12:20:47Z,2024-08-10T11:04:40+00:00,3,46.73
8934,embedding : add --pooling option to README.md [no ci],danbev,2024-08-08T13:55:39Z,2024-08-09T06:33:30+00:00,1,16.63
8936,Add one level list nesting for embeddings,gelim,2024-08-08T15:28:20Z,2024-08-09T06:32:02+00:00,1,15.06
8939,gguf-py : Numpy dequantization for most types,compilade,2024-08-09T03:42:49Z,2024-08-11T18:45:41+00:00,1,63.05
8943,Optimize Vulkan backend for better CPU performance and less GPU synchronization overhead.,mtavenrath,2024-08-09T08:37:39Z,2024-08-11T08:09:09+00:00,2,47.52
8949,ggml : move rope type enum to ggml.h,danbev,2024-08-09T13:37:56Z,2024-08-13T19:13:16+00:00,8,101.59
8951,Add support for lora adapters in T5 model,fairydreaming,2024-08-09T14:14:55Z,2024-08-09T16:53:09+00:00,1,2.64
8952,gguf-py: fix gguf-py/examples/writer.py,tarilabs,2024-08-09T14:36:42Z,2024-08-10T05:58:50+00:00,1,15.37
8955,Retrieval: Fix Memory Leak in Retrieval Query Handling,gtygo,2024-08-09T17:19:48Z,2024-08-15T07:40:12+00:00,2,134.34
8957,tests : add integration test for lora adapters,ltoniazzi,2024-08-09T19:43:41Z,2024-08-18T09:58:04+00:00,2,206.24
8958,Fix memory leak in src/llama.cpp,mjtalkiewicz,2024-08-09T19:45:54Z,2024-08-22T20:01:03+00:00,1,312.25
8959,Vulkan Optimizations and Fixes,0cc4m,2024-08-09T20:30:47Z,2024-08-14T16:32:53+00:00,4,116.03
8960,Add support for encoder-only T5 models,fairydreaming,2024-08-09T20:59:27Z,2024-08-10T09:43:26+00:00,1,12.73
8967,support MiniCPM-V-2.6,tc-mb,2024-08-10T12:02:14Z,2024-08-16T13:34:42+00:00,11,145.54
8970,llama : model-based max number of graph nodes calculation,nicoboss,2024-08-10T13:32:15Z,2024-08-12T15:13:59+00:00,2,49.7
8979,nix: update flake.lock,ggerganov,2024-08-11T00:20:35Z,2024-08-11T13:58:58+00:00,1,13.64
8980,llama : support RWKV v6 models,MollySophia,2024-08-11T02:09:47Z,2024-09-01T14:38:17+00:00,36,516.48
8981,Check all graph nodes when searching for result_embd_pooled (needed for gemma-2),fairydreaming,2024-08-11T06:06:12Z,2024-08-11T08:35:26+00:00,1,2.49
8982,py : fix requirements check '==' -> '~=',ggerganov,2024-08-11T08:16:00Z,2024-08-12T08:02:01+00:00,4,23.77
8984,llava: Add ACC OP for GPU acceleration to the Vulkan backend in the LLAVA CLIP model.,cyzero-kim,2024-08-11T10:39:01Z,2024-08-20T19:00:01+00:00,6,224.35
8987,server : fix segfault on long system prompt,compilade,2024-08-11T18:37:45Z,2024-08-14T06:51:02+00:00,4,60.22
8994,fix: duplication n_predict key in the generation_settings,snowyu,2024-08-12T03:08:08Z,2024-08-15T07:28:06+00:00,1,76.33
8998,Add Intel Advanced Matrix Extensions (AMX) support to ggml,mingfeima,2024-08-12T06:56:34Z,2024-10-18T05:34:36+00:00,21,1606.63
9001,Fix a spelling mistake in llama-sampling.cpp,Septa2112,2024-08-12T08:06:22Z,2024-08-12T09:46:03+00:00,1,1.66
9002,export-lora : throw error if lora is quantized,ngxson,2024-08-12T09:09:56Z,2024-08-13T09:41:14+00:00,1,24.52
9003,ggml: fix div-by-zero,DavidKorczynski,2024-08-12T09:11:03Z,2024-08-12T12:21:41+00:00,1,3.18
9004,grammar-parser: fix possible null-deref,DavidKorczynski,2024-08-12T10:45:37Z,2024-08-12T12:36:41+00:00,1,1.85
9006,ci : enable RPC in all of the released builds,rgerganov,2024-08-12T13:21:41Z,2024-08-12T16:17:04+00:00,2,2.92
9011,cmake : remove unused option GGML_CURL,ggerganov,2024-08-12T16:40:02Z,2024-08-14T06:14:49+00:00,1,37.58
9017,Simplify and improve CUDA graphs through use of indirect copy pointers,agray3,2024-08-13T08:56:22Z,2025-04-03T01:31:16+00:00,4,5584.58
9025,add EXAONE model support,mscheong01,2024-08-14T10:10:15Z,2024-08-16T06:35:18+00:00,5,44.42
9026,Setting stop and error fields of the result struct,jpodivin,2024-08-14T11:57:01Z,2024-08-15T06:21:57+00:00,1,18.42
9035,Doc: Fix inference example lacks required parameters,Aisuko,2024-08-15T06:29:18Z,2024-08-16T09:08:59+00:00,3,26.66
9040,rpc : prevent crashes on invalid input,rgerganov,2024-08-15T11:12:13Z,2024-08-19T07:10:21+00:00,3,91.97
9042,rpc : print error message when failed to connect endpoint,rgerganov,2024-08-15T11:52:49Z,2024-08-19T07:11:46+00:00,1,91.32
9046,llama : suppress conversion from 'size_t' to 'int',danbev,2024-08-15T18:03:49Z,2024-10-16T17:34:28+00:00,3,1487.51
9047,ggml : dynamic ggml_sched_max_splits based on graph_size,nicoboss,2024-08-15T19:13:18Z,2024-08-16T02:22:55+00:00,3,7.16
9051,gguf-py : bump version from 0.9.1 to 0.10.0,compilade,2024-08-15T22:33:39Z,2024-08-16T06:36:11+00:00,1,8.04
9052,[SYCL] Fix SYCL `im2col` and `convert` Overflow with Large Dims,zhentaoyu,2024-08-16T02:46:31Z,2024-08-20T15:06:51+00:00,12,108.34
9054,fix the script error in MobileVLM README,fengerhu1,2024-08-16T06:09:44Z,2024-09-12T11:34:23+00:00,1,653.41
9056,server : refactor middleware and /health endpoint,ngxson,2024-08-16T08:35:35Z,2024-08-16T15:19:05+00:00,1,6.72
9057,llama : fix llama_split_mode enum values in main_gpu document,kou,2024-08-16T10:28:30Z,2024-08-30T18:08:10+00:00,2,343.66
9058,Add lora test workflow (WIP) ,ltoniazzi,2024-08-16T13:31:17Z,2024-12-04T08:57:26+00:00,6,2635.44
9062,llama : std::move llm_bigram_bpe from work_queue,danbev,2024-08-17T05:16:41Z,2024-08-21T07:32:58+00:00,5,98.27
9063,Fix incorrect use of `ctx_split` for bias tensors,suhara,2024-08-17T06:33:26Z,2024-08-17T13:34:21+00:00,1,7.02
9068,nix: update flake.lock,ggerganov,2024-08-18T00:20:27Z,2024-08-18T14:43:32+00:00,1,14.38
9070,ci: fix library is missing during dynamic compiling,Aisuko,2024-08-18T07:29:55Z,2024-08-19T12:04:59+00:00,1,28.58
9074,Feat: Support for `falcon-mamba` architecture,younesbelkada,2024-08-18T11:36:30Z,2024-08-21T08:06:37+00:00,18,68.5
9082,llava : zero-initialize clip_ctx structure fields with aggregate initialization,fairydreaming,2024-08-18T18:35:21Z,2024-08-21T07:45:49+00:00,1,61.17
9088,[SYCL] fallback mmvq,airMeng,2024-08-19T05:44:25Z,2024-08-20T15:50:17+00:00,4,34.1
9090,cann: merge get row operations of float type,kylo5aby,2024-08-19T07:27:27Z,2025-04-09T06:05:13+00:00,2,5590.63
9091,[SYCL] Add oneDNN primitive support,luoyu-intel,2024-08-19T08:59:35Z,2024-08-22T04:50:10+00:00,6,67.84
9103,lora : raise error if lm_head is ignored,ngxson,2024-08-20T13:07:45Z,2024-09-12T11:33:57+00:00,5,550.44
9105,server : support reading arguments from environment variables,ngxson,2024-08-20T14:38:49Z,2024-08-21T09:04:34+00:00,1,18.43
9108,server : Add option to return token pieces in /tokenize endpoint,mathijshenquet,2024-08-20T21:38:15Z,2024-09-12T20:30:11+00:00,4,550.87
9111,llava: fix minicpm example directory,xyb,2024-08-21T06:06:07Z,2024-08-27T12:33:08+00:00,1,150.45
9116,server : add some missing env variables,ngxson,2024-08-21T09:57:05Z,2024-08-27T09:07:02+00:00,1,143.17
9117,lora : fix llama conversion script with model having ROPE_FREQS,ngxson,2024-08-21T12:55:47Z,2024-08-23T10:58:53+00:00,2,46.05
9118,Overlap cmdbuffer creation and cmdbuffer execution in Vulkan backend by submitting smaller cmdbuffers early.,mtavenrath,2024-08-21T13:46:54Z,2024-09-08T19:43:49+00:00,13,437.95
9126,llama : initial Mamba-2 support,compilade,2024-08-21T21:51:00Z,2025-07-02T17:10:25+00:00,4,7555.32
9130,llama:use F32 precision in GLM4 attention and no FA,piDack,2024-08-22T08:56:55Z,2024-08-23T07:27:17+00:00,1,22.51
9132,llama : fix typo in xcda_array_view comment [no ci],danbev,2024-08-22T12:35:04Z,2024-08-31T07:50:22+00:00,1,211.25
9133,[SYCL] Add a space between the string and the variable to supress a cmake warning,qnixsynapse,2024-08-22T12:46:41Z,2024-08-22T14:09:47+00:00,1,1.39
9137,Fix nix CUDA build: replace deprecated autoAddOpenGLRunpathHook,enolan,2024-08-22T21:25:18Z,2024-08-31T08:44:21+00:00,1,203.32
9141,fix: llama3.1 rope_freqs not respecting custom head_dim,nyxkrage,2024-08-23T05:13:39Z,2024-08-27T06:53:40+00:00,3,97.67
9149,Correct typo run_llama2.sh > run-llama2.sh,MakeDecisionWorth,2024-08-23T13:35:29Z,2024-08-30T12:10:02+00:00,3,166.58
9156,llama : fix qs.n_attention_wv for DeepSeek-V2,compilade,2024-08-24T14:32:38Z,2024-08-27T10:09:23+00:00,1,67.61
9159,metal : gemma2 flash attention support,slaren,2024-08-24T20:29:22Z,2024-08-26T09:08:59+00:00,1,36.66
9160,ggml-ci : try to improve build time,slaren,2024-08-24T21:56:51Z,2024-08-26T09:03:30+00:00,1,35.11
9161,common : Update stb_image.h to latest version,arch-btw,2024-08-24T22:52:43Z,2024-08-27T05:58:50+00:00,1,55.1
9162,nix: update flake.lock,ggerganov,2024-08-25T00:20:33Z,2024-08-29T04:28:14+00:00,1,100.13
9163,Fix time complexity of string replacement,jart,2024-08-25T03:29:55Z,2024-08-26T06:09:53+00:00,1,26.67
9165,Support video understanding,tc-mb,2024-08-25T07:04:54Z,2025-01-23T10:26:55+00:00,6,3627.37
9166,CUDA: fix Gemma 2 numerical issues for FA,JohannesGaessler,2024-08-25T07:26:45Z,2024-08-25T20:11:49+00:00,1,12.75
9183,server : update deps,ggerganov,2024-08-26T06:17:45Z,2024-08-26T09:16:58+00:00,1,2.99
9184,"Update build.yml, enable rpc for windows cuda builds",awatuna,2024-08-26T07:43:55Z,2024-09-05T22:34:36+00:00,1,254.84
9186,ggml:Mamba Cuda kernel performance improve,piDack,2024-08-26T09:42:43Z,2024-12-02T23:50:36+00:00,10,2366.13
9192,ggml : do not crash when quantizing q4_x_x with an imatrix,slaren,2024-08-26T14:58:59Z,2024-08-26T17:44:43+00:00,1,2.76
9194,"Fix ChatGLM4 wrong shape, releated to THUDM/glm-4-9b-chat-1m and CausalLM/miniG",RealJosephus,2024-08-26T19:56:58Z,2024-08-27T06:58:22+00:00,2,11.02
9210,server : fix crash when error handler dumps invalid utf-8 json,kaetemi,2024-08-28T00:28:13Z,2024-08-29T23:15:27+00:00,3,46.79
9213,docker : update CUDA images,slaren,2024-08-28T02:53:56Z,2024-08-28T11:20:36+00:00,1,8.44
9217,ggml : remove assert for AArch64 GEMV and GEMM Q4 kernels,chaxu01,2024-08-28T08:42:43Z,2024-09-25T13:12:20+00:00,10,676.49
9218,devops : only build some specific targets for full image,ngxson,2024-08-28T09:44:34Z,2024-08-30T14:55:12+00:00,1,53.18
9237,"the function ""clip"" should be int",tc-mb,2024-08-29T11:40:32Z,2024-08-30T05:21:58+00:00,1,17.69
9239,Improve Vulkan shader build system,mtavenrath,2024-08-29T13:50:47Z,2024-09-06T06:56:17+00:00,1,185.09
9249,src: make tail invalid when kv cell is intersection for mamba,kylo5aby,2024-08-30T09:09:45Z,2024-09-02T17:53:23+00:00,3,80.73
9251,Enable use to the rebar feature to upload buffers to the device.,mtavenrath,2024-08-30T09:37:50Z,2024-09-28T10:05:05+00:00,1,696.45
9255,feat: Implements retrying logic for downloading models using --model-url flag,farbodbj,2024-08-30T21:11:40Z,2024-09-11T09:22:37+00:00,4,276.18
9258,Add missing pthread includes on FreeBSD,yurivict,2024-08-31T06:35:06Z,2024-09-02T15:25:31+00:00,1,56.84
9261,nix: update flake.lock,ggerganov,2024-09-01T00:23:28Z,2024-09-03T23:36:43+00:00,1,71.22
9265,ggml: fix build break for the vulkan-debug.,cyzero-kim,2024-09-01T14:04:22Z,2024-09-06T12:54:50+00:00,2,118.84
9270,llama : disambiguate API,ggerganov,2024-09-02T07:07:16Z,2024-09-10T08:30:19+00:00,1,193.38
9274,server : refactor multitask handling,ngxson,2024-09-02T10:22:42Z,2024-09-02T15:11:51+00:00,3,4.82
9278,docker : fix missing binaries in full-cuda image,slaren,2024-09-02T14:49:47Z,2024-09-02T16:11:13+00:00,1,1.36
9279,[SYCL] Fix DMMV dequantization,OuadiElfarouki,2024-09-02T16:24:28Z,2024-09-04T15:26:33+00:00,2,47.03
9282,server : test script : add timeout for all requests,ngxson,2024-09-02T18:27:07Z,2024-09-02T20:08:38+00:00,1,1.69
9283,server : simplify state machine for slot,ngxson,2024-09-02T20:18:19Z,2024-09-06T21:21:29+00:00,2,97.05
9287,llama-bench : log benchmark progress,akx,2024-09-03T06:17:47Z,2024-09-06T21:03:01+00:00,1,86.75
9288,llama-bench : add JSONL (NDJSON) output mode,akx,2024-09-03T06:29:35Z,2024-09-03T17:58:54+00:00,1,11.49
9290,"Implemented vector length agnostic SVE using switch case for 512-bit, 256-bit, and 128-bit Vector lengths",Vithulep,2024-09-03T06:54:39Z,2024-09-09T15:37:18+00:00,3,152.71
9293,batched-bench : JSONL output,akx,2024-09-03T08:56:50Z,2024-09-06T15:59:58+00:00,11,79.05
9294,llama : refactor sampling v2,ggerganov,2024-09-03T09:10:47Z,2024-09-07T12:16:20+00:00,12,99.09
9296,rpc : make RPC servers come first in the device list,rgerganov,2024-09-03T09:39:33Z,2024-09-04T08:08:32+00:00,6,22.48
9300,readme : rename result_format to response_format,iscy,2024-09-03T19:23:57Z,2024-09-04T06:45:40+00:00,1,11.36
9305,batched-bench : remove unused code,ggerganov,2024-09-04T06:32:06Z,2024-09-11T07:03:54+00:00,1,168.53
9306,Fix broken links in docker.md,carlory,2024-09-04T07:14:17Z,2024-09-04T11:45:28+00:00,1,4.52
9308,common : refactor arg parser,ngxson,2024-09-04T11:49:59Z,2024-09-07T18:43:51+00:00,9,78.9
9313,llama-bench : fix NUL terminators in CPU name,slaren,2024-09-04T15:06:43Z,2024-09-05T00:19:39+00:00,1,9.22
9319,cuda : fix defrag with quantized KV,slaren,2024-09-05T01:48:40Z,2024-09-05T09:13:11+00:00,1,7.41
9320,rpc : update README [no ci],rgerganov,2024-09-05T09:05:01Z,2024-09-09T08:04:39+00:00,2,94.99
9321,Arm AArch64: Documentation updates,eddnjjn,2024-09-05T09:23:13Z,2024-09-09T07:02:45+00:00,1,93.66
9322,Support MiniCPM3.,CarryFun,2024-09-05T12:35:36Z,2024-09-16T06:45:20+00:00,15,258.16
9329,ci: Update HIP SDK to 24.Q3 (ROCm 6.1),no1wudi,2024-09-06T04:15:08Z,2024-09-12T11:28:44+00:00,1,151.23
9330,"Only enable sgemm for prompt processing, not for inference",netrunnereve,2024-09-06T04:17:12Z,2024-09-07T19:02:26+00:00,1,38.75
9331,"ggml: Add run-time detection of neon, i8mm and sve",eddnjjn,2024-09-06T05:55:14Z,2024-09-28T12:06:16+00:00,6,534.18
9333,common: warmup: Handle situation when eos=bos=-1,MollySophia,2024-09-06T08:47:14Z,2024-09-08T02:18:01+00:00,2,41.51
9334,server : fix missing lock,ngxson,2024-09-06T09:23:16Z,2024-09-06T12:06:04+00:00,1,2.71
9336,ggml : fix missing `cpu_set_t` on emscripten,ngxson,2024-09-06T14:25:26Z,2024-09-07T10:01:35+00:00,2,19.6
9338,`GGML_TARGET_DEFINES-NOTFOUND` fix for builds without `GGML_CDEF_PUBLIC`,Xarbirus,2024-09-06T19:09:56Z,2024-09-12T11:30:01+00:00,1,136.33
9339,ggml: link MATH_LIBRARY not by its full path,Xarbirus,2024-09-06T20:15:11Z,2024-09-16T11:06:50+00:00,1,230.86
9340,ci : disable rocm image creation,slaren,2024-09-06T20:39:13Z,2024-09-07T07:48:55+00:00,1,11.16
9346,[SYCL]add check malloc result on device,NeoZhangJianyu,2024-09-07T08:57:42Z,2024-09-08T11:05:30+00:00,2,26.13
9348,llama : set attrs of mislabelled EOT/EOM tokens,bakkot,2024-09-07T12:52:43Z,2024-09-08T05:51:01+00:00,1,16.97
9355,llama : llama_perf + option to disable timings during decode,ggerganov,2024-09-07T17:53:02Z,2024-09-13T06:53:38+00:00,13,133.01
9358,llama : fix empty ring buffer push,ggerganov,2024-09-07T20:31:59Z,2024-09-07T21:33:34+00:00,2,1.03
9359,llama : sanitize tokens in the upper bound,slaren,2024-09-07T22:35:37Z,2024-09-08T10:41:51+00:00,1,12.1
9360,nix: update flake.lock,ggerganov,2024-09-08T00:21:49Z,2024-09-10T22:47:00+00:00,1,70.42
9361,feat: add Phi-1.5/Phi-2 tokenizer,daminho,2024-09-08T04:35:48Z,2024-09-12T11:28:20+00:00,1,102.88
9371,common : restore --n-gpu-layers,slaren,2024-09-08T13:08:49Z,2024-09-08T14:44:42+00:00,2,1.6
9374,cuda : fix FA Q src index (1 -> 0),ggerganov,2024-09-08T15:25:03Z,2024-09-08T19:01:02+00:00,1,3.6
9377,ci: bump actions/checkout to v4,trivikr,2024-09-08T23:24:39Z,2024-09-12T11:27:45+00:00,1,84.05
9381,Add LLMUnity to UI projects,amakropoulos,2024-09-09T09:35:39Z,2024-09-09T11:21:38+00:00,1,1.77
9385,llama : update llm_build_copy_mask_state comment [no ci],danbev,2024-09-09T13:18:20Z,2024-09-10T07:03:21+00:00,1,17.75
9386,llama : minor sampling refactor (2),slaren,2024-09-09T13:22:49Z,2024-09-09T15:10:46+00:00,2,1.8
9387,RWKV v6: Add time_mix_decay_w1/w2 in quant exclusion list,MollySophia,2024-09-09T13:39:37Z,2024-09-10T07:02:30+00:00,2,17.38
9388,common : move arg parser code to `arg.cpp`,ngxson,2024-09-09T13:50:41Z,2024-09-09T21:36:09+00:00,12,7.76
9389,rpc : fix segfault with nkvo,rgerganov,2024-09-09T14:00:56Z,2024-09-09T15:40:10+00:00,1,1.65
9394,sycl : update support conditions ,Alcpz,2024-09-09T22:17:22Z,2024-09-11T00:53:42+00:00,3,26.61
9396,convert : refactor rope_freqs generation,compilade,2024-09-10T00:59:15Z,2024-10-01T06:31:37+00:00,2,509.54
9397,convert : identify missing model files,compilade,2024-09-10T01:09:27Z,2024-09-16T07:30:22+00:00,1,150.35
9398,llama : move random seed generation to the samplers,slaren,2024-09-10T01:42:13Z,2024-09-10T16:04:25+00:00,1,14.37
9399,make : do not run llama-gen-docs when building,slaren,2024-09-10T01:49:13Z,2024-09-10T06:23:33+00:00,1,4.57
9400,imatrix : use GGUF to store importance matrices,compilade,2024-09-10T02:14:44Z,2025-07-19T16:51:23+00:00,14,7502.61
9401,server: added loading page while model loading,VJHack,2024-09-10T03:08:12Z,2024-09-13T11:00:20+00:00,8,79.87
9405,naming : normalize the name of callback-related identifiers,ggerganov,2024-09-10T09:30:11Z,2025-07-02T18:52:43+00:00,1,7089.38
9406,[CANN]: Add host buffer type for Ascend NPU,bachelor-dou,2024-09-10T11:59:13Z,2024-09-12T11:46:43+00:00,4,47.79
9407,vulkan : do not use tensor->extra,rgerganov,2024-09-10T13:28:22Z,2024-10-02T10:49:16+00:00,1,525.35
9408,"ggml : hide ggml_object, ggml_cgraph, ggml_hash_set",ggerganov,2024-09-10T13:44:51Z,2024-09-12T11:23:50+00:00,1,45.65
9411,arg : bring back missing ifdef,ngxson,2024-09-10T15:12:32Z,2024-09-10T20:41:29+00:00,1,5.48
9412,IBM Granite Architecture,gabe-l-hart,2024-09-10T16:11:11Z,2024-09-17T06:44:58+00:00,5,158.56
9413,CUDA: fix --split-mode row race condition,JohannesGaessler,2024-09-10T16:12:48Z,2024-09-11T08:22:40+00:00,1,16.16
9418,common : reimplement logging,ggerganov,2024-09-10T17:43:03Z,2024-09-15T17:46:12+00:00,1,120.05
9419,add back the --special flag in llama-server,matteoserva,2024-09-10T18:10:41Z,2024-09-10T20:40:59+00:00,1,2.5
9422,IQ4_NL sgemm + Q4_0 AVX optimization,netrunnereve,2024-09-11T03:26:53Z,2024-09-16T06:48:25+00:00,1,123.36
9424,[CANN]: Fix error when running a non-exist op,bachelor-dou,2024-09-11T03:53:27Z,2024-09-12T01:02:35+00:00,1,21.15
9428,llama: Add special tokens in hf_converter for RWKV v6,MollySophia,2024-09-11T08:39:20Z,2024-09-12T11:25:16+00:00,1,26.77
9429,llava : correct args for minicpmv-cli,ngxson,2024-09-11T09:21:54Z,2024-09-11T10:59:13+00:00,1,1.62
9438,IBM Granite MoE Architecture,gabe-l-hart,2024-09-11T16:22:47Z,2024-09-25T07:06:52+00:00,13,326.73
9439,Add Jais to list of supported models,fmz,2024-09-11T16:51:38Z,2024-09-12T00:29:53+00:00,1,7.64
9442,[RISCV] Modify Makefile and Add a RISCV_VECT to print log info,Tameem-10xE,2024-09-11T18:39:06Z,2024-09-12T11:24:31+00:00,1,16.76
9445,feat: remove a sampler from a chain,giladgd,2024-09-12T00:05:03Z,2024-09-13T01:54:49+00:00,5,25.83
9448,[SYCL] enhance run script to be easy to change the parameters,NeoZhangJianyu,2024-09-12T02:58:25Z,2024-09-12T09:44:17+00:00,1,6.76
9449,vocab: refactor tokenizer to reduce the overhead of creating multi times tokenizer,kylo5aby,2024-09-12T09:40:20Z,2024-09-28T12:10:58+00:00,10,386.51
9450,fixed the order of linking libraries for llama-quantize,Xarbirus,2024-09-12T10:01:36Z,2024-09-12T11:27:14+00:00,1,1.43
9454,RWKV v6: RWKV_WKV op CUDA implementation,MollySophia,2024-09-12T16:02:00Z,2024-09-22T02:29:12+00:00,5,226.45
9458,ggml : allow using the default ggml_type value in the name function,ykhrustalev,2024-09-13T02:52:20Z,2024-09-14T09:54:37+00:00,1,31.04
9459,server : add [DONE] event to /chat/completions stream response,VoidIsVoid,2024-09-13T03:10:50Z,2024-09-14T09:36:44+00:00,3,30.43
9461,threadpool: skip polling for unused threads,max-krasnyansky,2024-09-13T05:22:29Z,2024-09-17T08:19:47+00:00,2,98.95
9462,Implement OLMoE architecture,2015aroras,2024-09-13T06:20:37Z,2024-09-16T06:47:37+00:00,1,72.45
9463,cmake : use list(APPEND ...) instead of set() + dedup linker,ggerganov,2024-09-13T06:45:45Z,2024-09-14T07:55:05+00:00,2,25.16
9467,CI: Provide prebuilt windows binary for hip,no1wudi,2024-09-13T08:48:29Z,2024-09-21T00:39:41+00:00,2,183.85
9468,server : add loading html page while model is loading,ngxson,2024-09-13T10:59:40Z,2024-09-13T12:23:11+00:00,1,1.39
9470,llama : make cell_id const in inp_s_mask block,danbev,2024-09-13T11:55:43Z,2024-09-14T07:50:12+00:00,1,19.91
9475,Added link to proprietary wrapper for Unity3d into README.md,OLSecret,2024-09-13T21:31:45Z,2024-09-15T07:36:53+00:00,2,34.09
9476,"[SYCL]set context default value to avoid memory issue, update guide",NeoZhangJianyu,2024-09-14T01:15:38Z,2024-09-18T00:30:31+00:00,2,95.25
9482,Update clip.cpp,Tejaakshaykumar,2024-09-14T13:32:57Z,2024-09-26T01:26:42+00:00,12,275.9
9484,main: option to disable context shift,VJHack,2024-09-14T17:34:31Z,2024-09-16T06:20:01+00:00,1,36.76
9485,"nvidia uses the LLaMAForCausalLM string in their config.json, example…",csabakecskemeti,2024-09-14T19:04:30Z,2024-09-15T07:48:25+00:00,1,12.73
9488,nix: update flake.lock,ggerganov,2024-09-15T00:22:35Z,2024-09-16T02:14:23+00:00,1,25.86
9504,llama : rename n_embed to n_embd in rwkv6_time_mix,danbev,2024-09-16T07:16:28Z,2024-09-16T11:07:13+00:00,1,3.85
9508,llama.cpp: Add a missing header for cpp23,ykhrustalev,2024-09-16T12:49:23Z,2024-09-17T06:51:15+00:00,1,18.03
9509,ggml : move common CPU backend impl to new header,slaren,2024-09-16T13:20:38Z,2024-09-16T14:22:08+00:00,1,1.02
9510,llama : add reranking support,ggerganov,2024-09-16T14:01:50Z,2024-09-28T14:42:04+00:00,10,288.67
9511,Fixed n vocab,Xarbirus,2024-09-16T16:38:01Z,2024-09-17T10:18:22+00:00,4,17.67
9512,llama: public llama_n_head,Xarbirus,2024-09-16T17:32:06Z,2024-09-17T06:23:30+00:00,1,12.86
9513,add env variable for parallel,bertwagner,2024-09-16T19:04:58Z,2024-09-17T13:35:39+00:00,1,18.51
9519,docs: update server streaming mode documentation,CentricStorm,2024-09-17T05:03:29Z,2024-12-11T22:40:40+00:00,7,2057.62
9525,llama: (proposal) propagating the results of `graph_compute` to the user interface,Xarbirus,2024-09-17T20:33:07Z,2024-11-13T18:00:35+00:00,1,1365.46
9526,"musa: enable building fat binaries, enable unified memory, and disable Flash Attention on QY1 (MTT S80)",yeahdongcn,2024-09-18T01:59:34Z,2024-09-22T14:55:49+00:00,12,108.94
9527,bugfix:  structured output response_format does not match openai ,VJHack,2024-09-18T02:14:53Z,2024-09-18T06:50:34+00:00,1,4.59
9529,server : fix OpenSSL build by removing invalid `LOG_INFO` references,EZForever,2024-09-18T02:59:27Z,2024-09-18T06:28:20+00:00,1,3.48
9531,server : clean-up completed tasks from waiting list,ggerganov,2024-09-18T07:22:34Z,2024-09-19T09:44:53+00:00,1,26.37
9532,Implementations for Q4_0_8_8 quantization based functions - AVX512 version of ggml_gemm_q4_0_8x8_q8_0,Srihari-mcw,2024-09-18T08:34:02Z,2024-09-23T14:06:39+00:00,3,125.54
9534,llama : use reserve/emplace_back in sampler_sample,danbev,2024-09-18T09:51:04Z,2024-09-18T11:42:37+00:00,1,1.86
9538,ggml : fix n_threads_cur initialization with one thread,slaren,2024-09-18T12:59:55Z,2024-09-18T17:13:08+00:00,3,4.22
9541,add solar pro support,mxyng,2024-09-18T22:38:06Z,2024-12-02T18:47:47+00:00,3,1796.16
9543,Imatrix input data should not be unescaped,CISC,2024-09-18T23:36:39Z,2024-09-19T07:58:14+00:00,1,8.36
9544,server: disable context shift,VJHack,2024-09-19T00:53:21Z,2024-09-23T16:12:34+00:00,3,111.32
9548,Perplexity input data should not be unescaped,CISC,2024-09-19T08:51:11Z,2024-09-20T06:38:10+00:00,1,21.78
9550,Update CUDA graph on scale change plus clear nodes/params ,agray3,2024-09-19T12:57:31Z,2024-09-21T00:41:07+00:00,3,35.73
9557,baby-llama : use unnamed namespace in baby_llama_layer,danbev,2024-09-20T02:56:28Z,2025-02-05T04:27:38+00:00,1,3313.52
9562,CUDA: fix sum.cu compilation for CUDA < 11.7,JohannesGaessler,2024-09-20T08:02:06Z,2024-09-20T16:35:36+00:00,1,8.56
9567,sync : ggml,ggerganov,2024-09-20T16:10:41Z,2024-09-20T18:15:05+00:00,3,2.07
9571,CUDA: Enable K-shift operation for -ctk q8_0 (limited),unknown,2024-09-20T19:11:00Z,2024-09-24T00:14:24+00:00,1,77.06
9573,ggml-alloc : fix list of allocated tensors with GGML_ALLOCATOR_DEBUG,slaren,2024-09-21T01:39:42Z,2024-09-21T12:24:23+00:00,1,10.74
9574,llama: remove redundant loop when constructing ubatch,shankarg87,2024-09-21T02:06:07Z,2024-09-22T02:30:34+00:00,2,24.41
9577,[SYCL] add missed dll file in package,NeoZhangJianyu,2024-09-21T11:41:25Z,2024-09-26T09:38:32+00:00,1,117.95
9579,"Revert ""[SYCL] fallback mmvq""",qnixsynapse,2024-09-21T14:35:37Z,2024-09-23T03:28:06+00:00,1,36.87
9581,CUDA: enable Gemma FA for HIP/Pascal,JohannesGaessler,2024-09-21T18:19:46Z,2024-09-22T07:34:52+00:00,1,13.25
9586,nix: update flake.lock,ggerganov,2024-09-22T00:22:50Z,2024-09-23T15:43:40+00:00,1,39.35
9591,Added link to Bielik model,32bitmicro,2024-09-22T14:20:33Z,2024-10-01T17:18:46+00:00,1,218.97
9597,musa: enable VMM support,yeahdongcn,2024-09-23T00:40:44Z,2024-09-26T01:27:41+00:00,1,72.78
9598,threads: improve ggml_barrier scaling with large number of threads,max-krasnyansky,2024-09-23T01:51:45Z,2024-09-23T18:42:43+00:00,1,16.85
9599,readme: Add offline-ai/cli programmable prompt engine language CLI for llama.cpp server,snowyu,2024-09-23T05:51:13Z,2024-09-23T15:58:17+00:00,1,10.12
9605,sampling : avoid expensive softmax during greedy sampling,ggerganov,2024-09-23T09:49:11Z,2024-09-24T06:03:17+00:00,2,20.23
9607,server : add --no-context-shift option,ngxson,2024-09-23T11:37:42Z,2024-09-23T20:23:54+00:00,2,8.77
9615,threads: fix msvc build without openmp,max-krasnyansky,2024-09-23T22:30:07Z,2024-09-24T04:18:49+00:00,1,5.81
9616,Add newline after chat example in llama-server,StrangeBytesDev,2024-09-23T23:40:11Z,2024-09-24T06:04:39+00:00,1,6.41
9619,make sure params --split and --merge are not specified at same time in gguf-split,kylo5aby,2024-09-24T02:32:40Z,2024-10-02T07:21:58+00:00,4,196.82
9622,ggml : add AVX512DQ requirement for AVX512 builds,EZForever,2024-09-24T06:45:20Z,2024-09-24T08:03:21+00:00,1,1.3
9627,[CANN]: Fix crash when running on multiple cann devices,bachelor-dou,2024-09-24T13:03:56Z,2024-09-25T03:30:38+00:00,1,14.45
9635,"server : add more env vars, improve gen-docs",ngxson,2024-09-25T10:33:57Z,2024-09-25T12:05:13+00:00,1,1.52
9638,ci : fix docker build number and tag name,ngxson,2024-09-25T14:14:46Z,2024-09-25T15:26:01+00:00,2,1.19
9639,"Tool call support (generic + native for Llama, Functionary, Hermes, Mistral, Firefunction, DeepSeek) w/ lazy grammars",ochafik,2024-09-25T15:37:26Z,2025-01-30T19:13:59+00:00,22,3051.61
9641,"Fix Docker ROCM builds, use AMDGPU_TARGETS instead of GPU_TARGETS",serhii-nakon,2024-09-25T19:36:05Z,2024-09-30T18:57:12+00:00,1,119.35
9655,Docs: Add akx/ollama-dl,akx,2024-09-26T12:57:48Z,2024-09-28T12:07:15+00:00,1,47.16
9657,test-backend-ops : use flops for some performance tests,slaren,2024-09-26T15:54:13Z,2024-09-28T12:32:46+00:00,4,44.64
9658,sycl: initial cmake support of SYCL for AMD GPUs,Alcpz,2024-09-26T16:16:04Z,2024-10-02T12:57:18+00:00,3,140.69
9661,cmake : add option for common library,iboB,2024-09-26T17:48:01Z,2024-09-27T07:42:06+00:00,1,13.9
9668,common: ensure token addition to batch does not exceed llama_batch size,matiaslin,2024-09-27T17:17:42Z,2024-09-29T12:25:00+00:00,2,43.12
9672,Update building for Android,amqdn,2024-09-27T22:10:54Z,2024-10-07T16:37:31+00:00,2,234.44
9675,contrib : add Resources section,ggerganov,2024-09-28T12:38:14Z,2024-09-29T11:38:18+00:00,1,23.0
9679,`server`: cancel prompt processing & non-streamed requests when connection closed,ochafik,2024-09-29T00:22:05Z,2025-01-22T10:06:31+00:00,9,2769.74
9680,nix: update flake.lock,ggerganov,2024-09-29T00:40:14Z,2024-09-30T14:48:49+00:00,1,38.14
9683,Use new model class for chameleon conversion,nopperl,2024-09-29T09:02:32Z,2024-09-29T12:02:06+00:00,1,2.99
9685,musa: add docker image support,yeahdongcn,2024-09-29T12:26:07Z,2024-10-10T18:10:37+00:00,8,269.74
9690,utf-8 fix for windows stdin,hasaranga,2024-09-29T18:03:04Z,2024-09-30T08:23:42+00:00,1,14.34
9696,convert : handle tokenizer merges format from transformers 4.45,compilade,2024-09-30T15:24:20Z,2024-10-03T14:22:15+00:00,4,70.97
9698,metal : reduce command encoding overhead,ggerganov,2024-09-30T18:06:04Z,2024-10-01T13:00:25+00:00,4,18.91
9702,added implementation of DRY sampler (post-refactor),wwoodsTM,2024-10-01T09:46:34Z,2024-10-25T16:07:34+00:00,64,582.35
9705,"[SYCL] Add SYCL Backend registry, device and Event Interfaces",OuadiElfarouki,2024-10-01T15:03:22Z,2024-10-18T05:46:16+00:00,7,398.71
9707,ggml-backend : add device and backend reg interfaces,slaren,2024-10-01T15:26:41Z,2024-10-02T23:49:47+00:00,11,32.38
9709,ggml: unify backend logging mechanism,bandoti,2024-10-01T20:00:24Z,2024-10-03T15:39:03+00:00,2,43.64
9710,ci : fine-grant permission,ngxson,2024-10-01T21:23:07Z,2024-10-04T09:47:19+00:00,1,60.4
9711,[SYCL] Fixed GET_ROWS failing unit-tests for type 1 quantizations,OuadiElfarouki,2024-10-02T02:30:18Z,2024-10-03T06:50:44+00:00,2,28.34
9712,llama : reduce compile time and binary size,ngxson,2024-10-02T07:34:06Z,2024-10-02T13:49:56+00:00,1,6.26
9713,ggml : add metal backend registry / device,ggerganov,2024-10-02T10:38:58Z,2024-10-07T15:27:51+00:00,6,124.81
9714,rpc : enable vulkan,rgerganov,2024-10-02T11:01:01Z,2024-10-03T10:00:52+00:00,2,23.0
9718,metal : fix compute pass descriptor autorelease crash,jmousseau,2024-10-02T20:32:19Z,2024-10-03T18:01:46+00:00,3,21.49
9720,ggml-backend : add device description to CPU backend,slaren,2024-10-03T00:46:07Z,2024-10-03T15:39:18+00:00,2,14.89
9721,vulkan : add backend registry / device interfaces,slaren,2024-10-03T01:39:59Z,2024-10-17T00:46:58+00:00,1,335.12
9723,Fixed RNG seed docs,d-kleine,2024-10-03T03:59:45Z,2024-10-04T08:54:44+00:00,1,28.92
9734,vulkan : add GGML_VK_FORCE_HEAP_INDEX env var,gyf304,2024-10-04T02:40:53Z,2025-08-23T02:49:14+00:00,4,7752.14
9737,rerank : use [SEP] token instead of [BOS],ggerganov,2024-10-04T08:56:11Z,2024-10-05T12:55:04+00:00,1,27.98
9742,sampling : add XTC sampler,MaggotHATE,2024-10-04T14:01:31Z,2024-10-15T10:54:55+00:00,46,260.89
9744,"Add Llama Assistant to the ""UI""",vietanhdev,2024-10-04T16:44:53Z,2024-10-04T18:29:35+00:00,1,1.75
9745,"llama : remove all_pos_0, all_pos_1, all_seq_id from llama_batch",ngxson,2024-10-04T21:23:31Z,2024-10-18T21:18:01+00:00,10,335.91
9747,Single allocation of encode_async block with non-ARC capture in ggml-metal.m,ptsochantaris,2024-10-05T01:07:28Z,2024-10-07T12:26:31+00:00,4,59.32
9752,ggml : add backend registry / device interfaces to BLAS backend,slaren,2024-10-05T19:16:08Z,2024-10-07T19:55:08+00:00,1,48.65
9753,nix: update flake.lock,ggerganov,2024-10-06T00:22:43Z,2024-10-07T16:35:42+00:00,1,40.22
9763,ggml: Add POOL2D OP for GPU acceleration to the Vulkan backend in the MobileVLM model.,cyzero-kim,2024-10-06T14:35:01Z,2024-10-29T08:52:56+00:00,1,546.3
9772,[gguf-py] gguf_reader: numpy 2 newbyteorder fix,jettjaniak,2024-10-07T12:12:38Z,2024-12-13T14:48:45+00:00,1,1610.6
9775,ggml : fix BLAS with unsupported types,slaren,2024-10-07T20:10:11Z,2024-10-08T12:21:43+00:00,1,16.19
9776,server : better security control for public deployments,ngxson,2024-10-07T21:32:09Z,2024-10-08T11:27:04+00:00,1,13.92
9778,Added support for SFTTrainer checkpoint models and adapter models containing some non-LoRA weights,Victoran0,2024-10-08T01:41:47Z,2024-10-19T18:41:42+00:00,4,281.0
9782,scripts : Fix spelling typo in messages and comments,standby24x7,2024-10-08T04:16:09Z,2024-10-08T06:19:53+00:00,1,2.06
9783,perplexity : fix integer overflow,ggerganov,2024-10-08T06:16:23Z,2024-10-09T14:00:18+00:00,6,31.73
9795,fix logging in examples/main/main.cpp,fuzzybritches0,2024-10-08T21:25:41Z,2024-10-23T19:45:54+00:00,1,358.34
9798,llama : improve infill support and special token detection,ggerganov,2024-10-09T07:49:59Z,2024-10-12T05:21:51+00:00,9,69.53
9803,examples : do not use common library in simple example,slaren,2024-10-09T09:58:07Z,2024-10-10T17:50:49+00:00,2,31.88
9804,cmake : do not build common library by default when standalone,slaren,2024-10-09T10:01:22Z,2024-10-09T16:49:52+00:00,1,6.81
9805,common : use common_ prefix for common library functions,slaren,2024-10-09T10:09:01Z,2024-10-10T20:57:42+00:00,3,34.81
9807,fix gguf-py:  Conversion error when multiple licenses are configured,mmngays,2024-10-09T12:26:47Z,2024-11-24T00:09:23+00:00,2,1091.71
9812,rpc : add backend registry / device interfaces,slaren,2024-10-09T19:57:04Z,2024-10-10T18:14:56+00:00,2,22.3
9816,Vectorize load instructions in dmmv f16 CUDA kernel,agray3,2024-10-10T09:03:25Z,2024-10-14T00:49:08+00:00,4,87.76
9839,ggml : move more prints to the ggml log system,slaren,2024-10-11T06:32:01Z,2024-10-11T13:34:46+00:00,1,7.05
9856,musa: update doc,yeahdongcn,2024-10-12T02:35:17Z,2024-10-12T05:09:54+00:00,1,2.58
9857,server : remove legacy system_prompt feature,ggerganov,2024-10-12T05:59:48Z,2024-10-12T11:51:54+00:00,1,5.87
9860,server : remove self-extend features,ggerganov,2024-10-12T08:01:38Z,2024-10-12T13:06:31+00:00,5,5.08
9867,server: fix the disappearance of the end of the text when streaming with stop strings,z80maniac,2024-10-12T15:50:22Z,2024-10-16T08:35:53+00:00,3,88.76
9870,nix: update flake.lock,ggerganov,2024-10-13T00:22:29Z,2024-10-13T03:11:26+00:00,1,2.82
9871,"server: Handle ""logprobs"" field with false value",VoidIsVoid,2024-10-13T01:43:35Z,2024-10-14T07:04:37+00:00,1,29.35
9875,fix: use `vm_allocate` to allocate CPU backend buffer on macOS,giladgd,2024-10-14T00:42:26Z,2024-10-16T22:36:52+00:00,6,69.91
9879,llava : fix memory leaks in minicpmv,tc-mb,2024-10-14T08:04:06Z,2025-01-18T14:06:21+00:00,4,2310.04
9889,Added swift bindings repo to readme,srgtuszy,2024-10-14T20:54:02Z,2024-10-15T08:20:34+00:00,1,11.44
9891,Fix cann compilation error after llama.cpp supports dynamically loadable backends,leo-pony,2024-10-15T02:06:02Z,2024-10-16T00:51:46+00:00,1,22.76
9896,llama : add infill sampler,ggerganov,2024-10-15T10:11:32Z,2024-10-15T13:35:33+00:00,5,3.4
9897,llama : default sampling changes + greedy update,ggerganov,2024-10-15T11:27:05Z,2024-10-21T06:46:40+00:00,1,139.33
9898,vulkan : improve ggml_vk_create_buffer error handling,FanShupei,2024-10-15T11:52:41Z,2024-11-01T18:33:14+00:00,2,414.68
9903,Fix JSON Schema to Grammar for string regexp with top-level alternation.,jemc,2024-10-16T02:02:25Z,2024-10-16T16:03:25+00:00,1,14.02
9912,rpc : backend refactoring,rgerganov,2024-10-16T13:31:57Z,2024-10-18T11:33:58+00:00,9,46.03
9921,backend cpu: add online flow for aarch64 Q4_0 GEMV/GEMM kernels,chaxu01,2024-10-17T08:09:36Z,2024-11-15T00:28:50+00:00,4,688.32
9930,ggml : fix possible buffer use after free in sched reserve,slaren,2024-10-17T22:54:40Z,2024-11-17T06:31:17+00:00,1,727.61
9935,loader:  refactor tensor weights storage,kylo5aby,2024-10-18T09:10:33Z,2024-10-31T18:50:40+00:00,2,321.67
9939,[SYCL]fix mul_mat_vec_q error,NeoZhangJianyu,2024-10-18T13:05:09Z,2024-10-21T06:26:09+00:00,2,65.35
9942,Add llama_cpp_canister to the README,icppWorld,2024-10-18T15:17:33Z,2024-10-20T16:01:34+00:00,1,48.73
9943,ggml:metal Add POOL2D op and fix IM2COL in Metal backend for running MobileVLM_V2.,junhee-yoo,2024-10-18T15:49:25Z,2024-10-23T10:33:45+00:00,8,114.74
9948,lora : error message if new token is added in the adapter,ngxson,2024-10-18T21:39:58Z,2024-10-22T11:08:42+00:00,1,85.48
9950,llama : rename batch to ubatch,danbev,2024-10-19T04:50:07Z,2024-10-22T13:31:06+00:00,1,80.68
9951,Update README.md,lcarrere,2024-10-19T11:22:09Z,2024-10-20T16:25:41+00:00,1,29.06
9955,nix: update flake.lock,ggerganov,2024-10-20T00:23:02Z,2024-10-23T01:28:07+00:00,1,73.08
9959,rpc : pack only RPC structs,rgerganov,2024-10-20T07:25:00Z,2024-10-21T10:35:41+00:00,1,27.18
9963,speculative : fix batch sizes at initialization,ggerganov,2024-10-20T16:40:44Z,2024-10-21T06:37:12+00:00,1,13.94
9966,llama : fix empty batch causing llama_batch_allocr to crash,ngxson,2024-10-20T22:13:09Z,2024-10-22T14:59:02+00:00,3,40.76
9968,Add chat template for RWKV-World,MollySophia,2024-10-21T01:41:29Z,2024-10-22T10:33:37+00:00,5,32.87
9970,[CANN] Adapt to dynamically loadable backends mechanism,leo-pony,2024-10-21T03:28:27Z,2024-10-22T08:16:01+00:00,5,28.79
9971,ggml : add asserts for type conversion in fattn kernels,ggerganov,2024-10-21T06:04:18Z,2024-10-21T13:20:46+00:00,1,7.27
9972,Update README.md,a-ghorbani,2024-10-21T08:04:02Z,2024-10-21T18:20:59+00:00,1,10.28
9976,[SYCL] Fix build on Windows when ccache enabled (#9954),MakeDecisionWorth,2024-10-21T08:54:21Z,2025-03-21T06:58:47+00:00,4,3622.07
9985,arg : fix attention non-causal arg value hint,danbev,2024-10-21T16:48:39Z,2024-10-21T18:12:53+00:00,1,1.4
9989,cmake: force MSVC compiler charset to utf-8,MakeDecisionWorth,2024-10-21T19:31:18Z,2024-11-19T17:42:00+00:00,5,694.18
9994,arg : fix typo in embeddings argument help [no ci],danbev,2024-10-22T06:00:05Z,2024-10-22T07:40:02+00:00,1,1.67
9995,llama.vim : add classic vim support,m18coppola,2024-10-22T06:18:25Z,2024-10-23T11:09:26+00:00,10,28.85
10001,Rwkv chat template fix,MollySophia,2024-10-22T11:32:19Z,2024-10-22T13:22:26+00:00,3,1.84
10010,Extend sgemm.cpp support for Q5_0 models,Srihari-mcw,2024-10-23T06:18:29Z,2024-10-25T07:27:41+00:00,1,49.15
10013,llama : Add IBM granite template,arch-btw,2024-10-23T11:29:41Z,2024-10-28T17:45:33+00:00,5,126.26
10019,Server - Sampling bug fix,wwoodsTM,2024-10-23T17:06:01Z,2024-10-23T19:27:51+00:00,2,2.36
10021,"CUDA: fix MMQ for non-contiguous src0, add tests",JohannesGaessler,2024-10-23T18:42:55Z,2024-10-24T09:09:36+00:00,3,14.44
10022,llama: string_split fix,Xarbirus,2024-10-23T19:31:01Z,2024-10-25T15:57:55+00:00,1,44.45
10023,"server : refactor slot input data, move tokenizer to HTTP thread",ngxson,2024-10-23T21:19:57Z,2024-10-24T19:51:22+00:00,3,22.52
10026,llama : refactor model loader with backend registry,slaren,2024-10-24T00:37:20Z,2024-10-30T01:01:23+00:00,1,144.4
10029,ggml : Implementations for Q4_0_8_8 quantization based functions - RISC-V vector version,xctan,2024-10-24T07:12:26Z,2024-10-30T07:00:40+00:00,1,143.8
10030,server : check that the prompt fits in the slot's context,ggerganov,2024-10-24T08:06:06Z,2024-10-25T07:13:46+00:00,2,23.13
10032,CUDA: fix insufficient buffer clearing for MMQ,JohannesGaessler,2024-10-24T09:57:19Z,2024-10-24T12:40:23+00:00,1,2.72
10041,[SYCL] pass SYCL CI,airMeng,2024-10-25T07:44:06Z,2025-07-30T08:50:43+00:00,2,6673.11
10042,musa: workaround for Guilty Lockup in cleaning src0 in #10032,yeahdongcn,2024-10-25T08:16:41Z,2024-10-28T09:02:49+00:00,1,72.77
10045,kompute: add backend registry / device interfaces and Q4_K shader,slp,2024-10-25T16:30:16Z,2024-10-30T16:01:52+00:00,3,119.53
10059,llama : rename missed batch params/vars to ubatch,danbev,2024-10-26T13:18:23Z,2025-01-06T09:28:18+00:00,1,1724.17
10063,nix: update flake.lock,ggerganov,2024-10-27T00:23:02Z,2024-10-28T15:41:24+00:00,1,39.31
10064,readme : more lora detail in main example readme,richdougherty,2024-10-27T02:10:15Z,2024-10-30T12:22:39+00:00,3,82.21
10065,convert : more detailed convert lora usage docs,richdougherty,2024-10-27T02:10:33Z,2024-10-30T12:22:22+00:00,3,82.2
10094,ggml : fix memory leaks when loading invalid gguf files,slaren,2024-10-30T11:51:42Z,2024-10-30T13:51:21+00:00,1,1.99
10097,kompute: add mul_mat_q4_k shader,slp,2024-10-30T16:18:13Z,2024-10-31T09:09:53+00:00,1,16.86
10098,llama : improve output buffer type selection,slaren,2024-10-30T20:11:31Z,2024-10-31T23:49:53+00:00,1,27.64
10100,ggml : check tensor name lengths in gguf files,slaren,2024-10-30T21:46:38Z,2024-10-31T10:40:59+00:00,1,12.91
10106,server : include scheme when printing URL,bakkot,2024-10-31T04:53:24Z,2024-10-31T13:02:35+00:00,1,8.15
10107,build: fix build error in Windows env with OneAPI setup,kylo5aby,2024-10-31T09:26:15Z,2024-11-01T03:10:00+00:00,2,17.73
10110,convert-lora : make `--base` optional,ngxson,2024-10-31T13:52:24Z,2024-11-02T11:53:18+00:00,3,46.02
10111,llama : fix buffer checks for mamba and rwk,slaren,2024-10-31T14:15:30Z,2024-10-31T21:54:23+00:00,5,7.65
10114,quantize : fix --keep-split,slaren,2024-10-31T20:57:34Z,2024-10-31T23:45:34+00:00,1,2.8
10117,llama : use smart pointers for ggml resources,slaren,2024-11-01T02:17:12Z,2024-11-01T22:48:26+00:00,7,20.52
10118,Q6_K AVX improvements,netrunnereve,2024-11-01T02:37:47Z,2024-11-04T22:06:32+00:00,1,91.48
10120,server : fix smart selection of available slot,sasha0552,2024-11-01T09:00:51Z,2024-11-01T13:33:14+00:00,2,4.54
10121,ggml : remove ggml_scratch,ggerganov,2024-11-01T09:27:04Z,2024-11-01T10:58:45+00:00,2,1.53
10124,llama : add simple-chat example,slaren,2024-11-01T15:54:30Z,2024-11-01T22:50:59+00:00,5,6.94
10126,server : fix slot selection by lru,sasha0552,2024-11-01T18:19:19Z,2024-11-02T16:34:57+00:00,1,22.26
10128,Server: handle generation until context is filled ,VJHack,2024-11-01T21:44:39Z,2025-06-27T19:21:18+00:00,1,5709.61
10129,simple-chat : only add bos on first prompt,slaren,2024-11-01T23:30:21Z,2024-11-02T12:08:53+00:00,1,12.64
10130,ggml : do not abort when ggml_aligned_malloc fails,slaren,2024-11-01T23:56:59Z,2024-11-04T22:14:48+00:00,2,70.3
10132,ggml : skip register metal backend on os simulator,jhen0409,2024-11-02T03:29:15Z,2025-03-05T07:04:27+00:00,1,2955.59
10133,Optimize RWKV6 Operator Naming and Implement Multi-core CPU/ SYCL Acceleration,zhiyuan1i,2024-11-02T05:37:12Z,2024-11-07T07:19:11+00:00,10,121.7
10134,Add apple arm to presets,kohnech,2024-11-02T08:33:56Z,2024-11-02T22:35:31+00:00,1,14.03
10135,server : fix endpoint checks,ggerganov,2024-11-02T09:30:58Z,2024-11-02T16:34:00+00:00,1,7.05
10136,llama : adjust default context size + print warnings,ggerganov,2024-11-02T10:39:08Z,2024-11-02T13:18:57+00:00,2,2.66
10143,metal : minor fixup in FA kernel,ggerganov,2024-11-02T18:47:59Z,2024-11-03T13:18:40+00:00,2,18.51
10144,ggml : move CPU backend to a separate file,slaren,2024-11-02T23:08:01Z,2024-11-03T18:34:09+00:00,6,19.44
10146,nix: update flake.lock,ggerganov,2024-11-03T00:37:28Z,2024-11-03T13:14:15+00:00,1,12.61
10149,metal : add quantized FA support,ggerganov,2024-11-03T13:24:19Z,2024-11-06T08:24:24+00:00,1,67.0
10153,cuda : clear error after changing peer access,slaren,2024-11-03T23:04:58Z,2024-11-04T12:10:24+00:00,2,13.09
10156,ggml : optimize llamafile's cpu matrix multiplication for ppc64le,amritahs-ibm,2024-11-04T05:05:03Z,2024-11-09T07:17:50+00:00,1,122.21
10158,[CANN] Fix compile error for CANN backend as get_name has been removed from ggml_backend_buffer_i,leo-pony,2024-11-04T08:52:16Z,2024-11-04T11:08:22+00:00,1,2.27
10162,"server : clarify /slots endpoint, add is_processing",ngxson,2024-11-04T11:04:20Z,2024-11-04T15:33:29+00:00,1,4.49
10164,ggml : fix arch check in bf16_to_fp32,slaren,2024-11-04T12:25:54Z,2024-11-04T22:17:01+00:00,1,9.85
10166,fix build break on arm64 linux,snadampal,2024-11-04T13:15:21Z,2024-11-04T15:08:34+00:00,1,1.89
10167,"ggml : fix q4xx mat mul, increase ggml_aligned_malloc alignment",slaren,2024-11-04T13:28:46Z,2024-11-04T16:34:08+00:00,1,3.09
10172,ggml : fix gelu tables initialization,slaren,2024-11-04T17:59:04Z,2024-11-04T19:06:59+00:00,1,1.13
10175,server : revamp chat UI with vuejs and daisyui,ngxson,2024-11-04T22:56:23Z,2024-11-07T21:31:11+00:00,9,70.58
10177,Add the <|tool_call|> formatting to the granite template,gabe-l-hart,2024-11-05T00:09:05Z,2024-11-05T12:23:05+00:00,1,12.23
10185,CUDA: always create events for split buffers,JohannesGaessler,2024-11-05T16:51:27Z,2024-11-14T12:00:15+00:00,2,211.15
10187,server : remove hack for extra parallel slot,ggerganov,2024-11-05T20:32:45Z,2024-11-06T11:29:01+00:00,1,14.94
10189,metal : fix from ptr buffer name,slaren,2024-11-06T00:14:04Z,2024-11-06T11:10:08+00:00,1,10.93
10192,DRY: Fixes clone functionality,wwoodsTM,2024-11-06T08:15:17Z,2024-11-07T15:20:25+00:00,1,31.09
10193,ggml : adjust is_first_call init value,ggerganov,2024-11-06T08:15:28Z,2024-11-06T09:20:10+00:00,1,1.08
10198,fix q4_0_8_8 format for corrupted tokens issue,snadampal,2024-11-07T06:00:42Z,2024-11-07T08:02:08+00:00,1,2.02
10206,vulkan: Add VK_NV_cooperative_matrix2 support for mul_mat and FlashAttention2,jeffbolznv,2024-11-07T17:32:45Z,2024-12-05T19:15:06+00:00,16,673.71
10207,server : minor UI fix,ngxson,2024-11-07T21:37:33Z,2024-11-07T22:44:38+00:00,1,1.12
10211,swift : exclude ggml-metal-embed.metal,jhen0409,2024-11-08T02:02:19Z,2024-11-08T09:34:06+00:00,1,7.53
10212,AVX BF16 and single scale quant optimizations,netrunnereve,2024-11-08T03:24:14Z,2024-11-15T11:47:58+00:00,1,176.4
10213,ggml: fix zero division in ‘dne’ calculation in CUDA COUNT_EQUAL operator when ‘ne’ is small,SongXiaoXi,2024-11-08T08:45:37Z,2024-11-09T07:35:46+00:00,1,22.84
10216,CANN Support Ascend310P to accelerate F32 and F16 LLM Model,leo-pony,2024-11-08T09:29:56Z,2024-11-22T06:07:21+00:00,7,332.62
10217,CANN: Add Ascend CANN build ci,xuedinge233,2024-11-08T09:41:32Z,2025-01-24T23:26:01+00:00,20,1861.74
10218,metal : opt-in compile flag for BF16,ggerganov,2024-11-08T09:48:36Z,2024-11-08T19:59:46+00:00,1,10.19
10221,scripts: fix pattern and get n_tokens in one go,lhpqaq,2024-11-08T14:01:53Z,2024-11-09T07:06:54+00:00,1,17.08
10222,vulkan: Throttle the number of shader compiles during the build step,jeffbolznv,2024-11-08T20:35:22Z,2024-11-11T17:13:51+00:00,3,68.64
10224,support for llguidance grammars,mmoskal,2024-11-09T01:53:40Z,2025-02-02T07:55:32+00:00,5,2046.03
10226,vulkan: Fix newly added tests for permuted mul_mat and 1D im2col,jeffbolznv,2024-11-09T03:05:48Z,2024-11-10T11:37:57+00:00,5,32.54
10233,server : enable KV cache defrag by default,ggerganov,2024-11-09T10:22:56Z,2024-11-11T06:38:44+00:00,1,44.26
10238,metal : refactor kernel args into structs,ggerganov,2024-11-09T14:31:15Z,2024-11-17T09:23:02+00:00,5,186.86
10239,server : (web UI) Add back sampler settings,MaggotHATE,2024-11-09T16:00:22Z,2024-11-10T19:42:25+00:00,1,27.7
10242,"server : (web UI) add copy button for code block, fix api key",ngxson,2024-11-09T22:44:17Z,2024-11-15T09:48:49+00:00,2,131.08
10243,nix: update flake.lock,ggerganov,2024-11-10T00:22:23Z,2024-11-10T19:45:25+00:00,1,19.38
10247,metal : more precise Q*K in FA vec kernel,ggerganov,2024-11-10T14:55:32Z,2024-11-11T06:39:13+00:00,1,15.73
10254,vulkan: Optimize contiguous copies,jeffbolznv,2024-11-11T19:10:19Z,2024-11-13T06:58:57+00:00,2,35.81
10255,server: (web UI) Add samplers sequence customization,MaggotHATE,2024-11-11T19:45:27Z,2024-11-16T13:26:54+00:00,5,113.69
10256,ggml : build backends as libraries,slaren,2024-11-11T20:23:46Z,2024-11-14T17:04:35+00:00,5,68.68
10257,sycl : Fixes broken build and test-backend-ops,Alcpz,2024-11-11T21:40:10Z,2024-11-13T09:40:57+00:00,2,36.01
10259,vulkan: Use macros to make the mat mul pipeline creation more concise,jeffbolznv,2024-11-12T03:52:43Z,2024-11-13T20:59:47+00:00,1,41.12
10261,docs: update README.md,xuegao-tzx,2024-11-12T09:19:37Z,2024-11-13T11:17:10+00:00,1,25.96
10266,sycl: Add option to set the SYCL architecture for all targets,Rbiessy,2024-11-12T14:40:57Z,2024-11-19T08:02:24+00:00,11,161.36
10267,sycl: Use syclcompat::dp4a,Rbiessy,2024-11-12T14:41:35Z,2024-11-15T03:09:12+00:00,8,60.46
10269,server : add missing docs,z80maniac,2024-11-12T17:55:51Z,2024-11-13T11:16:31+00:00,1,17.34
10270,vulkan: optimize add/mul/div,jeffbolznv,2024-11-12T18:08:08Z,2024-11-14T05:22:55+00:00,1,35.25
10271,"readme : add option, update default value, fix formatting",pothitos,2024-11-12T23:17:36Z,2024-12-03T10:50:08+00:00,1,491.54
10272,server : fix incorrect res in validate_model_chat_template,jhen0409,2024-11-13T02:38:33Z,2024-11-13T11:15:23+00:00,1,8.61
10279,metal : fix build and swift package,ggerganov,2024-11-13T12:55:14Z,2024-11-13T13:57:24+00:00,1,1.04
10286,"save number of parameters and the size in llama_model, fixes #10285",FirstTimeEZ,2024-11-14T07:41:04Z,2024-11-16T00:42:13+00:00,3,41.02
10290,speculative : experiments with Qwen2.5-Coder,ggerganov,2024-11-14T09:42:02Z,2025-07-02T18:52:11+00:00,1,5529.17
10291,Introduce llama-run,ericcurtin,2024-11-14T11:33:23Z,2024-11-25T21:56:24+00:00,5,274.38
10296,vulkan: Optimize some mat-vec mul quant shaders,jeffbolznv,2024-11-14T18:17:48Z,2024-11-16T06:26:57+00:00,1,36.15
10298,ci: build test musa with cmake,yeahdongcn,2024-11-15T00:52:57Z,2024-11-15T11:47:25+00:00,1,10.91
10301,vulkan: Optimize soft_max,jeffbolznv,2024-11-15T02:36:56Z,2024-11-19T07:25:18+00:00,3,100.81
10302,[CANN] dockerfile and doc adjustment,noemotiovon,2024-11-15T03:49:50Z,2024-11-15T07:09:35+00:00,1,3.33
10303,docs: vulkan build instructions to use git bash mingw64,FirstTimeEZ,2024-11-15T06:50:31Z,2024-11-16T23:29:18+00:00,7,40.65
10306,vulkan: cmake preset debug/release,FirstTimeEZ,2024-11-15T10:08:10Z,2024-11-16T01:59:33+00:00,1,15.86
10308,Add .clang-format file,ericcurtin,2024-11-15T12:15:02Z,2024-12-09T23:46:42+00:00,4,587.53
10314,Make updates to fix issues with clang-cl builds while using AVX512 flags,Srihari-mcw,2024-11-15T16:58:37Z,2024-11-15T21:27:01+00:00,2,4.47
10318,"CUDA: remove DMMV, consolidate F16 mult mat vec",JohannesGaessler,2024-11-15T19:54:36Z,2024-11-17T08:09:55+00:00,4,36.26
10320,CMake: default to -arch=native for CUDA build,JohannesGaessler,2024-11-15T20:21:27Z,2024-11-17T08:06:34+00:00,2,35.75
10324,ggml: Optimize Q4_0 into Q4_0_X_Y repack,eddnjjn,2024-11-15T21:56:50Z,2024-11-16T00:53:37+00:00,1,2.95
10333,make : add missing rules for ggml sources,ggerganov,2024-11-16T08:59:55Z,2024-11-16T18:36:26+00:00,2,9.61
10339,llama/ex: remove --logdir argument,JohannesGaessler,2024-11-16T16:39:08Z,2024-11-16T22:00:41+00:00,1,5.36
10346,nix: update flake.lock,ggerganov,2024-11-17T00:23:34Z,2024-11-18T14:08:20+00:00,1,37.75
10357,CUDA: fix MMV kernel being used for FP16 src1,JohannesGaessler,2024-11-17T10:12:23Z,2024-11-17T22:20:42+00:00,2,12.14
10358,llama : only use default buffer types for the KV cache,slaren,2024-11-17T10:17:38Z,2024-11-17T11:25:45+00:00,1,1.14
10361,Add support for Qwen2VL,HimariO,2024-11-17T12:08:31Z,2024-12-14T12:43:47+00:00,10,648.59
10366,Vulkan: Fix device info output format specifiers,0cc4m,2024-11-17T19:55:32Z,2024-11-18T10:02:43+00:00,3,14.12
10372,vulkan: remove use of null initializer,jeffbolznv,2024-11-18T03:32:22Z,2024-11-18T14:28:42+00:00,1,10.94
10382,server: Fix the status of finish_reason if the stream value is False,SeongBeomLEE,2024-11-18T11:35:44Z,2024-12-06T10:02:36+00:00,2,430.45
10383,cmake: fix issue with library path during cross-compile,bandoti,2024-11-18T13:12:38Z,2024-11-18T15:23:59+00:00,1,2.19
10385,sycl: Revert MUL_MAT_OP support changes,Alcpz,2024-11-18T14:38:17Z,2024-11-19T00:50:04+00:00,1,10.2
10387,vulkan: further optimize mul_mat_vec using larger loads,jeffbolznv,2024-11-18T15:43:06Z,2024-11-20T07:11:00+00:00,1,39.47
10394,Add OLMo November 2024 model,2015aroras,2024-11-18T19:13:58Z,2024-11-19T09:04:08+00:00,3,13.84
10395,Update recommended release version to 4040,NeoZhangJianyu,2024-11-19T01:06:27Z,2024-11-20T05:54:25+00:00,1,28.8
10401,llama : add check for KV cache shifts,ggerganov,2024-11-19T10:02:05Z,2024-11-19T11:29:27+00:00,3,1.46
10403,cuda : fix CUDA_FLAGS not being applied,slaren,2024-11-19T12:27:58Z,2024-11-19T13:29:39+00:00,1,1.03
10407,cmake: add required ggml-base and backend libs to cmake pkg,bandoti,2024-11-19T14:52:24Z,2024-11-19T16:10:30+00:00,1,1.3
10408,sycl : permuted mul_mat through oneMKL,Alcpz,2024-11-19T14:56:26Z,2024-11-29T09:49:43+00:00,2,234.89
10409,vulkan: copy iq4_nl LUT into shared memory,jeffbolznv,2024-11-19T15:05:43Z,2024-11-20T07:40:18+00:00,1,16.58
10411,cmake: Add RISC-V compiler support,lhpqaq,2024-11-19T16:06:29Z,2024-11-19T20:10:31+00:00,1,4.07
10413,Fix missing file renames in Makefile due to changes in commit ae8de6d50a,avdg,2024-11-19T21:13:49Z,2024-11-19T22:18:17+00:00,1,1.07
10415,llama : add .clang-format file,slaren,2024-11-19T22:16:56Z,2024-11-20T11:57:53+00:00,4,13.68
10416,server : replace behave with pytest,ngxson,2024-11-19T22:40:29Z,2024-11-26T15:20:19+00:00,3,160.66
10419,bug-fix: snprintf prints NULL in place of the last character,kallewoof,2024-11-20T06:25:28Z,2024-12-11T13:48:04+00:00,1,511.38
10424,allocate c strings in metadata functions,kallewoof,2024-11-20T10:25:24Z,2024-11-20T13:27:59+00:00,1,3.04
10425,server: (web UI) Add custom chat formatting that uses `input_prefix` and `input_suffix`,MaggotHATE,2024-11-20T10:36:13Z,2024-11-24T13:32:49+00:00,11,98.94
10426,GitHub: ask for more info in issue templates,JohannesGaessler,2024-11-20T11:09:54Z,2024-11-22T07:32:40+00:00,5,44.38
10428,ci: Update oneAPI runtime dll packaging,MakeDecisionWorth,2024-11-20T12:10:22Z,2024-11-22T09:44:08+00:00,1,45.56
10432,sycl : offload of get_rows set to false,Alcpz,2024-11-20T14:18:26Z,2024-11-29T12:38:45+00:00,2,214.34
10440,vulkan: define all quant data structures in types.comp,jeffbolznv,2024-11-20T20:22:47Z,2024-11-27T07:32:54+00:00,1,155.17
10441,cuda : optimize argmax,slaren,2024-11-21T00:12:24Z,2024-11-21T17:18:50+00:00,13,17.11
10446,Refactor/online repacking,Djip007,2024-11-21T20:46:00Z,2024-12-07T12:37:50+00:00,26,375.86
10448,fix: ggml: fix vulkan-shaders-gen build,sparkleholic,2024-11-22T02:42:39Z,2025-01-15T13:17:42+00:00,22,1306.58
10452,llava: return false instead of exit,tinglou,2024-11-22T07:52:43Z,2024-11-27T12:36:39+00:00,1,124.73
10454,[CANN] Improve the Inferencing Performance for Ascend NPU Device,shen-shanshan,2024-11-22T10:08:25Z,2024-11-26T10:08:37+00:00,6,96.0
10456,"ci : add ubuntu cuda build, build with one arch on windows",slaren,2024-11-22T13:44:57Z,2024-11-26T12:05:07+00:00,1,94.34
10457,ggml : do not use ARM features not included in the build,slaren,2024-11-22T19:24:54Z,2024-11-23T13:41:13+00:00,1,18.27
10458,XLMRoberta Type Vocab Size,gabe-l-hart,2024-11-22T23:33:04Z,2024-11-24T09:02:34+00:00,1,33.49
10459,vulkan: optimize Q2_K and Q3_K mul_mat_vec,jeffbolznv,2024-11-23T02:14:25Z,2024-11-27T07:00:50+00:00,1,100.77
10468,vulkan: Handle GPUs with less shared memory,jeffbolznv,2024-11-23T22:27:48Z,2024-11-27T07:30:27+00:00,1,81.04
10469,ggml : add support for dynamic loading of backends,slaren,2024-11-23T23:08:37Z,2024-11-25T14:13:40+00:00,3,39.08
10470,nix: update flake.lock,ggerganov,2024-11-24T00:24:17Z,2024-11-24T16:03:25+00:00,1,15.65
10474,cmake : enable warnings in llama,ggerganov,2024-11-24T13:02:56Z,2024-11-26T12:18:08+00:00,3,47.25
10479,vulkan: further optimize q5_k mul_mat_vec,jeffbolznv,2024-11-25T04:04:09Z,2024-11-27T07:21:59+00:00,1,51.3
10481,Add download chat feature to server chat,brucepro,2024-11-25T06:45:12Z,2024-11-25T16:11:55+00:00,1,9.45
10483,[SYCL] Fix building Win package for oneAPI 2025.0 update,NeoZhangJianyu,2024-11-25T07:29:31Z,2024-11-25T09:31:10+00:00,2,2.03
10484,fix: Fix a vulkan-shaders-gen arugment parsing error,sparkleholic,2024-11-25T08:38:56Z,2024-11-26T01:47:20+00:00,1,17.14
10487,ggml-cpu: cmake add arm64 cpu feature check for macos,chaxu01,2024-11-25T11:30:14Z,2024-11-26T11:37:06+00:00,4,24.11
10488,[CANN] RoPE and CANCAT operator optimization,noemotiovon,2024-11-25T12:09:53Z,2024-11-26T09:31:05+00:00,2,21.35
10489,Github: update issue templates,JohannesGaessler,2024-11-25T12:43:28Z,2024-11-25T18:18:37+00:00,2,5.59
10492,imatrix-combine-only idea,robbiemu,2024-11-25T14:32:06Z,2024-11-29T17:21:37+00:00,1,98.83
10496,vulkan: fix group_norm,jeffbolznv,2024-11-25T15:22:46Z,2024-11-26T15:45:06+00:00,1,24.37
10497,llama : accept a list of devices to use to offload a model,slaren,2024-11-25T16:14:43Z,2024-11-25T18:30:06+00:00,1,2.26
10498,Add some minimal optimizations for CDNA,IMbackK,2024-11-25T16:41:50Z,2024-11-27T16:10:08+00:00,4,47.47
10499,vulkan: get the first command buffer submitted sooner,jeffbolznv,2024-11-25T16:54:05Z,2024-11-29T06:18:02+00:00,1,85.4
10500,Rename Olmo1124 to Olmo2,2015aroras,2024-11-25T16:59:41Z,2024-11-25T18:36:10+00:00,2,1.61
10506,vulkan: skip integer div/mod in get_offsets for batch_idx==0,jeffbolznv,2024-11-26T00:43:55Z,2024-11-27T07:08:54+00:00,1,30.42
10507,[SYCL] restore the condition to build & update package when execute merge,NeoZhangJianyu,2024-11-26T03:03:20Z,2024-11-26T13:43:48+00:00,2,10.67
10508,Opt class for positional argument handling,ericcurtin,2024-11-26T04:40:07Z,2024-12-13T18:34:25+00:00,17,421.9
10514,make : deprecate,ggerganov,2024-11-26T10:50:23Z,2024-12-02T19:22:53+00:00,2,152.54
10516,mtgpu: Add MUSA_DOCKER_ARCH in Dockerfiles && update cmake and make,yeahdongcn,2024-11-26T13:12:54Z,2024-11-26T16:00:41+00:00,1,2.8
10519,CANN: cann backend build failed when manually specify SOC_TYPE or gcc version that isn't verified,leo-pony,2024-11-26T14:08:05Z,2024-11-28T07:25:24+00:00,1,41.29
10524,Fix HIP flag inconsistency & build docs,tristandruyen,2024-11-26T16:25:14Z,2024-11-26T18:27:29+00:00,1,2.04
10526,ci : remove nix workflows,slaren,2024-11-26T18:45:53Z,2024-11-26T20:13:54+00:00,2,1.47
10536,vulkan: Dynamic subgroup size support for Q6_K mat_vec,netrunnereve,2024-11-27T03:02:41Z,2024-11-30T07:00:02+00:00,8,75.96
10537,ci : faster CUDA toolkit installation method and use ccache,slaren,2024-11-27T03:25:46Z,2024-11-27T10:03:25+00:00,1,6.63
10538,Update cann.md to ensure it displays correctly on all platforms.,HRXWEB,2024-11-27T09:12:50Z,2024-11-28T07:27:12+00:00,1,22.24
10540,[CANN] ROPE operator optimization,noemotiovon,2024-11-27T09:58:16Z,2024-11-28T06:24:47+00:00,1,20.44
10541,ggml-cpu: support IQ4_NL_4_4 by runtime repack,FanShupei,2024-11-27T10:01:50Z,2024-11-28T12:52:03+00:00,9,26.84
10542,kompute: improve backend to pass test_backend_ops,slp,2024-11-27T11:11:04Z,2024-11-28T11:51:38+00:00,1,24.68
10543,cmake : fix ARM feature detection,ggerganov,2024-11-27T11:18:46Z,2024-11-28T12:56:23+00:00,1,25.63
10544,llama/ggml: add LLM training support,JohannesGaessler,2024-11-27T11:32:50Z,2025-05-12T12:44:49+00:00,1,3985.2
10546,llava: return false instead of exit,tinglou,2024-11-27T12:37:59Z,2024-11-29T00:09:46+00:00,1,35.53
10548,"server: Add ""tokens per second"" information in the backend",lhpqaq,2024-11-27T15:36:41Z,2024-12-02T13:45:54+00:00,6,118.15
10550,common : fix duplicated file name with hf_repo and hf_file,ngxson,2024-11-27T18:34:10Z,2024-11-27T21:30:52+00:00,2,2.94
10558,Faster ssm scan,A3shTnT,2024-11-28T05:55:10Z,2025-03-31T16:05:13+00:00,2,2962.17
10559,llama: Support MiniCPM-1B (with & w/o longrope),JFLFY2255,2024-11-28T06:21:21Z,2024-12-04T09:42:50+00:00,2,147.36
10561,ggml : fix row condition for i8mm kernels,ggerganov,2024-11-28T10:25:59Z,2024-11-28T12:56:37+00:00,1,2.51
10562,ggml : fix I8MM Q4_1 scaling factor conversion,ggerganov,2024-11-28T11:13:47Z,2024-11-29T14:25:39+00:00,4,27.2
10563,[cann] RoPE operator optimization,noemotiovon,2024-11-28T11:16:54Z,2024-11-29T06:46:55+00:00,1,19.5
10564,common: fix warning message when no GPU found,JohannesGaessler,2024-11-28T13:06:58Z,2024-11-28T17:15:25+00:00,1,4.14
10567,ggml-cpu: replace AArch64 NEON assembly with intrinsics in ggml_gemv_q4_0_4x4_q8_0(),angt,2024-11-28T14:42:39Z,2024-11-30T17:13:18+00:00,2,50.51
10568,"server : (tests) don't use thread for capturing stdout/stderr, bump openai client library",ngxson,2024-11-28T15:48:23Z,2024-11-28T18:17:49+00:00,1,2.49
10569,server : add more test cases,ngxson,2024-11-28T16:10:25Z,2024-11-29T20:48:56+00:00,1,28.64
10570,ggml : move AMX to the CPU backend,slaren,2024-11-28T17:09:18Z,2024-11-29T20:54:58+00:00,12,27.76
10572,"Add `mistral-v1`, `mistral-v3`, `mistral-v3-tekken` and `mistral-v7` chat template types",jukofyork,2024-11-29T01:04:19Z,2024-12-01T22:09:49+00:00,1,69.09
10573,Add support for GLM-Edge and GLM-Edge-V series models,piDack,2024-11-29T05:53:51Z,2025-02-02T07:48:46+00:00,21,1561.92
10574,clip add sycl support,piDack,2024-11-29T09:09:45Z,2024-12-04T00:26:37+00:00,1,111.28
10577,cleanup UI link list,slaren,2024-11-29T12:13:56Z,2024-11-29T16:45:09+00:00,3,4.52
10579,SYCL: Fix and switch to GGML_LOG system instead of fprintf,qnixsynapse,2024-11-29T12:45:37Z,2024-12-02T07:04:11+00:00,2,66.31
10584,SYCL : Move to compile time oneMKL interface backend selection for NVIDIA backend,s-Nick,2024-11-29T16:29:53Z,2024-12-04T01:29:20+00:00,7,104.99
10587,readme : refresh,ggerganov,2024-11-29T20:09:49Z,2024-11-30T07:47:07+00:00,5,11.62
10590,server : bind to any port when specified,alek3y,2024-11-29T23:03:00Z,2024-12-01T11:33:12+00:00,1,36.5
10593,contrib : refresh,ggerganov,2024-11-30T07:53:28Z,2024-12-02T06:53:27+00:00,1,47.0
10597,Vulkan: VK_KHR_cooperative_matrix support to speed up prompt processing,0cc4m,2024-11-30T09:52:13Z,2024-12-07T09:24:15+00:00,23,167.53
10598,build: update Makefile comments for C++ version change,wangqin0,2024-11-30T11:53:42Z,2024-12-01T03:19:44+00:00,1,15.43
10599,"server : (web ui) Various improvements, now use vite as bundler",ngxson,2024-11-30T12:09:50Z,2024-12-03T18:38:44+00:00,3,78.48
10600,Update deprecation-warning.cpp,aryantandon01,2024-11-30T12:10:01Z,2024-12-02T04:12:14+00:00,2,40.04
10605,fix typo of README.md,WrRan,2024-11-30T19:00:22Z,2024-12-04T01:22:50+00:00,1,78.37
10606,ggml : automatic selection of best CPU backend,slaren,2024-11-30T19:41:14Z,2024-12-01T15:12:42+00:00,1,19.52
10608,ci: add error handling for Python venv creation in run.sh,wangqin0,2024-12-01T00:12:08Z,2024-12-01T18:11:43+00:00,1,17.99
10616,"Avoid using __fp16 on ARM with old nvcc, fixes #10555",frankier,2024-12-01T17:07:59Z,2024-12-04T00:41:37+00:00,1,55.56
10619,Update deprecation-warning.cpp,aryantandon01,2024-12-02T04:36:31Z,2024-12-04T22:19:20+00:00,1,65.71
10623,llama : add enum for built-in chat templates,ngxson,2024-12-02T11:34:52Z,2024-12-02T21:10:20+00:00,2,9.59
10626,ggml : add predefined list of CPU backend variants to build,slaren,2024-12-02T16:18:49Z,2024-12-04T13:45:41+00:00,1,45.45
10627,server: add OpenAI compatible response format for /completions,Nero7991,2024-12-02T20:32:16Z,2024-12-04T00:02:57+00:00,2,27.51
10630,Add docs for creating a static build (#10268),mostlygeek,2024-12-02T23:54:57Z,2024-12-04T00:40:37+00:00,3,24.76
10637,vulkan: optimize and reenable split_k,jeffbolznv,2024-12-03T14:52:40Z,2024-12-03T19:29:54+00:00,1,4.62
10641,server : fix speculative decoding with context shift,ggerganov,2024-12-03T20:45:24Z,2024-12-04T20:38:20+00:00,1,23.88
10642,"vulkan: Implement ""fast divide"" (mul+shift) for unary ops like copy",jeffbolznv,2024-12-03T22:21:50Z,2024-12-04T07:28:59+00:00,1,9.12
10643,server : (refactoring) do not rely on JSON internally,ngxson,2024-12-03T22:50:04Z,2024-12-06T10:14:32+00:00,3,59.41
10646,ggml-cpu : fix HWCAP2_I8MM value,slaren,2024-12-04T00:40:17Z,2024-12-04T13:40:44+00:00,1,13.01
10655,"GGUF: backend support, fixed-width I/O, misc fixes",JohannesGaessler,2024-12-04T13:02:26Z,2024-12-04T16:54:14+00:00,9,3.86
10663,Make->CMake,jboero,2024-12-04T19:16:35Z,2025-02-14T05:23:44+00:00,5,1714.12
10665,vulkan: small mul_mat_vec optimizations,netrunnereve,2024-12-04T22:52:56Z,2024-12-13T08:42:05+00:00,4,201.82
10667,py : update outdated copy-paste instructions [no ci],danbev,2024-12-05T04:18:33Z,2024-12-05T07:47:55+00:00,1,3.49
10668,Changes to CMakePresets.json to add ninja clang target on windows,Srihari-mcw,2024-12-05T06:26:41Z,2024-12-09T17:40:19+00:00,3,107.23
10669,Support for Llama-3_1-Nemotron-51B,ymcki,2024-12-05T06:57:56Z,2024-12-23T00:22:33+00:00,3,425.41
10672,cmake : simplify msvc charsets,iboB,2024-12-05T14:26:26Z,2024-12-09T07:15:13+00:00,1,88.81
10673,Add Minerva 7B model support,Riccorl,2024-12-05T15:05:34Z,2024-12-05T18:30:59+00:00,1,3.42
10674,fix(server) : handling DONE message in simple chat,pminev,2024-12-05T15:29:44Z,2024-12-05T21:36:41+00:00,1,6.12
10676,metal : Extend how Llama.cpp locates metal resources (#10675),ormandi,2024-12-05T15:58:37Z,2024-12-07T07:55:01+00:00,1,39.94
10686,common : bring back --no-warmup to server,ngxson,2024-12-06T10:20:36Z,2024-12-06T12:29:05+00:00,1,2.14
10691,server : (refactor) no more json in server_task input,ngxson,2024-12-06T14:12:59Z,2024-12-07T19:21:09+00:00,5,29.14
10693,Introducing experimental OpenCL backend with support for Qualcomm Adreno GPUs,lhez,2024-12-06T19:27:36Z,2024-12-13T20:23:52+00:00,20,168.94
10695,feat: add support for Roberta embeddings,Ssukriti,2024-12-06T21:47:22Z,2024-12-07T07:02:14+00:00,1,9.25
10698,add 128k yarn context for Qwen,robbiemu,2024-12-06T23:41:30Z,2024-12-07T21:12:27+00:00,6,21.52
10699,ggml: load all backends from a user-provided search path,giladgd,2024-12-07T00:09:17Z,2024-12-11T00:47:22+00:00,4,96.63
10704,server : various fixes,ggerganov,2024-12-07T10:09:27Z,2024-12-07T16:02:05+00:00,5,5.88
10713,vulkan: compile a test shader in cmake to check for coopmat2 support,jeffbolznv,2024-12-08T01:34:22Z,2024-12-08T08:05:55+00:00,1,6.53
10714,more perfo with llamafile tinyblas on x86_64.,Djip007,2024-12-08T03:19:15Z,2024-12-24T17:54:49+00:00,30,398.59
10721,Vulkan: Add VK_EXT_subgroup_size_control support to ensure full subgroups for coopmats,0cc4m,2024-12-08T14:48:02Z,2024-12-12T17:35:37+00:00,3,98.79
10722,server : bring back info of final chunk in stream mode,ngxson,2024-12-08T16:26:33Z,2024-12-08T19:38:52+00:00,1,3.21
10723,Vulkan: fix NaN in tanh.comp with AMD proprietary driver on Windows,stduhpf,2024-12-08T16:46:01Z,2024-12-08T18:19:19+00:00,1,1.55
10724,server : fix format_infill,ngxson,2024-12-08T20:10:30Z,2024-12-08T22:04:29+00:00,4,1.9
10731,vulkan: fix compile warnings,jeffbolznv,2024-12-09T04:19:49Z,2024-12-09T07:24:01+00:00,1,3.07
10736,Renames NVIDIA GPU-architecture flags to avoid name clashes with WinAPI,aendk,2024-12-09T10:39:32Z,2024-12-10T17:23:24+00:00,6,30.73
10745,vulkan: dynamic subgroup size for the remaining k quants,netrunnereve,2024-12-10T03:08:42Z,2024-12-10T19:33:24+00:00,2,16.41
10748,SYCL: Reduce most of the compiler warnings,qnixsynapse,2024-12-10T08:11:16Z,2024-12-13T06:42:15+00:00,24,70.52
10751,server: add flag to disable the web ui,eugeniosegala,2024-12-10T10:42:31Z,2024-12-10T17:22:34+00:00,2,6.67
10752,ggml: GGML_NATIVE uses -mcpu=native on ARM,angt,2024-12-10T11:23:05Z,2024-12-18T18:44:57+00:00,10,199.36
10760,[backend](cuda): faster uncontiguous concat,A3shTnT,2024-12-10T14:15:14Z,2024-12-12T18:09:51+00:00,7,51.91
10763,vulkan: disable spirv-opt for coopmat shaders,jeffbolznv,2024-12-10T14:46:14Z,2024-12-10T17:22:20+00:00,1,2.6
10765,server: bench: minor fixes,phymbert,2024-12-10T16:23:27Z,2025-01-02T17:06:12+00:00,1,552.71
10767,vulkan: request round-to-even for fp16 in im2col/rope_head,jeffbolznv,2024-12-10T18:01:10Z,2024-12-10T20:23:18+00:00,3,2.37
10771,Removes spurious \r in output that causes logging in journalctl to tr…,cduk,2024-12-10T21:49:40Z,2024-12-13T22:21:49+00:00,1,72.54
10772,Fix a small typo in the Quantization Docs,qingy1337,2024-12-11T01:42:46Z,2024-12-11T15:16:33+00:00,1,13.56
10776,docs: fix server documentation formatting,CentricStorm,2024-12-11T05:27:42Z,2024-12-11T10:47:43+00:00,1,5.33
10779,ci : pin nodejs to 22.11.0,ngxson,2024-12-11T07:56:16Z,2024-12-11T13:59:41+00:00,1,6.06
10781,musa: fix aarch64 build,BodhiHu,2024-12-11T12:00:07Z,2024-12-23T09:05:02+00:00,1,285.08
10783,"server : fix logprobs, make it OAI-compatible",ngxson,2024-12-11T13:52:23Z,2024-12-19T14:40:09+00:00,25,192.8
10784,tts : add OuteTTS support,ggerganov,2024-12-11T16:52:21Z,2024-12-18T17:27:21+00:00,1,168.58
10786,"server : (UI) add tok/s, get rid of completion.js",ngxson,2024-12-11T18:38:38Z,2024-12-11T19:52:14+00:00,1,1.23
10794,nix: allow to override rocm gpu targets,kurnevsky,2024-12-12T09:52:51Z,2024-12-14T18:17:37+00:00,1,56.41
10797,remove CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS,slaren,2024-12-12T13:24:08Z,2024-12-12T18:02:49+00:00,1,4.64
10798,Vulkan: Use improved q4_k and q5_k dequant code in dequant shaders,0cc4m,2024-12-12T13:38:05Z,2024-12-12T17:36:00+00:00,1,3.97
10802,Documentation: Add ggml_type value choices for KV cache data type in …,MichelleTanPY,2024-12-12T17:32:01Z,2024-12-12T20:22:07+00:00,4,2.83
10803,sampling : refactor + optimize penalties sampler,ggerganov,2024-12-12T18:54:12Z,2024-12-16T10:31:15+00:00,7,87.62
10806,common : improve -ctv -ctk CLI arguments,ngxson,2024-12-12T20:20:27Z,2024-12-12T21:53:05+00:00,1,1.54
10808,server: (UI) add syntax highlighting and latex math rendering,VJHack,2024-12-12T20:35:25Z,2024-12-15T11:55:55+00:00,10,63.34
10809,vulkan: bugfixes for small subgroup size systems + llvmpipe test,netrunnereve,2024-12-12T21:29:03Z,2024-12-17T05:52:55+00:00,12,104.4
10811,ggml: Fix compilation issues on ARM platform when building without fp16,kkontny,2024-12-12T22:56:57Z,2024-12-13T00:04:19+00:00,1,1.12
10812,Fix crash caused by ggml_backend_load_all when launching on Android Activity,sienaiwun,2024-12-12T23:57:40Z,2024-12-13T12:56:08+00:00,2,12.97
10815,fix: graceful shutdown for Docker images,co42,2024-12-13T15:44:46Z,2024-12-13T17:23:50+00:00,1,1.65
10817,Add support for Microsoft Phi-4 model ,fairydreaming,2024-12-13T19:03:22Z,2024-12-19T09:37:13+00:00,8,134.56
10818,server: Fix `has_next_line` in JSON response,MichelleTanPY,2024-12-13T19:28:33Z,2024-12-14T22:29:46+00:00,3,27.02
10821,Improve progress bar,ericcurtin,2024-12-13T22:52:03Z,2024-12-19T02:58:00+00:00,14,124.1
10825,add `ggml_backend_sched_dump_dot`,foldl,2024-12-14T10:27:07Z,2025-02-24T23:27:06+00:00,7,1741.0
10827,Add Deepseek MoE v1 & GigaChat models,Inf1delis,2024-12-14T13:18:56Z,2024-12-15T17:02:46+00:00,2,27.73
10829,rwkv6: add wkv6 support for Vulkan backend,zhiyuan1i,2024-12-14T14:15:54Z,2024-12-16T21:00:47+00:00,7,54.75
10830,tests: add tests for GGUF,JohannesGaessler,2024-12-14T15:19:41Z,2024-12-17T18:09:35+00:00,2,74.83
10832,added docker-multi-stage builds,rudiservo,2024-12-14T20:38:59Z,2024-12-22T22:22:58+00:00,10,193.73
10833,Allow locally downloaded models for QwenVL,bartowski1182,2024-12-14T20:55:18Z,2024-12-15T20:43:25+00:00,3,23.8
10836,"scripts : change build path to ""build-bench"" for compare-commits.sh",ggerganov,2024-12-15T09:51:58Z,2024-12-15T16:44:48+00:00,1,6.88
10840,SYCL: Migrate away from deprecated ggml_tensor->backend,qnixsynapse,2024-12-15T14:09:40Z,2024-12-20T15:31:29+00:00,10,121.36
10846,vulkan: multi-row k quants,netrunnereve,2024-12-16T03:59:47Z,2024-12-26T15:54:44+00:00,5,251.92
10851,SYCL: Fixes for building SYCL backend for AMD GPUs,lhl,2024-12-16T07:52:54Z,2025-04-01T08:50:45+00:00,7,2544.96
10852,server : fill usage info in embeddings and rerank responses,krystiancha,2024-12-16T14:24:30Z,2024-12-17T16:00:25+00:00,3,25.6
10853,"server : add ""tokens"" output",ggerganov,2024-12-16T19:06:30Z,2024-12-18T09:05:29+00:00,7,37.98
10855,vulkan: optimize coopmat2 dequant functions,jeffbolznv,2024-12-16T22:28:50Z,2024-12-21T07:04:45+00:00,1,104.6
10856,Roberta embeddings fixes,Ssukriti,2024-12-16T22:51:58Z,2024-12-19T13:04:51+00:00,3,62.21
10857,server : (UI) fix missing async generator on safari,ngxson,2024-12-16T23:40:11Z,2024-12-17T08:52:09+00:00,2,9.2
10859,server: avoid overwriting Authorization header,vesath,2024-12-17T07:32:40Z,2024-12-17T17:22:19+00:00,2,9.83
10861,server : output embeddings for all tokens when pooling = none,ggerganov,2024-12-17T09:10:04Z,2024-12-18T11:01:41+00:00,7,25.86
10864,Add Falcon3 model support,mokeddembillel,2024-12-17T09:49:41Z,2024-12-17T15:24:56+00:00,1,5.59
10866,server : add bad input handling in embeddings,krystiancha,2024-12-17T12:17:27Z,2024-12-17T20:53:42+00:00,5,8.6
10868,Use model->gguf_kv for loading the template instead of using the C API.,dranger003,2024-12-17T16:06:05Z,2024-12-17T22:24:22+00:00,1,6.3
10872,"server : (embeddings) using same format for ""input"" and ""content""",ngxson,2024-12-17T20:37:09Z,2024-12-18T08:55:09+00:00,2,12.3
10874,ggml-cpu: replace NEON asm with intrinsics in ggml_gemv_q4_0_4x8_q8_0(),angt,2024-12-17T20:47:40Z,2024-12-20T23:33:38+00:00,1,74.77
10876,"Revert ""Add Falcon3 model support""",slaren,2024-12-17T22:31:10Z,2024-12-18T00:36:47+00:00,1,2.09
10878,server: avoid overwriting Authorization header,vesath,2024-12-18T02:11:50Z,2024-12-18T14:00:07+00:00,1,11.8
10880,docs: Fix HIP (née hipBLAS) in README,brianredbeard,2024-12-18T04:51:08Z,2024-12-18T08:35:00+00:00,1,3.73
10883,Add Falcon3 support and Fix issue #10875,mokeddembillel,2024-12-18T05:31:02Z,2024-12-22T22:09:58+00:00,11,112.65
10886,tests: disable GGUF test for bad value size,JohannesGaessler,2024-12-18T09:29:54Z,2024-12-19T07:53:58+00:00,1,22.4
10889,ggml-cpu: re-add AArch64 NEON assembly for ggml_gemv_q4_0_4x4_q8_0() for non-dotprod,smpurkis,2024-12-18T17:20:27Z,2024-12-18T18:38:01+00:00,2,1.29
10890,ggml : fix arm build,slaren,2024-12-18T18:42:11Z,2024-12-18T22:21:42+00:00,2,3.66
10893,Support InfiniAI Megrez 3b,dixyes,2024-12-19T09:06:44Z,2024-12-23T00:35:44+00:00,4,87.48
10895,ggml: fix arm build with gcc,angt,2024-12-19T11:39:20Z,2024-12-19T13:20:42+00:00,1,1.69
10899,Update llama-run to include temperature option,ericcurtin,2024-12-19T14:00:35Z,2024-12-23T00:21:40+00:00,5,82.35
10900,llama : add support for Cohere2ForCausalLM,dranger003,2024-12-19T14:50:35Z,2025-01-04T14:33:32+00:00,8,383.72
10902,llama : refactor `src/llama.cpp`,ggerganov,2024-12-19T14:54:36Z,2025-01-03T08:18:53+00:00,3,353.4
10906,ggml : add test for SVE and disable when it fails,slaren,2024-12-19T20:19:41Z,2024-12-20T12:31:28+00:00,1,16.2
10912,llamafile_sgemm API - INT8 implementation,amritahs-ibm,2024-12-20T05:33:43Z,2025-01-08T10:54:19+00:00,27,461.34
10913,Fix RWKV v6 model conversion,MollySophia,2024-12-20T07:10:39Z,2024-12-20T09:44:59+00:00,1,2.57
10916,server : (UI) fix copy to clipboard function,ngxson,2024-12-20T10:58:29Z,2024-12-20T13:12:06+00:00,1,2.23
10917,server : add system_fingerprint to chat/completion,ngxson,2024-12-20T11:46:37Z,2024-12-23T11:02:44+00:00,1,71.27
10927,vulkan: build fixes for 32b,jeffbolznv,2024-12-21T05:21:31Z,2024-12-22T09:44:01+00:00,1,28.38
10934,rpc-server : add support for the SYCL backend,rgerganov,2024-12-21T09:20:05Z,2024-12-23T08:39:30+00:00,1,47.32
10935,server : set default top-k to 1 in the web ui,ggerganov,2024-12-21T10:22:36Z,2024-12-21T14:51:19+00:00,1,4.48
10940,server:  allow filtering llama server response fields,nvrxq,2024-12-21T23:09:30Z,2024-12-24T16:39:49+00:00,8,65.51
10942,vulkan: im2col and matmul optimizations for stable diffusion,jeffbolznv,2024-12-22T04:57:14Z,2024-12-29T09:16:34+00:00,1,172.32
10957,server : fix missing model id in /model endpoint,ngxson,2024-12-23T10:50:48Z,2024-12-23T11:52:25+00:00,1,1.03
10960,ggml : use wstring for backend search paths,slaren,2024-12-23T17:16:32Z,2024-12-24T03:05:27+00:00,1,9.82
10961,ggml : fix arm enabled features check,slaren,2024-12-23T17:36:42Z,2024-12-24T03:05:17+00:00,1,9.48
10967,"server : add support for ""encoding_format"": ""base64"" to the */embeddings endpoints",elk-cloner,2024-12-24T15:06:26Z,2024-12-24T20:33:04+00:00,3,5.44
10974,server : add OAI compat for /v1/completions,ngxson,2024-12-25T12:49:09Z,2024-12-31T11:34:14+00:00,1,142.75
10979,ggml : fix undefined reference to std::filesystem(#10978),Clauszy,2024-12-26T05:25:35Z,2025-01-06T01:50:47+00:00,3,260.42
10983,"examples, ggml : fix GCC compiler warnings",peter277,2024-12-26T12:08:36Z,2024-12-26T13:59:11+00:00,1,1.84
10987,vulkan: Use push constant offset to handle misaligned descriptors,jeffbolznv,2024-12-26T17:09:57Z,2024-12-29T08:35:11+00:00,1,63.42
10989,Vulkan: Destroy Vulkan instance on exit,0cc4m,2024-12-26T19:17:03Z,2025-02-15T07:41:55+00:00,3,1212.41
10991,vulkan: optimize mul_mat for small values of N,jeffbolznv,2024-12-26T22:30:30Z,2024-12-30T17:27:11+00:00,1,90.94
10994,server : allow using LoRA adapters per-request,ngxson,2024-12-27T15:12:54Z,2025-01-02T14:05:18+00:00,1,142.87
10995,server: added more docs for response_fields field,isaac-mcfadyen,2024-12-27T15:14:37Z,2024-12-28T15:09:19+00:00,1,23.91
10997,server : fix token duplication when streaming with stop strings,z80maniac,2024-12-27T19:05:17Z,2024-12-28T15:08:54+00:00,1,20.06
11001,Add support for QRWKV6 hybrid models & slight optimization for RWKV6,MollySophia,2024-12-28T10:22:55Z,2025-01-10T01:58:08+00:00,8,303.59
11003,model: Add support for PhiMoE arch,phymbert,2024-12-28T14:46:49Z,2025-01-09T10:21:41+00:00,5,283.58
11008,Fixed Llama-3_1-Nemotron-51B doesn't work when 4K or more tokens,ymcki,2024-12-29T14:35:28Z,2024-12-31T11:04:48+00:00,2,44.49
11013,"common, examples, ggml : fix MSYS2 GCC compiler errors and warnings when building with LLAMA_CURL=ON and GGML_OPENCL=ON",peter277,2024-12-30T01:45:00Z,2024-12-31T00:46:06+00:00,1,23.02
11014,fix https://github.com/ggerganov/llama.cpp/issues/9946,ag2s20150909,2024-12-30T02:46:20Z,2024-12-30T12:35:14+00:00,1,9.81
11016,Add Jinja template support,ochafik,2024-12-30T03:48:15Z,2025-01-21T13:18:51+00:00,32,537.51
11026,server : clean up built-in template detection,ngxson,2024-12-31T11:18:47Z,2024-12-31T14:22:01+00:00,5,3.05
11027,Fixes for AVXVNNI instruction set - Clang Compiler,Srihari-mcw,2024-12-31T13:00:33Z,2024-12-31T14:23:33+00:00,2,1.38
11030,"GGUF: C++ refactor, backend support, misc fixes",JohannesGaessler,2025-01-01T18:01:31Z,2025-01-07T17:01:58+00:00,13,143.01
11032,README: Add llama-swap to infrastructure section of README,mostlygeek,2025-01-01T21:05:40Z,2025-01-02T07:14:54+00:00,1,10.15
11037,fix: Vulkan shader gen binary path,giladgd,2025-01-02T01:51:47Z,2025-01-04T08:17:31+00:00,1,54.43
11042,CUDA Graph Compute Function Refactor (precursor for performance improvements),aendk,2025-01-02T12:46:15Z,2025-01-13T15:45:53+00:00,3,266.99
11047,[GGML][RPC] Support for models with non-512-aligned tensors over RPC.,matt23654,2025-01-02T17:31:36Z,2025-01-04T16:10:30+00:00,11,46.65
11049,Add support for DeepSeek V3,fairydreaming,2025-01-02T20:10:47Z,2025-01-04T20:06:11+00:00,9,47.92
11053,Disable KV cache shifting automatically for unsupported models,MollySophia,2025-01-03T08:36:41Z,2025-01-03T12:13:18+00:00,1,3.61
11058,tokenize : escape the prompt,ggerganov,2025-01-03T10:07:19Z,2025-01-06T08:54:25+00:00,2,70.78
11060,llama : use _impl suffix instead of _internal,ggerganov,2025-01-03T11:55:32Z,2025-01-06T08:52:01+00:00,1,68.94
11061,llama : avoid hardcoded QK_K constant,ggerganov,2025-01-03T12:07:57Z,2025-01-08T14:19:36+00:00,1,122.19
11062,llama : use LLAMA_TOKEN_NULL,ggerganov,2025-01-03T12:28:17Z,2025-01-06T08:52:15+00:00,1,68.4
11063,llama : update llama_model API names,ggerganov,2025-01-03T12:43:57Z,2025-01-06T08:55:19+00:00,1,68.19
11064,llama : remove notion of CLS token,ggerganov,2025-01-03T12:54:43Z,2025-01-12T10:15:54+00:00,1,213.35
11074,Vulkan: Add device-specific blacklist for coopmat for the AMD proprietary driver,0cc4m,2025-01-04T08:52:37Z,2025-01-04T20:09:59+00:00,3,11.29
11076,mmap : fix fileno macro clash,ggerganov,2025-01-04T14:22:27Z,2025-01-06T08:52:38+00:00,1,42.5
11080,Only call rocblas_initialize for versions < 4 to eliminate unncessary VRAM allocation on some AMD cards,sARY77,2025-01-05T01:11:22Z,2025-01-28T15:42:20+00:00,2,566.52
11081,vulkan: scale caching for k quants + misc fixes,netrunnereve,2025-01-05T02:26:21Z,2025-01-15T19:50:13+00:00,16,257.4
11087,SYCL: Use get_multi_ptr instead of deprecated get_pointer in wkv6,qnixsynapse,2025-01-05T12:50:02Z,2025-01-07T06:26:07+00:00,2,41.6
11090,github : add cmd line field to bug report,ngxson,2025-01-05T15:10:11Z,2025-01-06T15:34:49+00:00,4,24.41
11093,CUDA: add BF16 support,JohannesGaessler,2025-01-05T20:21:43Z,2025-01-06T01:33:52+00:00,1,5.2
11094,llama-run : fix context size,ericcurtin,2025-01-05T23:44:15Z,2025-01-06T22:45:28+00:00,1,23.02
11096,fix: Vulkan shader gen binary path when Cross-compiling,ag2s20150909,2025-01-06T01:41:56Z,2025-01-08T08:17:30+00:00,2,54.59
11101,fix(log): prevent system info string accumulation across calls in `llama_print_system_info()`,a-ghorbani,2025-01-06T10:04:52Z,2025-01-06T11:21:47+00:00,1,1.28
11104,llama : remove check flash_attn with lora,ngxson,2025-01-06T11:29:18Z,2025-01-06T12:41:13+00:00,1,1.2
11107,rpc : code cleanup,rgerganov,2025-01-06T13:17:05Z,2025-01-07T06:37:03+00:00,1,17.33
11109,llama : remove unused headers,ggerganov,2025-01-06T13:56:23Z,2025-01-06T15:52:36+00:00,1,1.94
11110,"llama : add `llama_vocab`, functions -> methods, naming",ggerganov,2025-01-06T14:14:07Z,2025-01-12T09:32:42+00:00,7,139.31
11116,gguf-py: moved scripts directory,VJHack,2025-01-07T03:50:59Z,2025-01-08T18:54:58+00:00,1,39.07
11117,Disable GL_KHR_cooperative_matrix Vulkan extension if not available.,mbaudier,2025-01-07T05:25:46Z,2025-01-08T08:18:13+00:00,4,26.87
11121,SYCL: Refactor ggml_sycl_compute_forward,qnixsynapse,2025-01-07T12:09:02Z,2025-01-10T00:13:03+00:00,8,60.07
11130,vulkan: optimize coopmat2 q2_k dequant function,jeffbolznv,2025-01-07T20:58:00Z,2025-01-16T21:16:39+00:00,1,216.31
11131,lora : improve compat with `mergekit-extract-lora`,ngxson,2025-01-07T21:30:38Z,2025-01-08T14:59:53+00:00,4,17.49
11135,Add Fedora CUDA Guide for Development in Toolbox Environment,teihome,2025-01-08T08:05:56Z,2025-01-09T11:32:07+00:00,2,27.44
11136,arg : option to exclude arguments from specific examples,ggerganov,2025-01-08T09:26:04Z,2025-01-08T10:55:36+00:00,1,1.49
11138,Enhance user input handling for llama-run,ericcurtin,2025-01-08T14:14:58Z,2025-01-08T18:47:05+00:00,1,4.54
11143,fix: add missing msg in `static_assert` on windows,hydai,2025-01-08T18:59:14Z,2025-01-08T20:03:28+00:00,1,1.07
11148,llama-chat : add phi 4 template,ngxson,2025-01-08T22:10:43Z,2025-01-09T09:07:33+00:00,2,10.95
11150,FR: server: Pre-fill textarea and auto-generate based on query parameters,tim-janik,2025-01-09T00:22:37Z,2025-03-02T17:34:54+00:00,7,1265.2
11154,server : add tooltips to settings and themes btn,danbev,2025-01-09T07:03:48Z,2025-01-09T10:28:30+00:00,1,3.41
11155,examples : add README.md to tts example [no ci],danbev,2025-01-09T10:53:10Z,2025-01-10T12:16:16+00:00,2,25.39
11156,llama : add `struct llama_vocab` to the API ,ggerganov,2025-01-09T13:31:58Z,2025-01-10T09:21:33+00:00,1,19.83
11159,hparams : move vocab params to llama_vocab,ggerganov,2025-01-09T14:55:49Z,2025-01-10T08:59:27+00:00,1,18.06
11160,vocab : minor tokenization optimizations,ggerganov,2025-01-09T15:21:09Z,2025-01-10T09:31:05+00:00,3,18.17
11161,Vulkan: Fix float16 use on devices without float16 support + fix subgroup_size_control validation error,0cc4m,2025-01-09T15:35:12Z,2025-01-10T05:39:33+00:00,1,14.07
11166,vulkan: support copy from f32 to q4_0/q4_1/q5_0/q5_1/q8_0/iq4_nl,jeffbolznv,2025-01-09T20:52:12Z,2025-01-16T21:47:11+00:00,4,168.92
11167,lora : update API names,ggerganov,2025-01-09T20:53:49Z,2025-01-11T14:39:52+00:00,5,41.77
11172,convert : add --print-supported-models option,danbev,2025-01-10T07:10:46Z,2025-01-10T10:30:53+00:00,1,3.34
11174,llama : update API names to use correct prefix,ggerganov,2025-01-10T12:43:13Z,2025-01-11T14:41:56+00:00,1,25.98
11175,SYCL: Add gated linear attention kernel,qnixsynapse,2025-01-10T13:26:36Z,2025-01-15T03:20:17+00:00,5,109.89
11177,contrib : add naming guidelines,ggerganov,2025-01-10T15:43:52Z,2025-01-13T12:46:36+00:00,11,69.05
11179,convert : sort print supported models [no ci],danbev,2025-01-10T17:09:59Z,2025-01-11T04:50:33+00:00,1,11.68
11180,gguf-py: fixed local detection of gguf package,VJHack,2025-01-10T18:35:58Z,2025-01-11T09:42:32+00:00,2,15.11
11186,"Added the ability to use guide tokens for OuteTTS, greatly improving TTS recitation accuracy over long input sequences.",LostRuins,2025-01-11T09:10:09Z,2025-01-18T10:20:57+00:00,1,169.18
11193,cmake : enable -Wshadow for C++ code,ggerganov,2025-01-11T15:56:37Z,2025-05-21T17:00:13+00:00,5,3121.06
11195,common : support tag-based --hf-repo like on ollama,ngxson,2025-01-11T18:51:10Z,2025-01-13T12:56:23+00:00,4,42.09
11205,Reset color before we exit,ericcurtin,2025-01-12T16:56:42Z,2025-01-12T18:23:10+00:00,1,1.44
11206,vulkan: optimize coopmat2 q4_k/q5_k dequant functions.,jeffbolznv,2025-01-12T20:00:48Z,2025-01-16T21:23:49+00:00,1,97.38
11208,llama-server: Support for RTL text as models input or output,ebraminio,2025-01-12T22:44:58Z,2025-01-13T13:46:39+00:00,1,15.03
11211,ggml : do not define GGML_USE_CUDA when building with GGML_BACKEND_DL,rgerganov,2025-01-13T09:28:52Z,2025-01-13T11:31:42+00:00,1,2.05
11213,"llama : refactor llama_kv_cache, llama_context and llm_build_context",ggerganov,2025-01-13T12:18:09Z,2025-03-13T10:36:05+00:00,8,1414.3
11214,cli : auto activate conversation mode if chat template is available,ngxson,2025-01-13T12:44:34Z,2025-01-13T19:18:12+00:00,1,6.56
11215,Added chat template support to llama-run,engelmi,2025-01-13T13:03:34Z,2025-02-17T08:18:07+00:00,25,835.24
11216,added kalavai (external) to infrastructure list on README,musoles,2025-01-13T14:57:13Z,2025-01-17T00:10:49+00:00,1,81.23
11221,server : Improve code snippets direction between RTL text,ebraminio,2025-01-13T20:15:56Z,2025-01-14T10:39:33+00:00,1,14.39
11223,sampling: add Top-nσ sampler,VJHack,2025-01-14T00:26:29Z,2025-02-13T06:45:57+00:00,15,726.32
11224,Refactor test-chat-template.cpp,ochafik,2025-01-14T00:38:39Z,2025-01-14T10:16:41+00:00,1,9.63
11227,ggml: aarch64: implement SVE kernels for q4_K_q8_K vector dot,fj-y-saito,2025-01-14T05:00:37Z,2025-01-16T09:11:49+00:00,1,52.19
11231,"vocab : add dummy tokens for ""no_vocab"" type",ggerganov,2025-01-14T08:28:18Z,2025-01-14T10:54:58+00:00,1,2.44
11233,Support internlm3,RunningLeon,2025-01-14T11:56:29Z,2025-01-16T18:10:38+00:00,1,54.24
11234,Allow s390x to load little endian models unmodified,AlekseiNikiforovIBM,2025-01-14T12:34:59Z,2025-08-28T08:53:01+00:00,5,5420.3
11235,examples : add embd_to_audio to tts-outetts.py [no ci],danbev,2025-01-14T12:42:28Z,2025-01-15T04:44:38+00:00,1,16.04
11240,"RoPE: fix back, CUDA support for back + noncont.",JohannesGaessler,2025-01-14T15:11:06Z,2025-01-15T11:51:37+00:00,2,20.68
11244,AMD: parse the architecture as supplied by gcnArchName,Haus1,2025-01-14T21:58:14Z,2025-01-27T13:58:17+00:00,2,304.0
11251,SYCL: Introducing memory host pool,s-Nick,2025-01-15T10:25:14Z,2025-01-19T13:33:34+00:00,14,99.14
11252,Adding linenoise.cpp to llama-run,ericcurtin,2025-01-15T13:29:25Z,2025-01-18T14:42:31+00:00,13,73.22
11255,llama : add `llama_model_load_from_splits`,ngxson,2025-01-15T16:59:42Z,2025-01-16T12:54:08+00:00,2,19.91
11257,"CUDA: backwards pass for misc. ops, add tests",JohannesGaessler,2025-01-15T20:01:54Z,2025-01-16T15:43:38+00:00,9,19.7
11261,SYCL: SOFTMAX F16 mask support and other fixes,qnixsynapse,2025-01-16T07:53:40Z,2025-01-28T09:56:58+00:00,9,290.06
11262,rpc : register backend devices,rgerganov,2025-01-16T09:01:37Z,2025-01-17T08:57:09+00:00,3,23.93
11269,llama: fix deprecation message: vocabable -> vocab,dwrensha,2025-01-17T00:37:19Z,2025-01-17T07:12:01+00:00,1,6.58
11270,llama.android: add field formatChat to control whether to parse special tokens when send message,codezjx,2025-01-17T03:26:48Z,2025-01-17T12:57:56+00:00,1,9.52
11278,simple-chat : fix BOS being added to each message,ggerganov,2025-01-17T10:34:12Z,2025-01-19T16:12:09+00:00,3,53.63
11279,cmake : add sanitizer flags for llama.cpp,ggerganov,2025-01-17T13:23:26Z,2025-01-18T14:18:16+00:00,10,24.91
11281,vulkan: fix coopmat2 flash attention for non-contiguous inputs,jeffbolznv,2025-01-17T17:36:43Z,2025-01-18T08:26:50+00:00,1,14.84
11284,vulkan: fix coopmat2 validation failures,jeffbolznv,2025-01-17T20:41:30Z,2025-01-20T16:38:32+00:00,1,67.95
11285,server : implement cancellable request,ngxson,2025-01-17T21:21:04Z,2025-01-18T13:12:05+00:00,3,15.85
11287,added rudimentary support for outetts v0.3 500m and 1b models,LostRuins,2025-01-18T10:58:03Z,2025-07-19T08:27:16+00:00,1,4365.49
11289,support Minicpm-omni in image understanding,tc-mb,2025-01-18T14:40:18Z,2025-01-22T07:35:48+00:00,3,88.92
11296,fix: llama-mmap: add include for cerrno; fixes issue #11295,mascguy,2025-01-18T21:14:30Z,2025-01-20T14:02:43+00:00,1,40.8
11300,tests : increase timeout when sanitizers are enabled,ggerganov,2025-01-19T16:47:49Z,2025-01-19T18:22:30+00:00,1,1.58
11301,linenoise.cpp refactoring,ericcurtin,2025-01-19T17:23:34Z,2025-01-21T09:32:35+00:00,1,40.15
11302,run : fix BOS being added to each message,ericcurtin,2025-01-19T17:53:08Z,2025-01-20T14:31:49+00:00,5,20.64
11305,Redded LLM_ARCH_PHIMOE,KyleBruene,2025-01-20T03:04:48Z,2025-01-20T07:21:02+00:00,1,4.27
11309,cmake: fix shell command quoting in build-info script,Xarbirus,2025-01-20T11:38:25Z,2025-01-20T14:02:15+00:00,2,2.4
11315,vulkan: sort shaders for more deterministic binary,jeffbolznv,2025-01-20T16:35:59Z,2025-01-23T07:07:50+00:00,2,62.53
11323,vulkan: fix diag_mask_inf,jeffbolznv,2025-01-21T04:20:28Z,2025-01-23T07:01:17+00:00,1,50.68
11330,export-lora : fix tok_embd tensor,ngxson,2025-01-21T11:30:04Z,2025-01-21T13:07:13+00:00,1,1.62
11331,rpc : better caching of the base buffer pointer,rgerganov,2025-01-21T11:46:36Z,2025-01-21T13:06:42+00:00,2,1.33
11340,server : add more clean up when cancel_tasks is called,ngxson,2025-01-21T23:09:57Z,2025-01-23T12:56:06+00:00,6,37.77
11342,`common`: utils to split / join / repeat strings (from json converter),ochafik,2025-01-22T02:26:23Z,2025-01-22T09:51:45+00:00,1,7.42
11343,Vulkan-run-test: fix mmq_wg_denoms,AMD-dwang,2025-01-22T06:24:51Z,2025-01-23T07:14:28+00:00,2,24.83
11344,Adding logprobs to /v1/completions,jpodivin,2025-01-22T08:22:29Z,2025-01-22T11:51:33+00:00,3,3.48
11349,gguf_convert_endian.py: implement byteswapping for q4_k and q6_k,AlekseiNikiforovIBM,2025-01-22T13:06:05Z,2025-02-24T11:27:01+00:00,1,790.35
11350,Treat hf.co/ prefix the same as hf://,ericcurtin,2025-01-22T13:43:14Z,2025-01-23T10:38:20+00:00,3,20.92
11353,main : update README documentation for batch size,slaren,2025-01-22T16:20:12Z,2025-01-22T18:22:20+00:00,1,2.04
11356,Avoid fp32->fp16->fp32 conversion on cdna in ggml_cuda_op_mul_mat_cublas,IMbackK,2025-01-22T18:21:14Z,2025-01-24T16:50:49+00:00,1,46.49
11360,vulkan: implement initial support for IQ2 and IQ3 quantizations,remyoudompheng,2025-01-22T20:37:27Z,2025-01-29T17:29:39+00:00,6,164.87
11362,Add hipGraph and VMM support to ROCM,IMbackK,2025-01-22T22:14:39Z,2025-01-24T23:02:23+00:00,3,48.8
11364,webui : put DeepSeek R1 CoT in a collapsible <details> element,stduhpf,2025-01-23T02:44:22Z,2025-01-24T08:02:38+00:00,3,29.3
11366,Avoid -march=native when reproducible build is wanted,bmwiedemann,2025-01-23T09:38:18Z,2025-01-24T11:21:35+00:00,1,25.72
11368,docs: Update Docker build instructions to use --target flag,JafarAbdi,2025-01-23T12:06:32Z,2025-01-24T13:30:13+00:00,1,25.39
11369,cmake: add ggml find package,bandoti,2025-01-23T13:17:05Z,2025-01-26T16:07:48+00:00,1,74.85
11373,Update documentation,ericcurtin,2025-01-23T16:20:55Z,2025-01-23T20:04:31+00:00,1,3.73
11375,tests: fix some mul_mat test gaps,jeffbolznv,2025-01-23T19:19:35Z,2025-01-23T20:51:24+00:00,1,1.53
11380,"CPU/CUDA: fix GQA mul mat back, add CUDA support",JohannesGaessler,2025-01-23T21:50:41Z,2025-01-24T11:38:31+00:00,1,13.8
11381,llama: refactor llama_decode_impl,JohannesGaessler,2025-01-23T22:53:04Z,2025-01-27T11:07:12+00:00,1,84.24
11390,ggml-cpu: Add CPU backend support for KleidiAI library,chaxu01,2025-01-24T10:05:06Z,2025-02-20T13:06:52+00:00,18,651.03
11392,release : pack /lib in the packages,ggerganov,2025-01-24T11:31:04Z,2025-01-24T16:41:30+00:00,10,5.17
11393,docs: update fedora cuda guide for 12.8 release,teihome,2025-01-24T16:08:20Z,2025-02-06T12:16:16+00:00,3,308.13
11397,llama : add option to override model tensor buffers,slaren,2025-01-24T20:03:53Z,2025-04-02T12:52:01+00:00,1,1624.8
11399,Handle missing model in CLI parameters for llama-run,engelmi,2025-01-24T20:44:32Z,2025-01-28T08:32:40+00:00,1,83.8
11403,docker : fix CPU ARM build,slaren,2025-01-25T00:22:27Z,2025-01-25T14:22:29+00:00,3,14.0
11406,vulkan: compile shaders on-demand,jeffbolznv,2025-01-25T04:12:13Z,2025-01-25T21:29:58+00:00,3,17.3
11407,build: add /bigobj to MSVC build,jeffbolznv,2025-01-25T05:55:34Z,2025-01-25T17:26:37+00:00,1,11.52
11420,Hip: disable VMM on hip as it seams that it dosent work in some configurations,IMbackK,2025-01-25T16:39:39Z,2025-01-25T20:01:12+00:00,2,3.36
11422,docker: add missing vulkan library to base layer and update to ubuntu 24.04,rare-magma,2025-01-25T21:29:08Z,2025-01-26T17:22:43+00:00,1,19.89
11424,rpc: fix register position,thxCode,2025-01-26T07:23:53Z,2025-01-26T15:20:34+00:00,3,7.94
11434,docker : fix ARM build and Vulkan build,ngxson,2025-01-26T19:32:34Z,2025-01-26T21:45:32+00:00,3,2.22
11436,vulkan: Catch pipeline creation failure and print an error message,jeffbolznv,2025-01-26T20:14:41Z,2025-01-29T15:26:51+00:00,1,67.2
11437,docker: allow installing pip packages system-wide,rare-magma,2025-01-26T21:46:47Z,2025-01-28T14:17:25+00:00,1,40.51
11438,docker: add perplexity and bench commands to full image,rare-magma,2025-01-26T22:18:21Z,2025-01-28T10:42:32+00:00,2,36.4
11441,metal: Handle null returned from MTLCreateSystemDefaultDevice(),booxter,2025-01-27T01:08:33Z,2025-01-27T07:41:59+00:00,1,6.56
11445,Fix graph for RWKV6Qwen2,MollySophia,2025-01-27T09:04:37Z,2025-01-29T04:07:22+00:00,1,43.05
11446,Optimized DeepSeek V2/V3 implementation (MLA),fairydreaming,2025-01-27T09:46:18Z,2025-04-26T09:10:25+00:00,3,2135.4
11448,Minor fixes for up llama load model speed,lexasub,2025-01-27T11:19:29Z,2025-01-27T13:42:10+00:00,8,2.38
11449,Add new hf protocol for ollama,ericcurtin,2025-01-27T11:46:28Z,2025-01-27T18:36:10+00:00,1,6.83
11453,ggml : x2 speed for WASM by optimizing SIMD,ngxson,2025-01-27T14:13:50Z,2025-02-12T23:33:45+00:00,32,393.33
11457,cmake: don't fail on `GGML_CPU=OFF`,someone13574,2025-01-27T21:56:07Z,2025-01-28T14:15:34+00:00,2,16.32
11465,Add github protocol pulling and http://,ericcurtin,2025-01-28T11:50:37Z,2025-01-28T14:45:41+00:00,1,2.92
11466,CMake Bugfix: Windows ggml find_package fix in llama-config.cmake.in file,Emreerdog,2025-01-28T14:37:02Z,2025-01-28T23:22:06+00:00,1,8.75
11471,Hip: Supress transformation warning in softmax.cu,IMbackK,2025-01-28T20:46:45Z,2025-01-28T22:06:33+00:00,1,1.33
11472,ci : fix build CPU arm64,ngxson,2025-01-28T21:54:01Z,2025-01-28T23:02:56+00:00,1,1.15
11475,embedding : enable --no-warmup option,danbev,2025-01-29T05:16:01Z,2025-01-29T08:38:55+00:00,1,3.38
11480,Parse https://ollama.com/library/ syntax,ericcurtin,2025-01-29T09:30:33Z,2025-01-29T11:23:10+00:00,5,1.88
11484,server : update auto gen files comments [no ci],danbev,2025-01-29T11:48:10Z,2025-01-29T15:34:18+00:00,4,3.77
11489,server : add /apply-template endpoint for additional use cases of Minja functionality,pnb,2025-01-29T14:33:21Z,2025-01-29T18:45:44+00:00,3,4.21
11492,server : update json snippets in README.md [no ci],danbev,2025-01-29T16:52:19Z,2025-01-30T04:48:15+00:00,1,11.93
11494,vulkan: Make Vulkan optional at runtime (#11493).,daym,2025-01-29T17:51:11Z,2025-02-10T06:17:22+00:00,17,276.44
11495,Start work on wave 64 optimizeation,IMbackK,2025-01-29T18:49:40Z,2025-01-30T15:25:44+00:00,6,20.6
11496,Correctly identify LF token for GPT-2 style BPE tokenizer,mgroeber9110,2025-01-29T19:45:43Z,2025-01-30T10:11:00+00:00,1,14.42
11499,`sync`: minja,ochafik,2025-01-29T20:33:40Z,2025-01-30T10:30:27+00:00,1,13.95
11501,vulkan: initial support for IQ4_XS quantization,remyoudompheng,2025-01-29T21:14:31Z,2025-02-06T06:09:59+00:00,5,176.92
11502,vulkan: account for lookup tables when checking shared memory size,jeffbolznv,2025-01-29T23:51:07Z,2025-02-09T07:43:52+00:00,1,247.88
11503,server: document /apply-template response format,isaac-mcfadyen,2025-01-29T23:53:33Z,2025-01-30T09:11:53+00:00,1,9.31
11505,docs(readme): reference examples links,guspan-tanadi,2025-01-30T03:21:25Z,2025-01-30T05:58:02+00:00,1,2.61
11507,server : use lambda instead of std::bind,danbev,2025-01-30T05:56:32Z,2025-01-30T10:05:01+00:00,1,4.14
11511,Implement s3:// protocol,ericcurtin,2025-01-30T12:51:29Z,2025-02-01T10:30:54+00:00,2,45.66
11512,server : update help metrics processing/deferred,danbev,2025-01-30T13:01:53Z,2025-01-31T05:04:53+00:00,1,16.05
11513,fix bug in minicpm-v code,tc-mb,2025-01-30T14:53:47Z,2025-03-10T08:33:24+00:00,7,929.66
11516,`ci`: ccache for all github worfklows,ochafik,2025-01-30T16:25:52Z,2025-01-30T22:01:07+00:00,2,5.59
11519,CUDA/HIP: add support for selectable warp size to mmv,IMbackK,2025-01-30T16:44:41Z,2025-02-02T21:40:09+00:00,2,76.92
11521,vulkan: optimize coopmat2 iq2/iq3 callbacks,jeffbolznv,2025-01-30T17:42:40Z,2025-02-06T06:15:30+00:00,1,156.55
11525,vulkan: use kompute matmul shaders on embedded GPUs,slp,2025-01-30T18:14:37Z,2025-02-12T11:35:59+00:00,8,305.36
11528,vulkan: initial support for IQ1_S and IQ1_M quantizations,remyoudompheng,2025-01-30T21:26:13Z,2025-02-15T08:01:41+00:00,6,370.59
11529,common: Add missing va_end,stevegrubb,2025-01-30T22:07:09Z,2025-01-31T05:58:55+00:00,2,7.86
11531,Fix --jinja when there's no tools or schema (typo was forcing JSON),ochafik,2025-01-30T23:17:14Z,2025-01-31T08:12:41+00:00,1,8.92
11533,Fix chatml fallback for unsupported builtin templates (when --jinja not enabled),ochafik,2025-01-31T00:45:23Z,2025-01-31T08:24:29+00:00,1,7.65
11535,server : add handle_slot_type lambda,danbev,2025-01-31T06:53:28Z,2025-02-05T04:25:31+00:00,6,117.53
11539,"`tool-call`: fix llama 3.x and functionary 3.2, play nice w/ pydantic_ai package, update readme",ochafik,2025-01-31T11:16:10Z,2025-01-31T14:15:25+00:00,1,2.99
11543,`server`: fix stop regression,ochafik,2025-01-31T12:31:06Z,2025-01-31T13:48:32+00:00,1,1.29
11545,`ci`: use sccache on windows instead of ccache,ochafik,2025-01-31T12:49:55Z,2025-01-31T17:12:40+00:00,1,4.38
11548,`ci`: simplify cmake build commands (nits),ochafik,2025-01-31T14:46:47Z,2025-02-01T00:01:20+00:00,1,9.24
11551,vulkan: use smaller combined allocations to avoid fragmentation,jeffbolznv,2025-01-31T15:17:46Z,2025-02-06T06:02:18+00:00,6,134.74
11553,ci: use ccache on windows HIP jobs,ochafik,2025-01-31T17:15:45Z,2025-02-01T18:22:39+00:00,1,25.11
11556,tool-call: add support for tool-calls using Model Context Protocol,bandoti,2025-01-31T18:46:42Z,2025-05-26T19:00:24+00:00,2,2760.23
11571,Load all MoE experts during warmup,fairydreaming,2025-02-01T09:42:00Z,2025-03-14T12:47:05+00:00,1,987.08
11573,Name colors,ericcurtin,2025-02-01T10:35:23Z,2025-02-02T15:14:48+00:00,1,28.66
11581,`ci`: fix openEuler ci env that lacks ostringstream::str,ochafik,2025-02-01T21:34:31Z,2025-02-02T09:10:15+00:00,1,11.6
11583,CUDA: use mma PTX instructions for FlashAttention,JohannesGaessler,2025-02-01T23:12:31Z,2025-02-02T18:31:09+00:00,3,19.31
11585,"`tool-call`: support Command R7B (+ return tool_plan ""thoughts"" in API)",ochafik,2025-02-02T01:58:37Z,2025-02-02T09:25:38+00:00,2,7.45
11592,vulkan: add environment variable to avoid VRAM allocation,wbruna,2025-02-02T12:27:24Z,2025-02-10T06:08:22+00:00,1,185.68
11593,`sampling`: softer & more informative crash when grammar sampler fails,ochafik,2025-02-02T12:37:11Z,2025-02-02T19:58:34+00:00,1,7.36
11595,vulkan: add specific MMV kernels for IQ2 and IQ3 quants + optimizations,remyoudompheng,2025-02-02T15:05:12Z,2025-02-28T08:42:52+00:00,7,617.63
11596,De-duplicate fmt and format functions and optimize,ericcurtin,2025-02-02T15:16:15Z,2025-03-25T17:46:12+00:00,12,1226.5
11600,Change umlaut test,n00b001,2025-02-02T18:11:53Z,2025-05-28T13:49:28+00:00,2,2755.63
11604,HIP: fix flash_attn_stream_k_fixup warning,JohannesGaessler,2025-02-02T21:41:45Z,2025-02-02T22:48:30+00:00,1,1.11
11607,"`server`: fix tool-call of DeepSeek R1 Qwen, return reasoning_content (Command 7RB & DeepSeek R1) unless `--reasoning-format none`",ochafik,2025-02-03T02:40:03Z,2025-02-13T10:05:17+00:00,15,247.42
11608,`tool-call`: command r7b fix for normal responses,ochafik,2025-02-03T02:48:54Z,2025-02-04T15:48:53+00:00,1,37.0
11609,webui: Fix Shift+Enter handling,mashdragon,2025-02-03T04:06:06Z,2025-02-03T09:42:55+00:00,1,5.61
11615,CUDA: fix Volta FlashAttention logic,JohannesGaessler,2025-02-03T10:36:21Z,2025-02-03T12:25:56+00:00,1,1.83
11616,"`tool-call`: allow `--chat-template chatml` w/ `--jinja`, default to chatml upon parsing issue, avoid double bos",ochafik,2025-02-03T10:42:34Z,2025-02-03T23:49:27+00:00,2,13.11
11620,server : add try..catch to places not covered by set_exception_handler,ngxson,2025-02-03T14:33:26Z,2025-02-04T17:25:42+00:00,5,26.87
11621,HIP: force max threads per block to be 1024,fxzjshm,2025-02-03T14:34:46Z,2025-02-04T18:18:38+00:00,2,27.73
11622,server : remove CPPHTTPLIB_NO_EXCEPTIONS define,danbev,2025-02-03T14:42:15Z,2025-02-03T15:45:38+00:00,1,1.06
11624,server : use httplib status codes,danbev,2025-02-03T15:27:35Z,2025-02-05T06:55:11+00:00,3,39.46
11626,server : (webui) allow typing and submitting during llm response,woof-dog,2025-02-03T19:17:10Z,2025-02-03T22:16:27+00:00,3,2.99
11628,Add llm_client Rust crate to readme bindings,ShelbyJenkins,2025-02-03T19:58:12Z,2025-02-04T11:20:55+00:00,1,15.38
11641,`sync`: minja,ochafik,2025-02-04T04:03:22Z,2025-02-05T01:00:12+00:00,1,20.95
11644,"Added quantization for the visual projector LLAVA, Qwen2VL",samkoesnadi,2025-02-04T06:06:51Z,2025-02-05T07:45:41+00:00,3,25.65
11645,swift : fix llama-vocab api usage,jhen0409,2025-02-04T06:46:43Z,2025-02-04T11:15:24+00:00,1,4.48
11648,metal : use residency set for other platforms,jhen0409,2025-02-04T07:33:45Z,2025-02-04T11:07:18+00:00,1,3.56
11655,arg : list RPC devices first when using --list-devices,rgerganov,2025-02-04T12:21:01Z,2025-02-04T16:16:20+00:00,1,3.92
11656,CUDA: support for mat. mul. with ne03 != ne13,JohannesGaessler,2025-02-04T12:42:26Z,2025-02-05T07:58:31+00:00,1,19.27
11658,build : fix llama.pc,angt,2025-02-04T14:10:55Z,2025-02-06T11:08:13+00:00,1,44.95
11659,CUDA: non-contiguous (RMS) norm support,JohannesGaessler,2025-02-04T14:25:30Z,2025-02-04T21:21:42+00:00,2,6.94
11664,llguidance build fixes for Windows,mmoskal,2025-02-04T19:36:58Z,2025-02-14T20:46:09+00:00,1,241.15
11666,ggml-cpu : add chunking support to mul_mat_id,slaren,2025-02-05T02:32:24Z,2025-02-13T00:02:38+00:00,2,189.5
11670,llama: fix (non hf) glm4 models,tv1wnd,2025-02-05T06:55:27Z,2025-02-06T21:48:52+00:00,1,38.89
11674,SYCL: Adjust support condition for norm operators,qnixsynapse,2025-02-05T12:46:59Z,2025-02-06T11:42:35+00:00,1,22.93
11677,common : add default embeddings presets,danbev,2025-02-05T14:30:56Z,2025-02-07T08:15:22+00:00,8,41.74
11684,readme : add link to Autopen under UIs,blackhole89,2025-02-05T20:07:44Z,2025-02-06T00:55:26+00:00,1,4.79
11687,When llama_chat_apply_template doesn't work,ericcurtin,2025-02-05T21:11:27Z,2025-09-05T22:04:55+00:00,4,5088.89
11688,server : (webui) migrate project to ReactJS with typescript,ngxson,2025-02-05T22:40:42Z,2025-02-06T16:32:30+00:00,4,17.86
11690,metal : avoid breaking build when metal API predates TARGET_OS_VISION,charles-dyfis-net,2025-02-05T22:55:21Z,2025-02-06T01:52:31+00:00,1,2.95
11697,Add docs for OpenCL backend,lhez,2025-02-06T06:23:01Z,2025-02-11T22:04:13+00:00,1,135.69
11701,Fix LoongArch compile error with 128-bit SIMD,junchao-loongson,2025-02-06T07:47:56Z,2025-02-06T09:20:00+00:00,1,1.53
11707,llama : fix defrag logic,ggerganov,2025-02-06T11:05:03Z,2025-02-07T14:05:34+00:00,2,27.01
11709,Some optimize and build warning fix for LoongArch,MQ-mengqing,2025-02-06T12:16:13Z,2025-02-07T07:38:32+00:00,1,19.37
11712,SYCL: remove XMX/tensor core info from print devices,qnixsynapse,2025-02-06T14:39:51Z,2025-02-07T09:27:54+00:00,2,18.8
11714,Make logging more verbose,ericcurtin,2025-02-06T15:36:48Z,2025-02-07T14:42:46+00:00,1,23.1
11719,vulkan: print shared memory size,jeffbolznv,2025-02-06T20:24:35Z,2025-02-07T10:26:03+00:00,1,14.02
11727,Add `llama_sampler_init` for safe usage of `llama_sampler_free`,cfillion,2025-02-07T07:43:22Z,2025-02-07T09:33:27+00:00,1,1.83
11729,Ignore invalid UTF-8 input in the BPE tokenizer,cfillion,2025-02-07T09:02:58Z,2025-02-07T13:55:47+00:00,4,4.88
11730,llama : fix progress dots,magicse,2025-02-07T11:00:28Z,2025-02-07T13:48:47+00:00,1,2.81
11731,llama : fix ctrl+c handler,magicse,2025-02-07T11:13:13Z,2025-02-10T17:04:39+00:00,2,77.86
11736,ggml: Fix data race in ggml threadpool,kkontny,2025-02-07T14:55:47Z,2025-02-08T14:30:54+00:00,2,23.59
11739,server : (webui) fix numeric settings being saved as string,ngxson,2025-02-07T17:30:22Z,2025-02-08T09:42:34+00:00,1,16.2
11740,server : use common_token_to_piece instead of common_detokenize,danbev,2025-02-07T18:26:22Z,2025-02-11T13:06:45+00:00,12,90.67
11751,CUDA: fix min. version for movmatrix,JohannesGaessler,2025-02-08T08:06:03Z,2025-02-08T09:46:07+00:00,1,1.67
11756,There's a better way of clearing lines,ericcurtin,2025-02-08T10:24:37Z,2025-02-09T10:34:50+00:00,1,24.17
11759,"server : (webui) revamp Settings dialog, add Pyodide interpreter",ngxson,2025-02-08T13:36:23Z,2025-02-08T20:54:50+00:00,1,7.31
11760,server : minor log updates,ggerganov,2025-02-08T14:24:56Z,2025-02-08T16:08:43+00:00,1,1.73
11763,server : (webui) increase edit textarea size,woof-dog,2025-02-08T17:55:31Z,2025-02-08T19:09:55+00:00,1,1.24
11767,vulkan: linux builds + small subgroup size fixes,netrunnereve,2025-02-08T23:16:54Z,2025-02-14T02:59:41+00:00,5,123.71
11769,vulkan: implement several ops relevant for ggml_opt,remyoudompheng,2025-02-09T10:09:33Z,2025-02-17T06:55:57+00:00,15,188.77
11770,docs: utilize the forward slash (/) as the path separator for Unix-li…,MambaWong,2025-02-09T12:48:39Z,2025-02-10T22:17:49+00:00,1,33.49
11773,ggml : fix more imatrix nan cases,slaren,2025-02-09T17:18:23Z,2025-07-03T11:17:37+00:00,3,3449.99
11774,`sync`: minja,ochafik,2025-02-09T18:02:42Z,2025-02-10T09:34:09+00:00,1,15.52
11775,CUDA: use arch list for compatibility check,JohannesGaessler,2025-02-09T18:35:01Z,2025-02-10T23:17:23+00:00,3,28.71
11780,fix crash on non-AVX systems dynamically loading GGML CPU backends,jmorganca,2025-02-10T02:16:51Z,2025-02-13T17:05:04+00:00,1,86.8
11781,Update README.md,pascal-lc,2025-02-10T02:24:37Z,2025-02-10T08:05:57+00:00,1,5.69
11786,fix A800 not supper 'movmatrix' module,glide-the,2025-02-10T10:12:39Z,2025-02-10T14:29:36+00:00,1,4.28
11791,fix: typos in documentation files,maximevtush,2025-02-10T13:14:10Z,2025-02-10T22:21:31+00:00,1,9.12
11792,server : (webui) introduce conversation branching + idb storage,ngxson,2025-02-10T13:38:21Z,2025-02-10T20:23:17+00:00,1,6.75
11794,Add Granite Vision Support,alex-jw-brooks,2025-02-10T15:49:41Z,2025-02-24T16:09:52+00:00,16,336.34
11795,server : correct signal handler,ngxson,2025-02-10T15:54:13Z,2025-02-10T17:03:29+00:00,1,1.15
11803,Fix #11802: Compile bug - RegQueryValueExA changed to RegQueryValueEx,sheldonrobinson,2025-02-11T07:32:51Z,2025-02-11T15:55:46+00:00,1,8.38
11809,docs(examples/server): Update wrong tool calling example,RezaRahemtola,2025-02-11T13:44:37Z,2025-02-13T16:22:44+00:00,1,50.64
11811,cleanup: fix compile warnings associated with gnu_printf,bandoti,2025-02-11T22:12:44Z,2025-02-12T14:06:53+00:00,1,15.9
11814,webui: Give copy button back to all message bubbles,woof-dog,2025-02-12T05:24:34Z,2025-02-12T22:47:11+00:00,1,17.38
11817,ggml-cpu: Fix duplicate MATMUL_INT8,ownia,2025-02-12T07:44:17Z,2025-02-12T12:22:58+00:00,1,4.64
11818,cmake: Fix ggml backend dependencies and installation,vvuksanovic,2025-02-12T09:03:00Z,2025-02-27T07:42:49+00:00,2,358.66
11820,HIP: Switch to std::vector in rocblas version check,IMbackK,2025-02-12T09:21:44Z,2025-02-12T16:25:03+00:00,7,7.06
11821,CUDA: fix CUDART_VERSION checks,JohannesGaessler,2025-02-12T09:34:35Z,2025-02-12T12:16:40+00:00,1,2.7
11822,musa: bump MUSA SDK version to rc3.1.1 ,yeahdongcn,2025-02-12T09:57:13Z,2025-02-13T12:28:18+00:00,1,26.52
11824,Bug fix for clamp_f32,Burton2000,2025-02-12T11:31:08Z,2025-02-12T13:57:33+00:00,2,2.44
11826,vulkan: improve im2col,daniandtheweb,2025-02-12T13:52:03Z,2025-02-28T06:52:51+00:00,10,377.01
11831,HIP: Remove GCN from list of devices that avoid MMQ,IMbackK,2025-02-12T16:38:53Z,2025-02-12T21:25:28+00:00,1,4.78
11832,llama-bench : fix unexpected global variable initialize sequence issue,theraininsky,2025-02-12T16:40:44Z,2025-02-14T01:13:43+00:00,2,32.55
11833,opencl: Fix rope and softmax,lhez,2025-02-12T17:25:08Z,2025-02-14T19:12:23+00:00,1,49.79
11839,server: fix warning message [easy],okuvshynov,2025-02-13T02:50:06Z,2025-02-13T06:25:34+00:00,1,3.59
11843,MUSA: support ARM64 and enable dp4a .etc,BodhiHu,2025-02-13T08:30:49Z,2025-02-21T07:46:23+00:00,24,191.26
11846,llama : add --completion-bash option,danbev,2025-02-13T10:43:43Z,2025-02-13T13:46:59+00:00,1,3.05
11850,vocab : add special infill tokens for CodeLlama,danbev,2025-02-13T15:11:04Z,2025-03-31T16:40:56+00:00,1,1105.5
11854,Upgrade init_tensor API to return a ggml_status,WilliamTambellini,2025-02-14T01:49:25Z,2025-02-28T13:41:47+00:00,13,347.87
11860,llama : add completion for --chat-template-file,danbev,2025-02-14T06:53:25Z,2025-02-14T10:16:56+00:00,1,3.39
11870,cuda : add ampere to the list of default architectures,slaren,2025-02-14T12:55:45Z,2025-02-14T14:33:53+00:00,1,1.64
11880,tool-call: fix type promotion typo causing crashes w/ --jinja w/o tools,ochafik,2025-02-15T00:50:05Z,2025-02-15T10:11:36+00:00,1,9.36
11884,examples: fix typo in imatrix/README.md (#11884),708-145,2025-02-15T12:40:36Z,2025-02-15T19:03:30+00:00,2,6.38
11886,repo : update links to new url,ggerganov,2025-02-15T13:37:41Z,2025-02-15T14:40:57+00:00,2,1.05
11890,readme : add notice about new package registry,ggerganov,2025-02-15T15:22:26Z,2025-02-15T18:29:56+00:00,2,3.12
11891,scripts: fix compare-llama-bench commit hash logic,JohannesGaessler,2025-02-15T16:46:54Z,2025-02-15T19:23:22+00:00,1,2.61
11892,metal : optimize dequant q6_K kernel,akretz,2025-02-15T17:20:13Z,2025-02-15T18:39:20+00:00,1,1.32
11894,CUDA: use async data loading for FlashAttention,JohannesGaessler,2025-02-15T21:15:45Z,2025-02-17T13:03:24+00:00,2,39.79
11895,ci : fix (again) arm64 build fails,ngxson,2025-02-15T21:43:27Z,2025-02-16T09:36:40+00:00,1,11.89
11896,sampling: add Top-nσ sampler to `llama-server`,CasualAutopsy,2025-02-15T22:25:47Z,2025-05-05T20:05:34+00:00,11,1893.66
11897,ci: fix vulkan build and release dependencies,netrunnereve,2025-02-15T23:30:02Z,2025-02-17T11:20:24+00:00,2,35.84
11899,common : Fix a typo in help,standby24x7,2025-02-16T01:38:48Z,2025-02-16T09:51:13+00:00,1,8.21
11900,tool-call: refactor common chat / tool-call api (+ tests / fixes),ochafik,2025-02-16T03:14:10Z,2025-02-18T18:03:24+00:00,16,62.82
11902,"vulkan: support multi/vision rope, and noncontiguous rope",jeffbolznv,2025-02-16T04:35:47Z,2025-02-16T07:52:24+00:00,1,3.28
11904,Fix the crash caused by the lack of residency set support on Intel Macs.,halechan,2025-02-16T05:41:47Z,2025-02-16T06:50:26+00:00,1,1.14
11905,gguf-py: handle numpy 2.0 byte-ordering changes,parsapoorsh,2025-02-16T06:38:46Z,2025-02-16T19:05:44+00:00,1,12.45
11907,scripts: corrected encoding when getting chat template (#11866),MoonRide303,2025-02-16T12:02:54Z,2025-02-18T09:30:16+00:00,1,45.46
11908,server : bump httplib to 0.19.0,ngxson,2025-02-16T12:33:00Z,2025-02-16T17:11:22+00:00,1,4.64
11909,Refactor gguf scripts to improve metadata handling,CISC,2025-02-16T12:55:00Z,2025-02-26T13:04:48+00:00,4,240.16
11914,vulkan: implement more backpropagation operators,remyoudompheng,2025-02-16T21:35:58Z,2025-02-25T11:04:45+00:00,2,205.48
11915,server : fix divide-by-zero in metrics reporting,aviallon,2025-02-16T21:48:26Z,2025-02-17T10:25:12+00:00,1,12.61
11917,ggml: aarch64: implement SVE kernels for q3_K_q8_K vector dot,Vithulep,2025-02-17T03:58:13Z,2025-02-20T10:08:33+00:00,5,78.17
11922,Added --chat-template-file to llama-run,engelmi,2025-02-17T08:17:51Z,2025-02-19T15:16:42+00:00,12,54.98
11935,Fix duplicated .py extension in test command for llama-server,xiaobing318,2025-02-17T22:40:16Z,2025-02-18T09:12:50+00:00,1,10.54
11936,make `ggml_is_view_op` public.,foldl,2025-02-18T01:42:00Z,2025-02-24T22:52:20+00:00,2,165.17
11940,Webui: Enable communication with parent html (if webui is in iframe):,igardev,2025-02-18T08:35:41Z,2025-02-18T22:01:45+00:00,13,13.43
11942,server : add TEI API format for /rerank endpoint,ngxson,2025-02-18T10:32:56Z,2025-02-18T13:21:41+00:00,4,2.81
11943,llama : fix indentation in llama-grammar [no ci],danbev,2025-02-18T11:45:45Z,2025-02-19T05:16:24+00:00,1,17.51
11945,common : add llama.vim preset for Qwen2.5 Coder,danbev,2025-02-18T17:06:16Z,2025-02-19T11:29:52+00:00,5,18.39
11950,opencl: fix for small models,lhez,2025-02-18T23:25:28Z,2025-02-24T21:47:07+00:00,1,142.36
11954,speculative : update default params,ggerganov,2025-02-19T06:21:27Z,2025-02-19T11:29:43+00:00,2,5.14
11958,doc: add link to MNIST example [no ci],JohannesGaessler,2025-02-19T14:11:24Z,2025-02-19T19:45:18+00:00,1,5.57
11961,Added --chat-template-file to llama-run,engelmi,2025-02-19T15:17:00Z,2025-02-20T08:35:11+00:00,2,17.3
11964,HIP: workaround runtime bug in hipGraph support,IMbackK,2025-02-19T17:42:11Z,2025-03-02T20:40:20+00:00,1,266.97
11969,doc: update contributing guidelines [no ci],JohannesGaessler,2025-02-20T09:24:11Z,2025-02-21T11:51:25+00:00,3,26.45
11971,server (webui): Fix Premature Submission During IME Conversion,mmngays,2025-02-20T12:37:18Z,2025-02-20T18:43:22+00:00,1,6.1
11973,Some llama-run cleanups,ericcurtin,2025-02-20T13:54:19Z,2025-02-23T13:14:32+00:00,2,71.34
11982,Fix visual encoders with no CLS,alex-jw-brooks,2025-02-20T19:44:09Z,2025-02-21T06:11:03+00:00,1,10.45
11984,CUDA: correct the lowest Maxwell supported by CUDA 12,PureJourney,2025-02-20T21:03:30Z,2025-02-21T11:21:05+00:00,2,14.29
11990,CANN: Fix build error with GCC 13,hipudding,2025-02-21T01:49:41Z,2025-02-28T07:23:47+00:00,1,173.57
11994,deepseek r1 series debug log warning fix and chat template support,swordow,2025-02-21T05:54:02Z,2025-03-07T22:05:29+00:00,2,352.19
11995,SYCL: Fix GGML_SYCL_DEBUG macro,qnixsynapse,2025-02-21T06:22:46Z,2025-02-24T10:18:26+00:00,2,75.93
11996,llama : add xcframework build script,danbev,2025-02-21T07:29:12Z,2025-03-05T05:30:32+00:00,14,286.02
11997,llama : expose llama_model_n_head_kv in the API,vlovich,2025-02-21T07:46:13Z,2025-02-25T09:29:34+00:00,1,97.72
11998,"llama.swiftui : add ""Done"" dismiss button to help view",danbev,2025-02-21T07:55:27Z,2025-02-22T05:33:30+00:00,1,21.63
11999,llava: build clip image from pixels,tinglou,2025-02-21T08:08:32Z,2025-02-22T14:28:28+00:00,2,30.33
12000,"cuda: Add Q5_1, Q5_0, Q4_1 and Q4_0 to F32 conversion support. (#10976)",gcp,2025-02-21T09:32:39Z,2025-02-22T08:43:24+00:00,3,23.18
12006,Add Doc for Converting Granite Vision -> GGUF,alex-jw-brooks,2025-02-21T17:15:23Z,2025-02-25T09:46:05+00:00,1,88.51
12009,Run CI on Github-hosted arm64 runners too,Rohanjames1997,2025-02-21T18:55:46Z,2025-02-22T10:48:58+00:00,1,15.89
12011,`common`: add -jf / --json-schema-file flag,ochafik,2025-02-21T20:38:16Z,2025-04-30T12:52:35+00:00,1,1624.24
12014,CUDA: optimize FA for GQA + large batches,JohannesGaessler,2025-02-21T22:19:54Z,2025-02-22T11:20:17+00:00,1,13.01
12015,vulkan: matmul dequantization improvements,netrunnereve,2025-02-21T22:35:53Z,2025-02-28T07:20:09+00:00,7,152.74
12019,ggml-cpu: Support s390x SIMD Instruction Set,taronaeo,2025-02-22T08:34:02Z,2025-02-22T21:39:24+00:00,3,13.09
12020,server : disable Nagle's algorithm,ggerganov,2025-02-22T08:59:27Z,2025-02-22T10:46:31+00:00,1,1.78
12025,CUDA: app option to compile without FlashAttention,JohannesGaessler,2025-02-22T13:29:57Z,2025-02-22T19:44:34+00:00,1,6.24
12029,CUDA: compress-mode size,Green-Sky,2025-02-22T18:01:05Z,2025-03-01T11:57:22+00:00,3,161.94
12032,Add GGML_HIP_ROCWMMA_FATTN to enable rocWMMA for FlashAttention,hjc4869,2025-02-22T19:49:10Z,2025-03-03T21:10:54+00:00,10,217.36
12034,"`tool-call`: fix Qwen 2.5 Coder support, add micro benchmarks, support trigger patterns for lazy grammars",ochafik,2025-02-22T22:23:25Z,2025-03-05T13:05:14+00:00,9,254.7
12035,[SYCL] Optimize mul_mat for Q4_0 on Intel GPU,NeoZhangJianyu,2025-02-23T09:52:40Z,2025-02-24T14:33:23+00:00,8,28.68
12041,run: allow to customize prompt by env var LLAMA_PROMPT_PREFIX,benoitf,2025-02-23T15:02:28Z,2025-02-23T17:15:51+00:00,8,2.22
12042,llama-tts : add -o option,marcoStocchi,2025-02-23T16:27:36Z,2025-03-15T08:10:32+00:00,9,471.72
12048,tts: add speaker file support,dm4,2025-02-24T08:12:42Z,2025-03-03T13:09:29+00:00,6,172.95
12049,"PR: Refine ggml-qnn backend(QNN, Qualcomm Neural Network,aka Qualcomm AI Engine Direct) for latest ggml,whisper.cpp,llama.cpp",jeffzhou2000,2025-02-24T08:59:59Z,2025-03-01T02:37:32+00:00,4,113.63
12056,Vulkan: add OP sigmoid,foldl,2025-02-25T01:53:09Z,2025-02-25T11:32:20+00:00,2,9.65
12059,ggml-cpu: Fix build with sve,MollySophia,2025-02-25T03:28:29Z,2025-02-25T11:28:23+00:00,1,8.0
12060,server: handle echo=false on /v1/completions,rhjdvsgsgks,2025-02-25T05:39:08Z,2025-02-25T11:52:53+00:00,1,6.23
12062,server: support add_generation_prompt query param,ochafik,2025-02-25T05:52:30Z,2025-02-25T10:40:22+00:00,1,4.8
12064,ggml: aarch64: implement SVE kernels for q2_k_q8_k vector dot,Vithulep,2025-02-25T07:56:16Z,2025-02-28T07:36:13+00:00,4,71.67
12068,vulkan: fix assertion when qy_needs_dequant,jeffbolznv,2025-02-25T14:14:14Z,2025-02-25T15:30:22+00:00,1,1.27
12069,docs: add docs/function-calling.md to lighten server/README.md's plight,ochafik,2025-02-25T15:48:27Z,2025-02-25T18:52:56+00:00,1,3.07
12079,llava : add struct for FFI bindgen,tinglou,2025-02-26T10:09:42Z,2025-02-26T14:26:52+00:00,2,4.29
12087,vulkan: subgroup size tuning,daniandtheweb,2025-02-26T14:48:09Z,2025-03-17T11:42:33+00:00,5,452.91
12094,cmake : fix undefined reference errors for std::filesystem in ggml (#12092),hbuxiaofei,2025-02-27T11:25:20Z,2025-03-06T22:58:26+00:00,1,179.55
12095,opencl:Fix profile-related errors,simon886212,2025-02-27T12:53:25Z,2025-03-06T01:30:05+00:00,1,156.61
12097,sycl: cleanup oneDNN related code,sgeor255,2025-02-27T16:26:23Z,2025-03-21T02:15:56+00:00,10,513.83
12098,CUDA: fix logic for V100 + GGML_CUDA_FORCE_MMQ,JohannesGaessler,2025-02-27T17:36:27Z,2025-02-28T08:26:43+00:00,1,14.84
12099,Add Phi-4-mini-instruct support,ns3284,2025-02-27T22:40:02Z,2025-02-28T09:52:52+00:00,1,11.21
12105,Update granite vision docs for 3.2 model,alex-jw-brooks,2025-02-28T07:39:25Z,2025-02-28T11:31:47+00:00,2,3.87
12108,llama : add Phi-4-mini support (supersede #12099),ngxson,2025-02-28T09:52:35Z,2025-02-28T11:44:11+00:00,2,1.86
12111,Adding UTF-8 support to linenoise.cpp,ericcurtin,2025-02-28T12:47:57Z,2025-03-03T12:44:56+00:00,1,71.95
12112,examples: bugfix: fix memory leak on main.cpp,lizhenneng,2025-02-28T13:25:24Z,2025-02-28T15:40:39+00:00,1,2.25
12114,convert : fix Norway problem when parsing YAML,ngxson,2025-02-28T15:31:02Z,2025-02-28T16:44:46+00:00,5,1.23
12116,minor: web-ui typo fixes,vynride,2025-02-28T17:24:20Z,2025-03-01T10:15:09+00:00,1,16.85
12118,main: use jinja chat template system prompt by default,CISC,2025-02-28T22:08:31Z,2025-03-02T13:53:48+00:00,8,39.75
12131,"common : add --system-prompt parameter, replace behavior of -p on -cnv mode",CISC,2025-03-01T10:48:47Z,2025-03-01T12:56:45+00:00,9,2.13
12133,SYCL: Move CPY kernels to a separate file and add few missing kernels,qnixsynapse,2025-03-01T15:46:12Z,2025-03-03T10:07:22+00:00,2,42.35
12135,Vulkan: Add DP4A MMQ and Q8_1 quantization shader,0cc4m,2025-03-01T19:12:50Z,2025-03-31T12:37:01+00:00,12,713.4
12144,ggml-backend : keep paths in native string type when possible,slaren,2025-03-02T15:11:06Z,2025-03-02T21:11:00+00:00,1,6.0
12145,main: allow preloading conversation with -p and add -st / --single-turn,CISC,2025-03-02T15:16:57Z,2025-03-04T16:19:40+00:00,14,49.05
12148,webui : add ?m=... and ?q=... params,ngxson,2025-03-02T17:34:42Z,2025-03-03T10:42:45+00:00,5,17.13
12150,Some portability improvements from trying to build with Visual Studio 2017,mgroeber9110,2025-03-02T18:09:31Z,2025-03-04T16:53:26+00:00,1,46.73
12154,ggml-cpu: Faster IQ1 mul_mat_vec on AVX2 using BMI2 instructions,remyoudompheng,2025-03-02T22:11:24Z,2025-03-06T01:26:10+00:00,1,75.25
12155,test-backend-ops : add option -p to filter by op params,slaren,2025-03-02T22:40:31Z,2025-03-03T13:00:46+00:00,1,14.34
12157,build: fix build error when build source code on Windows,jeffzhou2000,2025-03-03T05:22:42Z,2025-03-10T05:20:12+00:00,5,167.96
12162,ci : set GITHUB_ACTIONS to true for server tests,danbev,2025-03-03T13:00:40Z,2025-03-03T15:17:36+00:00,1,2.28
12168,`server`: fix response_format w/ json_schema.schema,ochafik,2025-03-03T21:46:56Z,2025-03-04T06:24:07+00:00,1,8.62
12174,opencl: Fix `ulong` kernel args were set from `int` variables,linehill,2025-03-04T10:28:01Z,2025-03-06T01:31:14+00:00,1,39.05
12177,CUDA/HIP: refractor mmqv to unify the calculation of nwarps and rows per block between host and device code.,IMbackK,2025-03-04T12:31:08Z,2025-03-11T19:16:03+00:00,3,174.75
12179,HIP: rocWMMA documentation and enabling in workflow builds,hjc4869,2025-03-04T13:38:17Z,2025-03-06T13:14:11+00:00,2,47.6
12181,"llama : refactor llama_context, llama_kv_cache, llm_build_context (v2)",ggerganov,2025-03-04T15:12:48Z,2025-03-13T10:35:44+00:00,1,211.38
12183,CUDA: Improve flash decoding kernel GPU occupancy for BS=1 case,gaugarg-nv,2025-03-04T16:05:52Z,2025-03-19T19:52:06+00:00,5,363.77
12188,vulkan: double buffer scale caches,netrunnereve,2025-03-04T23:05:36Z,2025-03-10T19:28:11+00:00,1,140.38
12192,SYCL: Rename oneMKL to oneMath,Rbiessy,2025-03-05T09:29:04Z,2025-04-01T08:24:30+00:00,39,646.92
12194,metal : simplify kernel arguments using a struct (#3229),BB-fat,2025-03-05T10:29:39Z,2025-03-07T07:35:57+00:00,3,45.1
12195,ci : add fetch-depth to xcframework upload,danbev,2025-03-05T11:20:22Z,2025-03-05T13:16:40+00:00,1,1.94
12197,opencl: Fix not enough space in the buffer,linehill,2025-03-05T12:22:58Z,2025-03-06T01:33:40+00:00,1,13.18
12200,ggml : refactor metal library loading to avoid GGMLMetalClass ODR,pminev,2025-03-05T13:40:30Z,2025-03-05T15:16:01+00:00,1,1.59
12201,SYCL: Disable f16 Unary OPs as not supported by the kernels,qnixsynapse,2025-03-05T14:14:17Z,2025-03-05T15:58:24+00:00,1,1.74
12209,HIP/CUDA: set the paramerter value in maintain_cuda_graph instead of replaceing it.,IMbackK,2025-03-05T20:46:46Z,2025-03-06T07:20:52+00:00,1,10.57
12212,android : Calculate required KV cache size by summing up tokens size and response token length (#12211),hanyin-arm,2025-03-05T22:29:39Z,2025-03-06T06:22:49+00:00,1,7.89
12214,docs: update function-calling.md w/ template override needed by functionary-small-v3.2,ochafik,2025-03-06T01:05:11Z,2025-03-06T09:03:32+00:00,1,7.97
12216,ggml-cpu: faster AVX2 variant for IQ1_M,remyoudompheng,2025-03-06T05:23:40Z,2025-03-07T11:54:23+00:00,1,30.51
12217,"opencl: Noncontiguous `norm`, `rms_norm`, disable `fp16` for some ops",lhez,2025-03-06T06:32:23Z,2025-03-07T00:20:35+00:00,1,17.8
12218,llava: add big-endian conversion for image encoder,taronaeo,2025-03-06T07:16:30Z,2025-03-06T08:33:21+00:00,1,1.28
12221,opencl: use OpenCL C standard supported by the device,linehill,2025-03-06T11:05:28Z,2025-03-10T16:57:00+00:00,1,101.86
12222,CUDA: fix FA logic for PTX 7.0 and CC >= 7.5,JohannesGaessler,2025-03-06T12:21:07Z,2025-03-06T17:45:09+00:00,1,5.4
12224,metal : fix default.metallib build,danbev,2025-03-06T12:53:21Z,2025-03-07T05:23:17+00:00,1,16.5
12230,Fix HIP rocWMMA CI build break,hjc4869,2025-03-06T18:48:37Z,2025-03-07T07:06:08+00:00,1,12.29
12233,server: log original chat template parsing error,CISC,2025-03-06T21:07:41Z,2025-03-07T10:15:34+00:00,1,13.13
12235,`sync`: minja (support QwQ-32B),ochafik,2025-03-06T21:58:25Z,2025-03-07T09:33:37+00:00,1,11.59
12241,Issues while enabling MMA support on AIX machines,mehendarkarprajwal,2025-03-07T07:22:06Z,2025-03-18T09:37:33+00:00,2,266.26
12244,clang-tidy : disable bugprone-branch-clone,ggerganov,2025-03-07T09:38:40Z,2025-03-10T12:08:50+00:00,1,74.5
12246,server : Add verbose output to OAI compatible chat endpoint.,mglambda,2025-03-07T11:39:24Z,2025-03-23T18:30:26+00:00,1,390.85
12247,ggml : skip intermediate .air file when compiling .metallib,danbev,2025-03-07T11:46:16Z,2025-03-07T13:15:27+00:00,1,1.49
12250,main : add -sysf / --system-prompt-file (#12249),CISC,2025-03-07T13:40:28Z,2025-03-14T15:57:06+00:00,8,170.28
12254,server : infill gen ends on new line,ggerganov,2025-03-07T15:22:56Z,2025-03-07T18:54:30+00:00,1,3.53
12258,vulkan: Adjust coopmat2 tile sizes and selection heuristic,jeffbolznv,2025-03-07T18:14:58Z,2025-03-17T09:35:00+00:00,1,231.33
12261,Add simple-tts example,danemadsen,2025-03-08T01:02:06Z,2025-03-28T04:23:25+00:00,3,483.36
12265,metal: Cache compiled library at device level,BB-fat,2025-03-08T05:32:12Z,2025-03-11T11:45:02+00:00,1,78.21
12272,vulkan: fix coopmat shader generation when cross-compiling,Icenowy,2025-03-08T17:02:41Z,2025-03-28T17:51:06+00:00,16,480.81
12273,"vulkan: Pad N dimension of B matrix for coopmat2 perf, to avoid bounds checking",jeffbolznv,2025-03-08T17:41:45Z,2025-03-17T09:41:59+00:00,1,208.0
12278,Refactoring '-o' option,marcoStocchi,2025-03-09T05:52:40Z,2025-03-10T11:34:13+00:00,2,29.69
12285,"server: fix ""--grammar-file"" parameter",dodekapod,2025-03-09T12:52:50Z,2025-03-14T10:21:18+00:00,1,117.47
12291,`sampler`: fixes trigger tokens + lazy grammars (fix typo cast from token to string),ochafik,2025-03-09T23:54:58Z,2025-03-10T09:44:42+00:00,3,9.83
12292,`tool-call`: ensure there's always a non-empty tool call id,ochafik,2025-03-10T00:11:38Z,2025-03-10T09:45:30+00:00,1,9.56
12293,`tool-call`: allow missing content in message if tool_calls provided,ochafik,2025-03-10T00:42:48Z,2025-03-10T09:45:07+00:00,1,9.04
12296,musa: support new arch mp_31 and update doc,yeahdongcn,2025-03-10T01:50:00Z,2025-03-10T17:18:25+00:00,1,15.47
12297,`server`: extract <think> tags from qwq outputs,ochafik,2025-03-10T02:08:59Z,2025-03-10T10:59:03+00:00,3,8.83
12299,webui: Stop rerender on textarea input and end the devastating lag,woof-dog,2025-03-10T04:24:23Z,2025-03-20T14:57:43+00:00,4,250.56
12301,Update build.yml for Windows Vulkan builder to use Vulkan 1.4.304 SDK…,oscarbg,2025-03-10T07:28:57Z,2025-03-12T19:06:58+00:00,2,59.63
12309,vulkan: use fp32 in coopmat2 q4_k dequant function,jeffbolznv,2025-03-10T13:45:08Z,2025-03-17T09:43:35+00:00,1,163.97
12310,ggml : fix quantized cpy op,ggerganov,2025-03-10T13:48:43Z,2025-03-22T14:23:26+00:00,6,288.58
12312,vulkan: Add N/2 and N/4 optimized paths in coopmat2 shader,jeffbolznv,2025-03-10T16:49:25Z,2025-03-17T14:26:18+00:00,1,165.61
12315,CUDA/HIP: Fix fattn-vec-* when device warp size is not 32,IMbackK,2025-03-10T18:36:31Z,2025-03-12T09:14:11+00:00,1,38.63
12316,vulkan: fix bug in coopmat1 mul_mat_id,jeffbolznv,2025-03-10T19:15:05Z,2025-03-12T05:59:19+00:00,1,34.74
12321,Enabling building llama.cpp using system libggml,ckastner,2025-03-10T21:45:40Z,2025-03-17T09:05:23+00:00,4,155.33
12322,clip : bring back GPU support,ngxson,2025-03-10T22:12:42Z,2025-03-11T08:20:16+00:00,5,10.13
12323,bugfix: Respect n_predict=-2 in server (#12264),ishaangandhi,2025-03-10T22:43:38Z,2025-03-13T10:51:54+00:00,10,60.14
12326,"PR: Refine ggml-hexagon backend(Qualcomm Hexagon NPU backend) for latest ggml,whisper.cpp,llama.cpp",jeffzhou2000,2025-03-11T06:32:03Z,2025-07-14T04:38:05+00:00,5,2998.1
12330,Fix backend search path,jklincn,2025-03-11T09:08:44Z,2025-03-11T13:25:17+00:00,4,4.28
12332,Block interleaving support for Q4_K quantization for x86 AVX2 architecture,Srihari-mcw,2025-03-11T13:24:41Z,2025-03-20T11:35:34+00:00,1,214.18
12336,sycl : variable sg_size support for mmvq kernels,Alcpz,2025-03-11T15:00:46Z,2025-03-12T09:57:32+00:00,2,18.95
12338,server : fix crash when using verbose output with input tokens that are not in printable range (#12178),ishaangandhi,2025-03-11T16:15:19Z,2025-03-13T10:10:05+00:00,3,41.91
12343,llama : Add Gemma 3 support (+ experimental vision capability),ngxson,2025-03-12T06:39:20Z,2025-03-12T08:30:24+00:00,1,1.85
12361,ggml: aarch64: implement SVE kernels for q6_K_q8_K vector dot,fj-y-saito,2025-03-13T08:14:18Z,2025-03-18T08:14:39+00:00,2,120.01
12364,arg : no n_predict = -2 for examples except for main and infill,ngxson,2025-03-13T10:29:42Z,2025-03-13T11:34:54+00:00,1,1.09
12366,SYCL: set extras only on GGML_TYPE_Q4_0,qnixsynapse,2025-03-13T12:07:00Z,2025-03-17T01:45:12+00:00,1,85.64
12370,Add CLI arg to llama-run to adjust the number of threads used,ericcurtin,2025-03-13T13:29:33Z,2025-03-14T16:41:20+00:00,1,27.2
12371,SYCL: using graphs is configurable by environment variable and compile option,lslusarczyk,2025-03-13T14:23:37Z,2025-03-18T10:16:31+00:00,17,115.88
12372,Add support for new gfx1200 and gfx1201 targets,slojosic-amd,2025-03-13T15:08:49Z,2025-03-26T22:46:30+00:00,11,319.63
12373,llama : fix Gemma3 SWA KV cache shift,ggerganov,2025-03-13T15:18:18Z,2025-03-13T17:08:08+00:00,3,1.83
12379,`server`: streaming of tool calls and thoughts when `--jinja` is on,ochafik,2025-03-14T04:45:40Z,2025-05-25T00:48:08+00:00,4,1724.04
12382,[CANN]MUL_MAT optimization,noemotiovon,2025-03-14T08:03:34Z,2025-03-15T01:31:08+00:00,1,17.46
12387,SYCL: Remove misleading ggml_sycl_op_flatten function,qnixsynapse,2025-03-14T12:06:40Z,2025-03-31T09:25:24+00:00,15,405.31
12391,SYCL: Delete redundant plus sign and space,aubreyli,2025-03-14T14:41:23Z,2025-03-15T14:49:03+00:00,2,24.13
12394,[CUDA] Enable CUDA Graph on CUDA Toolkit < 12.x,gaugarg-nv,2025-03-14T15:45:49Z,2025-03-17T18:25:13+00:00,3,74.66
12397,context : fix init of n_outputs,ggerganov,2025-03-15T07:01:10Z,2025-03-16T17:29:36+00:00,2,34.47
12398,llama-tts : add '-o' option,marcoStocchi,2025-03-15T09:09:57Z,2025-03-15T16:23:11+00:00,1,7.22
12399,"SYCL : support non-contiguous tensors in binary ops (add, sub, etc)",fairydreaming,2025-03-15T12:18:20Z,2025-03-15T14:19:30+00:00,3,2.02
12400,llama : support OLMo-2-0325-32B (#12376),CISC,2025-03-15T13:27:10Z,2025-03-16T17:46:36+00:00,1,28.32
12402,Add Qwen2.5VL support,HimariO,2025-03-15T19:16:30Z,2025-04-27T08:10:34+00:00,24,1020.9
12406,vulkan: Submit once enough matmul work has been recorded,jeffbolznv,2025-03-16T04:33:41Z,2025-03-19T07:26:26+00:00,1,74.88
12409,ci : add --symlinks to xcframework zip command,danbev,2025-03-16T06:46:15Z,2025-03-16T17:22:06+00:00,1,10.6
12412,llama: Add support for RWKV v7 architecture(v2),MollySophia,2025-03-16T13:00:55Z,2025-03-17T23:27:51+00:00,2,34.45
12415,Fix visionOS build and add CI,guusw,2025-03-17T04:55:17Z,2025-03-19T10:15:23+00:00,2,53.34
12416,ggml-vulkan: remove unused find_program(glslc),guusw,2025-03-17T05:00:30Z,2025-03-17T16:35:43+00:00,1,11.59
12424,sycl: fixed compilation warnings,lslusarczyk,2025-03-17T12:04:58Z,2025-03-18T00:51:25+00:00,3,12.77
12426,docs : bring llama-cli conversation/template docs up-to-date (#12036),CISC,2025-03-17T12:37:28Z,2025-03-17T20:14:32+00:00,1,7.62
12427,vulkan: optimize iq1 coopmat2 dequant functions,jeffbolznv,2025-03-17T13:22:36Z,2025-03-19T18:56:23+00:00,3,53.56
12428,ci: add Linux cross-compile build,bandoti,2025-03-17T14:41:49Z,2025-04-04T17:05:13+00:00,3,434.39
12434,Vulkan: Default to 1GB allocations instead of 4GB to avoid fragmentation and driver issues,0cc4m,2025-03-17T19:46:10Z,2025-03-18T06:21:40+00:00,1,10.59
12442,opencl: improve profiling,lhez,2025-03-18T04:49:55Z,2025-03-18T19:54:55+00:00,1,15.08
12445,musa: override warp_size of musa device to 32,yeahdongcn,2025-03-18T08:20:03Z,2025-03-18T18:28:26+00:00,1,10.14
12446,server : fix warmup draft cache type,ggerganov,2025-03-18T08:49:43Z,2025-03-18T10:05:43+00:00,1,1.27
12447,context : always use non-causal attention for encoder graphs,ggerganov,2025-03-18T09:16:06Z,2025-03-18T11:05:49+00:00,1,1.83
12449,"graph : normalize Q, K, V shapes and add comments",ggerganov,2025-03-18T11:55:05Z,2025-03-18T19:35:20+00:00,2,7.67
12450,llama : support converting Mistral Small (text-only),ngxson,2025-03-18T14:37:11Z,2025-03-18T18:16:19+00:00,1,3.65
12451,llama : add support for EXAONE tied word embeddings,ngxson,2025-03-18T14:56:31Z,2025-03-18T16:24:34+00:00,1,1.47
12454,speculative : fix seg fault in certain cases,ggerganov,2025-03-18T16:12:53Z,2025-03-18T17:35:11+00:00,1,1.37
12456,"llama : add support for GPT2, Bloom and CodeShell tied word embeddings",CISC,2025-03-18T19:48:30Z,2025-03-19T08:08:49+00:00,1,12.34
12457,Add PLM GGUF Conversion & Inference Support,Si1w,2025-03-18T20:35:23Z,2025-03-27T10:49:15+00:00,10,206.23
12460,convert : support chat_template.json,CISC,2025-03-19T04:34:41Z,2025-03-19T07:58:13+00:00,1,3.39
12466,Nomic Embed Text V2 with Mixture-of-Experts (MoE) architecture,manyoso,2025-03-19T13:45:32Z,2025-04-28T19:52:15+00:00,6,966.11
12470,context : clear sets of encoder output sequence ids before storing new values,fairydreaming,2025-03-19T18:26:42Z,2025-03-19T20:01:57+00:00,1,1.59
12472,vulkan: workaround for #10710 and #12147 16 bit unpack8 bug,netrunnereve,2025-03-19T22:07:32Z,2025-03-21T19:27:47+00:00,1,45.34
12480,Vulkan: RTE rounding for cpy to quant,stduhpf,2025-03-20T18:50:33Z,2025-03-21T19:34:50+00:00,2,24.74
12482,llama-tts : avoid crashes related to bad model file paths,marcoStocchi,2025-03-20T20:23:41Z,2025-03-21T09:12:45+00:00,1,12.82
12489,llamafile : ppc64le MMA implementation for Q4_0.,amritahs-ibm,2025-03-21T06:39:30Z,2025-03-27T06:51:48+00:00,1,144.21
12492,chore : cleanup llama_model_loader::TENSOR_ usage,CISC,2025-03-21T08:12:41Z,2025-03-21T09:21:37+00:00,1,1.15
12493,musa: refine compute capability,yeahdongcn,2025-03-21T09:01:59Z,2025-03-22T09:11:37+00:00,6,24.16
12496,rpc : send hash when tensor data is above some fixed threshold,rgerganov,2025-03-21T11:34:56Z,2025-03-28T06:18:04+00:00,8,162.72
12503,opencl: simplify kernel embedding logic in CMakeLists.txt,lhez,2025-03-21T17:40:25Z,2025-03-24T16:20:47+00:00,1,70.67
12505,vulkan: Optimize mul_mat_vec p021 and nc shaders,jeffbolznv,2025-03-21T19:18:50Z,2025-03-22T08:40:12+00:00,1,13.36
12506,llama : gemma3 : use output tensor if it exists in model weight,ngxson,2025-03-21T21:12:11Z,2025-03-22T22:28:19+00:00,1,25.27
12511,quantize: Handle user-defined quantization levels for additional tensors,EAddario,2025-03-22T08:21:22Z,2025-04-13T18:29:28+00:00,7,538.13
12512,perplexity: Add option to ignore context window overflow errors and continue score calculation,EAddario,2025-03-22T08:37:14Z,2025-04-09T19:05:53+00:00,2,442.48
12515,Vulkan: Remove dedicated aligned matrix matrix multiplication shaders,0cc4m,2025-03-22T14:58:13Z,2025-09-14T12:05:22+00:00,4,4221.12
12522,cmake: fix ccache conflict,BusyJay,2025-03-23T05:25:18Z,2025-03-29T10:04:58+00:00,1,148.66
12529,vulkan: fix mul_mat_vec failure in backend tests,jeffbolznv,2025-03-23T14:56:49Z,2025-03-24T06:56:18+00:00,1,15.99
12530,ggml : riscv: add 128-bit RVV support,xctan,2025-03-23T15:37:57Z,2025-03-27T06:38:34+00:00,4,87.01
12532,llama-vocab : add SuperBPE pre-tokenizer,compilade,2025-03-23T20:39:27Z,2025-03-24T10:47:24+00:00,1,14.13
12536,docs: update: improve the Fedoa CUDA guide,teihome,2025-03-24T04:25:15Z,2025-03-24T11:02:27+00:00,1,6.62
12539,Add Trillion 7B model support,juyoung-trl,2025-03-24T08:02:09Z,2025-03-25T01:01:11+00:00,1,16.98
12540,Fix clang warnings,yeahdongcn,2025-03-24T09:10:28Z,2025-03-24T10:28:35+00:00,1,1.3
12544,ggml : fix MUL_MAT_ID repack with Q8_K,ggerganov,2025-03-24T11:31:28Z,2025-03-26T11:02:00+00:00,2,47.51
12547,ci: [SYCL] Use main GPU and enable sysman,qnixsynapse,2025-03-24T13:34:33Z,2025-03-24T17:35:38+00:00,1,4.02
12554,vulkan: Add bfloat16 support,jeffbolznv,2025-03-24T21:16:11Z,2025-05-01T18:49:40+00:00,2,909.56
12556,Add Trillion 7B model support,juyoung-trl,2025-03-25T01:26:56Z,2025-03-30T18:38:33+00:00,5,137.19
12559,vulkan: Implement grouped query attention in the coopmat2 FA shader,jeffbolznv,2025-03-25T03:41:38Z,2025-04-02T17:40:32+00:00,1,205.98
12560,SYCL: disable Q4_0 reorder optimization by default,qnixsynapse,2025-03-25T04:48:07Z,2025-03-25T10:40:18+00:00,2,5.87
12563,docs : add build instructions for KleidiAI,eddnjjn,2025-03-25T08:33:34Z,2025-03-25T09:35:20+00:00,1,1.03
12566,clip: Fix llama-llava-clip-quantize-cli quantization error under CUDA backend,Ivy233,2025-03-25T10:08:42Z,2025-03-26T14:06:05+00:00,3,27.96
12571,convert: fix Mistral3/Gemma3 model hparams init,CISC,2025-03-25T14:50:58Z,2025-03-25T22:03:10+00:00,1,7.2
12573,convert : fix squeeze for ssm_conv tensors,ggerganov,2025-03-25T17:55:30Z,2025-03-26T12:21:05+00:00,1,18.43
12576,grammars: upgrade to llguidance 0.7.10,mmoskal,2025-03-25T23:34:39Z,2025-03-26T18:06:10+00:00,3,18.53
12580,SYCL: implement memset ggml backend buffer interface,qnixsynapse,2025-03-26T04:38:53Z,2025-03-27T01:46:00+00:00,9,21.12
12590,Fix T5Encoder model handling.,HighDoping,2025-03-26T12:17:54Z,2025-03-27T10:43:33+00:00,1,22.43
12593,llama : make loras compatible with repacking,ggerganov,2025-03-26T14:51:45Z,2025-03-27T06:24:10+00:00,1,15.54
12594,llamafile : ppc64le GEMV forwarding for FP32.,amritahs-ibm,2025-03-26T14:55:04Z,2025-03-28T07:43:23+00:00,1,40.81
12595,Support Qwen2_5_VLForConditionalGeneration,csabakecskemeti,2025-03-26T16:50:29Z,2025-03-27T10:11:24+00:00,1,17.35
12600,"opencl: add multi and vision rope, `gelu_quick` and `im2col`",lhez,2025-03-27T03:50:36Z,2025-03-27T15:08:08+00:00,1,11.29
12603,Include speculative decoding stats when timings_per_token is enabled,mostlygeek,2025-03-27T06:45:30Z,2025-03-28T08:05:44+00:00,5,25.34
12607,[CANN]: remove clang-format in ggml-cann,hipudding,2025-03-27T08:21:06Z,2025-03-29T10:03:29+00:00,2,49.71
12611,"musa: fix all warnings, re-enable `-DLLAMA_FATAL_WARNINGS=ON` in ci and update doc",yeahdongcn,2025-03-27T13:58:15Z,2025-03-30T08:59:39+00:00,12,67.02
12613,server : Support listening on a unix socket,p1-0tr,2025-03-27T15:33:47Z,2025-03-27T22:41:05+00:00,3,7.12
12615,llama : fix non-causal mask for gemma 3,ngxson,2025-03-27T17:10:40Z,2025-03-29T23:07:37+00:00,9,53.95
12621,Add Yandex instruct model template support,vorobyov01,2025-03-28T07:56:45Z,2025-03-30T18:12:03+00:00,3,58.26
12622,opencl: Add support for multiple devices,linehill,2025-03-28T08:20:34Z,2025-05-21T23:21:45+00:00,1,1311.02
12625,sycl: allow ggml-sycl configuration and compilation using Visual Studio project/solution,s-Nick,2025-03-28T10:55:54Z,2025-04-04T14:00:47+00:00,15,171.08
12627,vulkan: Implement split_k for coopmat2 flash attention.,jeffbolznv,2025-03-28T14:12:52Z,2025-04-02T19:25:08+00:00,1,125.2
12628,llama: fix error on bad grammar,JohannesGaessler,2025-03-28T15:24:33Z,2025-03-28T17:08:52+00:00,1,1.74
12630,vulkan: Hybrid waitForFences/getFenceStatus to reduce fence latency,jeffbolznv,2025-03-28T16:10:57Z,2025-04-04T05:54:35+00:00,2,157.73
12631,llama : fix incorrect Qwen2Moe ffn_moe_out graph callback,CISC,2025-03-28T20:02:34Z,2025-03-28T21:13:02+00:00,1,1.17
12632,change cpu_buft_list order: ACCEL -> GPU host -> CPU extra -> CPU,Djip007,2025-03-28T20:32:43Z,2025-03-29T13:07:37+00:00,2,16.58
12634,llama : support BailingMoE (Ling),CISC,2025-03-28T22:32:09Z,2025-03-30T20:21:03+00:00,3,45.81
12635,llama-server : implement universal assisted decoding,g2mt,2025-03-28T23:17:43Z,2025-07-31T12:25:23+00:00,9,2989.13
12640,llama-tts refactor console output,marcoStocchi,2025-03-29T07:43:16Z,2025-03-31T08:20:30+00:00,1,48.62
12649,opencl : fix memory allocation size,sparkleholic,2025-03-30T00:26:44Z,2025-04-01T16:54:34+00:00,1,64.46
12660,llava : fix clip loading GGUFs with missing description,CISC,2025-03-30T18:33:07Z,2025-03-31T09:07:07+00:00,1,14.57
12664,contrib: support modelscope community,tastelikefeet,2025-03-31T02:37:00Z,2025-04-11T12:01:57+00:00,13,273.42
12665,update `rope_multi`:,foldl,2025-03-31T03:39:12Z,2025-08-13T10:45:15+00:00,2,3247.1
12667,convert : Qwerky : use lora_rank_tokenshift and lora_rank_decay if present,CISC,2025-03-31T07:38:29Z,2025-03-31T14:36:25+00:00,1,6.97
12671,[CANN]get_rows and dup optimization,noemotiovon,2025-03-31T13:31:15Z,2025-04-02T07:22:13+00:00,6,41.85
12672,use LLM_KV instead of gguf_find_key,jklincn,2025-03-31T13:38:45Z,2025-04-01T12:54:28+00:00,1,23.26
12674,SYCL: switch to SYCL namespace,qnixsynapse,2025-03-31T16:04:15Z,2025-04-01T08:11:39+00:00,2,16.12
12677,vocab : BailingMoE : change possessive quantifiers to greedy,CISC,2025-03-31T20:43:37Z,2025-04-02T09:21:48+00:00,1,36.64
12681,gguf-split now respects dry-run option,nickhuang99,2025-03-31T23:29:27Z,2025-04-04T14:09:12+00:00,1,86.66
12683,vulkan: fix build when glslc doesn't support coopmat,wbruna,2025-04-01T01:17:05Z,2025-04-01T09:38:08+00:00,1,8.35
12686,Fix clang warning in gguf_check_reserved_keys,yeahdongcn,2025-04-01T02:39:48Z,2025-04-01T11:12:53+00:00,2,8.55
12687,convert : BailingMoE : fix qkv split when head_dim is 0,CISC,2025-04-01T06:32:27Z,2025-04-01T12:37:13+00:00,5,6.08
12694,"common : refactor downloading system, handle mmproj with -hf option",ngxson,2025-04-01T14:29:57Z,2025-04-01T21:44:06+00:00,1,7.24
12697,common : remove json.hpp from common.cpp,ngxson,2025-04-01T21:58:03Z,2025-04-02T07:58:34+00:00,1,10.01
12699,Sync minja patch to support inclusionAI/Ling-lite template and update tests,yeahdongcn,2025-04-02T02:33:59Z,2025-04-03T11:51:35+00:00,1,33.29
12702,opencl: update doc for OpenCL,lhez,2025-04-02T04:10:19Z,2025-04-04T05:18:18+00:00,1,49.13
12704,fix MUSA compiler warning,A3shTnT,2025-04-02T04:48:45Z,2025-04-03T07:32:55+00:00,2,26.74
12705,opencl: use `max_alloc_size` in backend ctx instead of querying again,lhez,2025-04-02T05:45:35Z,2025-04-03T00:01:42+00:00,1,18.27
12706,Use string_view::find() to search for tokenization to speed up,yumeyao,2025-04-02T06:30:32Z,2025-04-03T15:32:54+00:00,3,33.04
12708,CANN: Fix backend op fail,hipudding,2025-04-02T08:25:55Z,2025-04-03T00:49:51+00:00,2,16.4
12709,[CANN]Support operator SIN COS ARGMAX,noemotiovon,2025-04-02T08:28:02Z,2025-04-03T07:18:09+00:00,6,22.84
12711,model : print tensor size during load,ggerganov,2025-04-02T08:44:50Z,2025-04-02T13:38:54+00:00,1,4.9
12718,imatrix: add option to display importance score statistics for a given imatrix file,EAddario,2025-04-02T13:40:19Z,2025-07-22T12:33:38+00:00,18,2662.89
12719,cmake: remove caching from vulkan coopmat checks,bandoti,2025-04-02T14:37:05Z,2025-04-02T17:56:27+00:00,1,3.32
12720,vulkan: Use unclamped loads for flash attention mask,jeffbolznv,2025-04-02T15:16:12Z,2025-04-06T08:47:13+00:00,1,89.52
12721,vulkan: Fix missing cmake logic for dot product extension,jeffbolznv,2025-04-02T15:26:49Z,2025-04-03T15:08:26+00:00,9,23.69
12722,Vulkan: Fix mmq int dot float cache size,0cc4m,2025-04-02T15:29:20Z,2025-04-02T17:12:30+00:00,1,1.72
12725,DeepSeek V2/V3 with `-mla` option,jukofyork,2025-04-02T22:04:14Z,2025-04-05T12:54:26+00:00,5,62.84
12731,CANN: Refactor to reduce duplicate code,hipudding,2025-04-03T07:37:13Z,2025-04-07T09:10:36+00:00,11,97.56
12733,CANN: fix typo in ggml-cann,jeffzhou2000,2025-04-03T08:46:08Z,2025-04-07T11:34:15+00:00,1,98.8
12734,sycl:remove redundant memcopy in function ggml_backend_sycl_buffer_set_tensor,jeffzhou2000,2025-04-03T09:02:58Z,2025-04-07T15:22:58+00:00,5,102.33
12735,"Server webui: Upgrade daisyui, tailwindcss.",nauful,2025-04-03T11:10:33Z,2025-04-04T14:09:52+00:00,6,26.99
12738,CUDA: Prefer vector flash decoding kernel for Gemma models,gaugarg-nv,2025-04-03T12:59:29Z,2025-04-03T16:20:29+00:00,1,3.35
12739,sync: minja,ochafik,2025-04-03T14:42:53Z,2025-04-04T20:16:39+00:00,2,29.56
12744,vulkan: set cmake minimum and project name in vulkan-shaders,jeffbolznv,2025-04-03T21:45:08Z,2025-04-04T05:53:20+00:00,5,8.14
12747,Fix CMake Error for Vulkan with LLVM,hydroo,2025-04-04T08:55:19Z,2025-04-04T13:12:40+00:00,1,4.29
12749,Added all CPU to Docker GPU images for 'token_embd.weight' compatibility,rudiservo,2025-04-04T13:34:38Z,2025-04-09T23:17:12+00:00,2,129.71
12757,"clip : refactor clip_init, add tests",ngxson,2025-04-04T17:33:45Z,2025-04-05T15:17:40+00:00,5,21.73
12760,opencl: better identify Adreno GPU,lhez,2025-04-04T21:14:14Z,2025-04-07T20:22:54+00:00,1,71.14
12761,cmake : enable curl by default,ngxson,2025-04-04T21:56:05Z,2025-04-07T11:35:20+00:00,9,61.65
12766,Fix includes in arg.cpp and gemma3-cli.cpp,barracuda156,2025-04-05T06:54:27Z,2025-04-05T15:46:01+00:00,9,8.86
12767,Vulkan: Tune Vulkan mmq int dot shader for performance,0cc4m,2025-04-05T07:34:31Z,2025-04-05T16:04:04+00:00,1,8.49
12769,common: custom hf endpoint support,eternaphia,2025-04-05T09:15:19Z,2025-04-05T13:31:42+00:00,3,4.27
12773,ggml: use _mm[512/256]_dpbusd[_avx]_epi32 to directly accumulate into the result register,SongXiaoXi,2025-04-05T16:08:40Z,2025-04-14T05:47:55+00:00,7,205.65
12776,vulkan: fix NaN issue in flash attention shader,jeffbolznv,2025-04-06T06:33:09Z,2025-04-06T09:03:47+00:00,1,2.51
12783,vulkan: Use fp16 for the flash attention P*V multiplication,jeffbolznv,2025-04-06T18:04:31Z,2025-04-09T05:12:57+00:00,1,59.14
12785, Improve Chat Input with Auto-Sizing Textarea,characharm,2025-04-06T22:58:58Z,2025-04-08T09:14:59+00:00,2,34.27
12786,[CANN]Support Opt CONV_TRANSPOSE_1D and ELU,noemotiovon,2025-04-07T02:09:37Z,2025-04-09T06:04:14+00:00,9,51.91
12788,SYCL: Add fp16 type support to unary op kernels,qnixsynapse,2025-04-07T05:50:27Z,2025-04-11T08:03:50+00:00,16,98.22
12791,llama : Support llama 4 text-only,ngxson,2025-04-07T07:25:20Z,2025-04-07T21:06:45+00:00,25,13.69
12795,opencl: fix couple crashes,linehill,2025-04-07T10:49:45Z,2025-05-21T20:21:17+00:00,1,1065.53
12797,hellaswag: display estimated score confidence interval,stduhpf,2025-04-07T12:34:17Z,2025-04-07T15:47:08+00:00,1,3.21
12799,kv-cache : separate recurrent vs non-recurrent impl,ggerganov,2025-04-07T13:57:09Z,2025-05-02T14:48:36+00:00,13,600.86
12801,DeepSeek V2/V3 MLA implementation,jukofyork,2025-04-07T15:24:12Z,2025-04-15T06:49:57+00:00,4,183.43
12802,`server`: inject date_string in llama 3.x template + fix date for firefunction v2,ochafik,2025-04-07T16:28:10Z,2025-05-15T01:39:51+00:00,2,897.19
12804,ci: fix cross-compile sync issues,bandoti,2025-04-07T17:22:14Z,2025-05-01T22:06:39+00:00,9,580.74
12806,cuda : add f32 to bf16 copy op,CISC,2025-04-07T20:20:34Z,2025-04-08T21:21:32+00:00,1,25.02
12808,`common`: add partial regex support,ochafik,2025-04-07T23:19:12Z,2025-05-14T18:50:57+00:00,4,883.53
12809,gguf-py : support lazy tensor splitting,compilade,2025-04-07T23:33:26Z,2025-04-08T07:03:07+00:00,1,7.49
12812,"Revert ""sycl:remove redundant memcopy in function ggml_backend_sycl_buffer_set_tensor""",NeoZhangJianyu,2025-04-08T02:30:02Z,2025-04-08T07:03:22+00:00,3,4.56
12813,examples: fix coredump in llama-server after ctrl+c (#12180),tyronecai,2025-04-08T02:55:38Z,2025-04-08T16:36:57+00:00,2,13.69
12820,convert : ability to lazy-load safetensors remotely without downloading to disk,ngxson,2025-04-08T10:28:18Z,2025-04-10T15:24:45+00:00,12,52.94
12822,Include limits header file on AIX,mehendarkarprajwal,2025-04-08T10:33:51Z,2025-04-08T12:31:00+00:00,1,1.95
12824,llava: add more helper functions to check projector types,dm4,2025-04-08T10:39:24Z,2025-04-08T13:49:14+00:00,1,3.16
12825,llama : fix FA when KV cache is not used (i.e. embeddings),ggerganov,2025-04-08T10:52:27Z,2025-04-08T16:54:52+00:00,1,6.04
12827,ci: detach common from the library,pminev,2025-04-08T12:12:22Z,2025-04-09T08:11:11+00:00,2,19.98
12828,Support Qwen3 and Qwen3MoE,bozheng-hit,2025-04-08T12:52:40Z,2025-04-09T09:47:36+00:00,7,20.92
12829,Add AVX512 implementation of GEMM - Q4_Kx8,Srihari-mcw,2025-04-08T13:38:44Z,2025-04-15T06:22:36+00:00,1,160.73
12830,Fixes #12823,mehendarkarprajwal,2025-04-08T13:47:52Z,2025-04-09T23:18:01+00:00,2,33.5
12831,server : fix thread.join() on exit,ngxson,2025-04-08T14:18:08Z,2025-04-08T16:37:06+00:00,1,2.32
12832,clip : do not print ftype,ngxson,2025-04-08T14:28:38Z,2025-04-09T08:09:53+00:00,1,17.69
12833,"vulkan: In coopmat2 mmq, load q4_k/q5_k scales through shared memory",jeffbolznv,2025-04-08T15:36:23Z,2025-04-09T05:25:08+00:00,1,13.81
12834,llava: improve clip_ctx destructor to not memleak load_image_size,mattjcly,2025-04-08T17:18:23Z,2025-04-08T20:01:58+00:00,1,2.73
12839,musa: enable freediskspace for docker image build,yeahdongcn,2025-04-09T03:42:02Z,2025-04-09T09:22:30+00:00,1,5.67
12841,[CANN]Support Opt LOG && MEAN && PAD_REFLECT_1D && STEP ...,noemotiovon,2025-04-09T07:02:24Z,2025-04-10T00:51:52+00:00,1,17.82
12843,Llama-3_1-Nemotron-Ultra-253B-v1 support,ymcki,2025-04-09T08:04:03Z,2025-05-03T15:39:51+00:00,2,583.6
12848,ggml: fixes #12846 compilation error,taronaeo,2025-04-09T10:31:07Z,2025-04-11T05:20:07+00:00,4,42.82
12849,llava : introduce libmtmd,ngxson,2025-04-09T10:35:13Z,2025-04-10T20:57:16+00:00,13,34.37
12851,gguf-py: byteswapping improvements,AlekseiNikiforovIBM,2025-04-09T12:10:27Z,2025-08-28T08:56:42+00:00,1,3380.77
12853,vulkan: use aligned loads for flash attention mask,jeffbolznv,2025-04-09T14:36:16Z,2025-04-12T08:44:48+00:00,1,66.14
12854,xcf : add check for visionos build version,danbev,2025-04-09T17:41:43Z,2025-04-11T07:24:34+00:00,1,37.71
12855,ggml-impl.h: fix build on POWER9,pkubaj,2025-04-09T19:12:46Z,2025-04-09T23:00:25+00:00,1,3.79
12856,ggml-cpu-impl.h: do not redefine bool on POWER9,pkubaj,2025-04-09T19:16:37Z,2025-04-09T23:00:34+00:00,1,3.73
12858,sycl : implementation of reordered Q4_0 MMVQ for Intel GPUs ,Alcpz,2025-04-10T00:48:38Z,2025-05-09T15:34:08+00:00,44,710.76
12861,Replace freediskspace to free_disk_space in docker.yml,yeahdongcn,2025-04-10T07:06:46Z,2025-04-11T07:26:17+00:00,1,24.33
12864,CANN: Add support for async operator submission,hipudding,2025-04-10T07:52:31Z,2025-04-17T12:34:16+00:00,7,172.7
12865,[CANN]Opt ROPE optimization,noemotiovon,2025-04-10T07:54:57Z,2025-04-15T02:09:35+00:00,1,114.24
12867,glm4-4-0414 : add Glm4Model implementation for GLM-4-0414,zRzRzRzRzRzRzR,2025-04-10T08:43:50Z,2025-04-11T10:10:11+00:00,4,25.44
12868,opencl: fix incorrect local_size index in profiling log,kimminsu38oo,2025-04-10T09:05:02Z,2025-04-16T21:25:58+00:00,1,156.35
12869,clip : use smart pointer (⚠️ breaking change),ngxson,2025-04-10T09:21:29Z,2025-04-11T10:09:39+00:00,11,24.8
12870,convert : proper tensor name mapping for llama4,ngxson,2025-04-10T09:40:53Z,2025-04-11T07:23:37+00:00,1,21.71
12871,ggml : add SSE 4.2 and x64 base variant for CPUs without AVX,slaren,2025-04-10T10:00:21Z,2025-04-21T16:13:52+00:00,1,270.23
12873,SYCL: Support sycl_ext_oneapi_limited_graph,EwanC,2025-04-10T10:36:04Z,2025-04-11T13:32:14+00:00,10,26.94
12874,llama-bench: enhance benchmark with improved token throughput measurements ,thevishalagarwal,2025-04-10T11:03:48Z,2025-04-24T12:31:14+00:00,5,337.46
12875,[CANN] Optimize CANN buffer pool memory management,bachelor-dou,2025-04-10T11:23:35Z,2025-04-15T02:04:25+00:00,1,110.68
12882,llama : correct rms norm for llama 4,ngxson,2025-04-10T21:33:22Z,2025-04-11T06:49:51+00:00,1,9.27
12886,opencl: split `ggml-opencl.cl` into multiple files and cleanup,lhez,2025-04-11T04:43:44Z,2025-04-15T19:26:00+00:00,1,110.7
12887,SYCL: Add ROPE vision kernel,qnixsynapse,2025-04-11T05:12:36Z,2025-04-15T08:37:43+00:00,13,99.42
12891,ggml: disable CUDA graphs for unsupported DUP and CONT node types,agray3,2025-04-11T08:58:34Z,2025-04-13T21:12:22+00:00,1,60.23
12892,Define cache directory on FreeBSD,yurivict,2025-04-11T09:39:21Z,2025-04-11T19:45:44+00:00,3,10.11
12896,server : add VSCode's Github Copilot Chat support,ggerganov,2025-04-11T14:17:24Z,2025-04-11T20:37:41+00:00,2,6.34
12898, server : vision support via libmtmd,ngxson,2025-04-11T16:08:00Z,2025-05-09T17:29:38+00:00,37,673.36
12900,`tool-call`: fix non-tool-calling grammar crashes w/ Qwen / Hermes 2 templates,ochafik,2025-04-11T17:03:21Z,2025-04-11T19:47:52+00:00,1,2.74
12903,Set cache directory in rpc-server.cpp on FreeBSD,yurivict,2025-04-11T18:29:04Z,2025-04-11T20:04:15+00:00,1,1.59
12906,mtmd : add methods to access `mtmd_image_tokens`,ngxson,2025-04-11T20:20:50Z,2025-04-18T08:04:51+00:00,14,155.73
12907,llava: Fix cpu-only clip image encoding sefault,mattjcly,2025-04-11T20:32:30Z,2025-04-12T05:29:03+00:00,2,8.94
12910,SYCL: Fix im2col,qnixsynapse,2025-04-12T05:23:08Z,2025-04-14T12:23:53+00:00,5,55.01
12915,Define cache directory on AIX,mehendarkarprajwal,2025-04-12T12:13:11Z,2025-04-12T15:33:39+00:00,2,3.34
12922,llama-bench : Add `--override-tensors` arg,4onen,2025-04-12T18:15:19Z,2025-04-27T21:48:26+00:00,4,363.55
12924,feat: Add Clear All Conversations for llama-server web-ui ,characharm,2025-04-12T19:51:33Z,2025-04-14T12:40:30+00:00,4,40.82
12929,llava: add performance print for gemma3 example,Russyyds,2025-04-13T13:43:49Z,2025-04-14T17:18:21+00:00,1,27.58
12930,gguf-py : GGUF Editor GUI - Python + Qt,christopherthompson81,2025-04-13T15:20:19Z,2025-04-18T18:30:42+00:00,9,123.17
12931,vulkan: enable coopmat2 FA gqa and split_k optimizations more often,jeffbolznv,2025-04-13T16:22:49Z,2025-04-16T18:37:25+00:00,1,74.24
12934,CUDA/HIP: Share the same unified memory allocation logic.,hjc4869,2025-04-14T05:03:54Z,2025-04-15T09:20:38+00:00,8,28.28
12936,server : use std::move whenever possible,ngxson,2025-04-14T06:57:21Z,2025-04-18T17:58:13+00:00,11,107.01
12937,[SYCL]Disable curl lib check in examples/sycl/build.sh,NeoZhangJianyu,2025-04-14T07:16:31Z,2025-04-14T10:19:07+00:00,1,3.04
12938,rpc : use ggml_context_ptr,rgerganov,2025-04-14T08:41:19Z,2025-04-14T10:59:34+00:00,2,2.3
12943,rpc : do not wait for response when sending RPC_CMD_SET_TENSOR,rgerganov,2025-04-14T13:12:32Z,2025-04-25T07:08:09+00:00,1,257.93
12944,llava: n_patches for clip_image_u8,mattjcly,2025-04-14T14:55:23Z,2025-04-14T20:11:27+00:00,4,5.27
12950,CANN: Add x86 build ci,hipudding,2025-04-15T02:30:50Z,2025-04-15T11:08:56+00:00,1,8.63
12951,main : Fix Ctrl+D/newline handling,danielzgtg,2025-04-15T03:09:34Z,2025-04-18T20:02:55+00:00,3,88.89
12955,rpc : add RPC_CMD_HELLO,rgerganov,2025-04-15T08:58:21Z,2025-04-18T07:13:43+00:00,2,70.26
12957," Resolved half rope,multi-EOS issues in convert_hf_togguf.py for GLM4Z Model",piDack,2025-04-15T10:06:53Z,2025-04-23T07:32:32+00:00,13,189.43
12962,[CANN]310P OPT Support,noemotiovon,2025-04-15T13:16:57Z,2025-04-16T08:21:05+00:00,1,19.07
12970,ggml: Re-enable CUDA graphs in presence of CONT and DUP nodes,agray3,2025-04-16T08:47:04Z,2025-04-17T13:19:42+00:00,1,28.54
12972,sycl: use oneDNN for matrices multiplication,lslusarczyk,2025-04-16T11:34:32Z,2025-05-15T14:53:41+00:00,15,699.32
12975,SYCL: Refactor and enable FP16 in binary broadcast OPs,qnixsynapse,2025-04-16T13:44:50Z,2025-04-18T13:57:56+00:00,10,48.22
12988,Recognize IBM Granite 3.3 FIM tokens. Makes llama-server /infill usable.,Noeda,2025-04-17T01:33:10Z,2025-04-17T08:37:06+00:00,1,7.07
12993,SYCL: Add non-contiguous support in ROPE,qnixsynapse,2025-04-17T05:38:49Z,2025-04-21T13:43:30+00:00,1,104.08
12994,[CANN] Add the n_graph_splits performance metric to llama-bench.,bachelor-dou,2025-04-17T08:20:44Z,2025-04-23T07:32:52+00:00,5,143.2
12995,"threading: support for GGML_SCHED_PRIO_LOW, update thread info on Windows to avoid throttling",max-krasnyansky,2025-04-17T16:37:36Z,2025-05-31T22:39:20+00:00,3,1062.03
13002,make memset range dynamic,pockers21,2025-04-18T05:52:14Z,2025-04-28T10:29:10+00:00,3,244.62
13003,[SYCL][OPT] Fix reorder optimization for Q4_0,NeoZhangJianyu,2025-04-18T06:16:08Z,2025-04-25T09:37:51+00:00,6,171.36
13011,"clip : refactor, add `image_manipulation` and `llava_uhd` classes",ngxson,2025-04-18T12:46:50Z,2025-04-19T07:15:45+00:00,1,18.48
13012,"mtmd : merge llava, gemma3 and minicpmv CLI into single `llama-mtmd-cli`",ngxson,2025-04-18T17:58:24Z,2025-04-21T13:32:58+00:00,4,67.58
13014,CUDA: noncont MMVQ + batched bs1 MUL_MAT_ID,JohannesGaessler,2025-04-18T20:38:44Z,2025-04-22T19:27:40+00:00,4,94.82
13016,vulkan: matmul gcn tuning,netrunnereve,2025-04-18T21:39:06Z,2025-04-24T07:18:33+00:00,3,129.66
13020,gguf-py : fix upload python package workflow,CISC,2025-04-19T11:07:13Z,2025-04-19T14:26:38+00:00,1,3.32
13021,"Append mult-eos,half-rope,bos to GLM4-0414 and Z",piDack,2025-04-19T12:47:24Z,2025-04-23T14:59:14+00:00,2,98.2
13022,Disable CI cross-compile builds,bandoti,2025-04-19T13:11:22Z,2025-04-19T16:05:03+00:00,1,2.89
13023,convert : experimental support for `--mmproj` flag,ngxson,2025-04-19T13:43:54Z,2025-04-20T21:29:37+00:00,15,31.76
13029,metal: add neg operator,jmorganca,2025-04-20T04:20:16Z,2025-04-20T05:28:40+00:00,1,1.14
13030,llava: fix errors in clip.h on certain pure C compilers,jmorganca,2025-04-20T04:28:04Z,2025-04-20T10:15:41+00:00,1,5.79
13031,vulkan: support noncontiguous rms_norm,jeffbolznv,2025-04-20T04:44:41Z,2025-04-20T08:50:02+00:00,1,4.09
13033,quantize: improve pattern matching for allowed tensors,EAddario,2025-04-20T09:08:09Z,2025-05-13T17:12:31+00:00,5,560.07
13036,gguf-py : avoid requiring PySide6 for packaged scripts,compilade,2025-04-20T20:39:39Z,2025-05-06T02:27:31+00:00,1,365.8
13037,quantize: Handle user-defined pruning of whole layers (blocks),EAddario,2025-04-20T21:05:28Z,2025-06-22T21:16:27+00:00,18,1512.18
13042,[CANN]Support OP MUL_MAT_ID,noemotiovon,2025-04-21T08:06:06Z,2025-05-19T06:21:17+00:00,4,670.25
13050,mtmd : support SmolVLM (version 1 and 2),ngxson,2025-04-21T14:57:59Z,2025-04-22T14:24:54+00:00,2,23.45
13053,ggml-cpu: Integrate fp32=bf16xbf16 SME KleidiAI kernel,eddnjjn,2025-04-21T18:41:30Z,2025-05-12T11:06:20+00:00,10,496.41
13055,llava : update documentations,ngxson,2025-04-21T20:22:10Z,2025-04-22T08:37:01+00:00,1,12.25
13060,rpc : add command line option for number of threads for the CPU backend,rgerganov,2025-04-22T09:01:43Z,2025-04-23T07:32:49+00:00,1,22.52
13061,security : add note about RPC and server functionality,ggerganov,2025-04-22T09:04:23Z,2025-04-22T13:16:10+00:00,2,4.2
13062,cmake : do not include ./src as public for libllama,ggerganov,2025-04-22T10:17:56Z,2025-04-24T13:00:10+00:00,1,50.7
13065,mtmd : Support Pixtral 12B,ngxson,2025-04-22T14:20:03Z,2025-04-23T18:21:59+00:00,7,28.03
13069,fix(rpc): Improve input validation and error handling,thevilledev,2025-04-22T14:58:47Z,2025-04-28T18:00:20+00:00,13,147.03
13074,SYCL: Add all missing unary kernels,qnixsynapse,2025-04-23T04:27:37Z,2025-04-28T09:33:25+00:00,11,125.1
13080,llama-mtmd-cli: Sigint rework in mtmd vision example,pl752,2025-04-23T16:14:39Z,2025-04-23T21:32:35+00:00,4,5.3
13081,clip : remove boi/eoi embeddings for GLM-edge model (⚠️ breaking change),ngxson,2025-04-23T21:07:12Z,2025-04-24T20:17:05+00:00,1,23.16
13082,arg : clean up handling --mmproj with -hf,ngxson,2025-04-23T22:03:36Z,2025-04-24T10:14:13+00:00,1,12.18
13088,server : fix coredump on std::terminate() #12831,tyronecai,2025-04-24T01:22:06Z,2025-04-24T13:49:22+00:00,1,12.45
13091,clang-tidy : disable warning about missing math parenthesis,ggerganov,2025-04-24T06:28:53Z,2025-04-24T12:44:05+00:00,1,6.25
13093,arg : add --no-mmproj-offload,ngxson,2025-04-24T08:30:46Z,2025-04-24T12:04:14+00:00,2,3.56
13095,CUDA: use switch statements in constexpr functions,JohannesGaessler,2025-04-24T12:16:53Z,2025-04-24T13:57:10+00:00,1,1.67
13096,llama-bench: add `-d` depth arg,thevishalagarwal,2025-04-24T12:30:20Z,2025-04-28T14:50:39+00:00,9,98.34
13097,clip : fix pixtral on some GPU backends,ngxson,2025-04-24T13:47:52Z,2025-04-25T12:31:42+00:00,2,22.73
13099,fix wrong template in GLM4-0414,matteoserva,2025-04-24T15:36:45Z,2025-04-27T18:10:00+00:00,2,74.55
13101,Force FP32 compute in GLM4 FFN Down,city96,2025-04-24T20:59:48Z,2025-04-25T12:38:34+00:00,3,15.65
13104,[CANN] Simplify the environment variable setting for GGML_CANN_MEM_POOL and GGML_CANN_ASYNC_MODE,bachelor-dou,2025-04-25T08:28:54Z,2025-06-09T11:47:39+00:00,6,1083.31
13107,ggml: move fp16/bf16 conversion optimizations to CPU backend + export conversion APIs,SongXiaoXi,2025-04-25T11:08:42Z,2025-04-26T14:05:31+00:00,1,26.95
13108,context : allow cache-less context for embeddings,ggerganov,2025-04-25T11:15:32Z,2025-05-08T11:28:33+00:00,4,312.22
13109,sycl : Implemented reorder Q4_K mmvq,sgeor255,2025-04-25T12:13:55Z,2025-05-15T15:35:44+00:00,34,483.36
13117,grammar : handle maxItems == 0 in JSON schema (#13116),rick-github,2025-04-25T19:39:03Z,2025-04-26T08:10:20+00:00,1,12.52
13118,clip : improve projector naming,ngxson,2025-04-25T22:18:00Z,2025-04-26T20:39:47+00:00,2,22.36
13122,convert : improve model arch handling,ngxson,2025-04-26T09:28:18Z,2025-04-30T14:56:24+00:00,14,101.47
13123,common : add common_remote_get_content,ngxson,2025-04-26T10:01:23Z,2025-04-26T20:58:12+00:00,1,10.95
13125,Added Quickstart section to README,DarkWanderer,2025-04-26T12:19:14Z,2025-04-26T14:58:31+00:00,1,2.65
13129,musa: fix build warning,yeahdongcn,2025-04-27T03:04:26Z,2025-04-27T11:22:49+00:00,1,8.31
13135,CUDA: build archs as virtual for GGML_NATIVE=OFF,JohannesGaessler,2025-04-27T13:24:45Z,2025-05-06T21:35:51+00:00,1,224.19
13136,clip : refactor set input for cgraph + fix qwen2.5vl input,ngxson,2025-04-27T15:01:02Z,2025-04-28T10:18:59+00:00,2,19.3
13137,CUDA: fix q_nope_absorbed precision for Deepseek 2 Lite f16,JohannesGaessler,2025-04-27T15:08:48Z,2025-04-28T07:29:26+00:00,1,16.34
13138,llama : (mrope) allow using normal 1D position for text token,ngxson,2025-04-27T16:25:27Z,2025-04-28T12:20:56+00:00,3,19.92
13139,mtmd : fix glm-edge redundant token count,ngxson,2025-04-27T17:40:58Z,2025-04-28T14:12:56+00:00,1,20.53
13140,fix wrong template in GLM4-0414,matteoserva,2025-04-27T18:24:09Z,2025-04-27T19:57:32+00:00,6,1.56
13141,mtmd : add qwen2vl and qwen2.5vl,ngxson,2025-04-27T18:47:24Z,2025-04-29T09:47:04+00:00,4,38.99
13142,arg : fix unused variable,ngxson,2025-04-27T19:38:56Z,2025-04-28T05:16:59+00:00,1,9.63
13143,llama-chat : fix typo GML --> GLM,ngxson,2025-04-27T19:59:57Z,2025-04-28T08:11:58+00:00,1,12.2
13144,musa: fix typo in cc control,yeahdongcn,2025-04-28T03:14:55Z,2025-04-28T07:33:29+00:00,1,4.31
13148,PowerPC: Enable MMA for BF16 in llamafile_sgemm,shalinib-ibm,2025-04-28T06:57:09Z,2025-05-02T16:53:12+00:00,1,105.93
13149,musa: add support for muBLAS and MMA,yeahdongcn,2025-04-28T07:17:50Z,2025-05-21T12:52:20+00:00,10,557.58
13150,readme : update hot topics,ggerganov,2025-04-28T07:48:25Z,2025-04-28T09:10:18+00:00,1,1.36
13152,delete buffer clear as suggested,pockers21,2025-04-28T10:15:56Z,2025-04-28T13:45:41+00:00,1,3.5
13153,clip : fix model size display,ngxson,2025-04-28T15:19:44Z,2025-04-28T19:23:20+00:00,1,4.06
13155,CUDA: fix non-cont. inputs for batched mat mul,JohannesGaessler,2025-04-28T17:21:03Z,2025-04-29T14:00:28+00:00,1,20.66
13159,llama-graph : fix text position for mrope,ngxson,2025-04-28T22:04:54Z,2025-04-29T06:45:49+00:00,1,8.68
13162,[CANN] Update CANN model support status,bachelor-dou,2025-04-29T02:08:18Z,2025-05-20T03:43:43+00:00,3,505.59
13173,sampling : when top-k <= 0 -> noop,ggerganov,2025-04-29T07:02:09Z,2025-04-29T17:22:58+00:00,2,10.35
13174,Prefilling assistant message in openai compatible API,matteoserva,2025-04-29T07:22:20Z,2025-04-29T18:33:11+00:00,12,11.18
13175,llama : set qwen3 model type sizes,CISC,2025-04-29T07:38:53Z,2025-04-29T09:00:31+00:00,5,1.36
13176,Fix for issue #13170,shalinib-ibm,2025-04-29T08:51:56Z,2025-04-30T11:17:08+00:00,2,26.42
13177,llama : llm_type order by size,CISC,2025-04-29T09:12:38Z,2025-04-29T11:25:53+00:00,2,2.22
13182,ggml-cpu: enable z17 compile detection,taronaeo,2025-04-29T12:38:47Z,2025-04-30T09:47:35+00:00,1,21.15
13183,llama-bench: fixed size of fields to correctly map to values,Alcpz,2025-04-29T13:43:17Z,2025-04-29T15:24:36+00:00,2,1.69
13184,mtmd : add C public API,ngxson,2025-04-29T13:51:55Z,2025-05-04T21:43:42+00:00,8,127.86
13187,test: non-cont. b in test-backend-ops -o MUL_MAT,JohannesGaessler,2025-04-29T14:30:40Z,2025-05-01T18:18:56+00:00,1,51.8
13188,rpc : fix cache directory initialization,hbuxiaofei,2025-04-29T14:31:49Z,2025-04-30T06:29:22+00:00,1,15.96
13191,vulkan: Handle src1 batch dimension in non-contiguous mat-vec-mul shader,jeffbolznv,2025-04-29T15:07:42Z,2025-05-01T18:19:31+00:00,1,51.2
13193,vulkan: use uint array index to avoid glslang bug,jeffbolznv,2025-04-29T16:19:51Z,2025-04-30T12:38:37+00:00,1,20.31
13194,kv-cache : add SWA support,ggerganov,2025-04-29T18:08:56Z,2025-05-20T05:05:47+00:00,11,490.95
13196,"Support jinja extra template kwargs (Qwen3 enable_thinking feature), from command line and from client",matteoserva,2025-04-29T18:58:24Z,2025-06-29T18:02:53+00:00,19,1463.07
13199,"CUDA: batched+noncont MMQ, refactor bs>1 MoE code",JohannesGaessler,2025-04-29T20:34:06Z,2025-04-30T21:12:59+00:00,1,24.65
13202,arg : allow using -hf offline,ngxson,2025-04-29T22:40:35Z,2025-04-30T08:46:32+00:00,1,10.1
13207,examples: remove the unnecessary include,tattn,2025-04-30T10:51:20Z,2025-04-30T13:25:20+00:00,1,2.57
13209,convert : converting mmproj for Qwen2/2.5VL from convert_hf_to_gguf,ngxson,2025-04-30T11:09:30Z,2025-05-02T15:17:15+00:00,6,52.13
13210,rpc: avoid uninitialized memory in serialize_tensor,justinsb,2025-04-30T13:35:26Z,2025-05-01T21:32:11+00:00,1,31.95
13214,"Support start strings, the opposite of stop tokens.",matteoserva,2025-04-30T16:23:17Z,2025-08-05T12:20:00+00:00,7,2323.95
13216,convert : fix context length for nomic-embed-text-v2-moe,cebtenzzre,2025-04-30T16:47:27Z,2025-05-02T15:41:54+00:00,1,46.91
13219,arg : -hf do not fail if url mismatch,ngxson,2025-04-30T17:21:33Z,2025-04-30T20:29:16+00:00,1,3.13
13222,ggml: Don't assert fail when tensor data changes,jessegross,2025-04-30T21:39:54Z,2025-05-01T20:46:10+00:00,1,23.1
13223,llama-model : fix the reported size class for nomic-embed-text-v2-moe,cebtenzzre,2025-04-30T21:41:00Z,2025-05-01T07:09:41+00:00,1,9.48
13228,arg : remove CURLINFO_EFFECTIVE_METHOD,ngxson,2025-05-01T06:59:55Z,2025-05-01T08:23:26+00:00,1,1.39
13230,server : add cache reuse card link to help,ggerganov,2025-05-01T07:39:00Z,2025-05-02T06:48:31+00:00,2,23.16
13231,mtmd : add **vision** support for Mistral Small 3.1,ngxson,2025-05-01T10:14:57Z,2025-05-01T15:05:42+00:00,3,4.85
13237,Re-enable upscaling of images smaller than the CLIP input size; fix MiniCPM evaluation on small bitmaps,lcarrere,2025-05-01T15:36:27Z,2025-05-01T19:32:21+00:00,1,3.93
13238,update GLM4 chat template,matteoserva,2025-05-01T16:21:15Z,2025-05-01T19:16:39+00:00,2,2.92
13239,build : fix build info on windows,slaren,2025-05-01T18:08:08Z,2025-05-01T19:48:08+00:00,1,1.67
13244,mtmd-cli : fix out_of_range when input image path is empty,ahmedshakill,2025-05-01T20:01:02Z,2025-05-02T08:20:27+00:00,13,12.32
13245,llama-model : support Qwen2 embedding models and pooling_mode_lasttoken,cebtenzzre,2025-05-01T20:03:35Z,2025-05-02T15:42:30+00:00,10,19.65
13246,convert : explicitly disable trust_remote_code for AutoConfig,ngxson,2025-05-01T20:12:36Z,2025-05-02T06:45:10+00:00,1,10.54
13249,llama : move end-user examples to tools directory,slaren,2025-05-01T22:47:17Z,2025-05-02T18:27:13+00:00,1,19.67
13253,reset glmedge chat template,piDack,2025-05-02T03:13:04Z,2025-05-02T09:06:10+00:00,2,5.88
13254,SYCL: Disable reorder optimize by default and stop setting tensor extras when optimize is disabled,qnixsynapse,2025-05-02T03:36:18Z,2025-05-06T14:57:06+00:00,1,107.35
13257,opencl: remove unnecessary assert for `add`,lhez,2025-05-02T07:03:30Z,2025-05-12T20:13:49+00:00,1,253.17
13259,clip : revert the change of BOI/EOI token for GLM-edge (⚠️ breaking change),ngxson,2025-05-02T09:15:35Z,2025-05-03T18:07:54+00:00,1,32.87
13260,llama : plamo rope type is neox,CISC,2025-05-02T09:28:04Z,2025-05-02T10:40:56+00:00,1,1.21
13263,cmake: simplify vulkan shader test logic,bandoti,2025-05-02T12:23:27Z,2025-05-14T10:53:57+00:00,1,286.51
13264,sampling: Integrate Top-nσ into main sampling chain (and add it to the server),oobabooga,2025-05-02T14:31:48Z,2025-05-05T20:12:19+00:00,3,77.68
13266,"vulkan: Additional type support for unary, binary, and copy",jeffbolznv,2025-05-02T15:07:25Z,2025-05-04T05:17:16+00:00,1,38.16
13267,context : fix reorder logic,ggerganov,2025-05-02T15:59:01Z,2025-05-02T17:54:14+00:00,1,1.92
13269,Model: Granite MoE shared,gabe-l-hart,2025-05-02T16:20:23Z,2025-05-13T13:12:01+00:00,12,260.86
13276,feat: Hybrid unified/recurrent cache,gabe-l-hart,2025-05-02T23:08:10Z,2025-05-29T22:17:54+00:00,7,647.16
13282,mtmd : add vision support for llama 4,ngxson,2025-05-03T14:54:09Z,2025-05-19T11:04:15+00:00,5,380.17
13283,examples : remove infill,ggerganov,2025-05-03T15:43:40Z,2025-05-07T07:28:02+00:00,1,87.74
13284,context : remove logits_all flag,ggerganov,2025-05-03T16:23:31Z,2025-05-08T11:26:50+00:00,2,115.06
13286,imatrix: fix oob writes if src1 is not contiguous,JohannesGaessler,2025-05-03T17:04:21Z,2025-05-03T22:50:37+00:00,3,5.77
13290,clip :  fix confused naming ffn_up and ffn_down,ngxson,2025-05-03T21:37:54Z,2025-05-05T10:54:44+00:00,1,37.28
13291,cuda: refactored ssm_scan and use CUB,Your-Cheese,2025-05-04T02:11:59Z,2025-08-09T18:29:44+00:00,11,2344.3
13294,CUDA: fix race condition in MMQ ids_dst,JohannesGaessler,2025-05-04T08:38:02Z,2025-05-04T11:58:38+00:00,1,3.34
13296,llama : deci : support ffn-free with attention,CISC,2025-05-04T10:41:57Z,2025-05-07T10:49:28+00:00,1,72.13
13301,ggml-cpu: Support Q3_K SIMD on s390x,taronaeo,2025-05-04T13:00:29Z,2025-05-04T17:49:12+00:00,1,4.81
13303,llava/mtmd : fixes to fully support dl backends,slaren,2025-05-04T13:09:37Z,2025-05-04T15:05:20+00:00,1,1.93
13304,"rpc : use backend registry, support dl backends",slaren,2025-05-04T14:37:04Z,2025-05-04T19:25:44+00:00,2,4.81
13306,CUDA: FA support for Deepseek (Ampere or newer),JohannesGaessler,2025-05-04T17:15:17Z,2025-05-09T11:34:58+00:00,6,114.33
13308,SYCL: Disable mul_mat kernels for noncontiguous tensor b,qnixsynapse,2025-05-05T02:49:42Z,2025-05-05T08:09:10+00:00,1,5.32
13309,Webui - change setText command from parent window to also send the message.,igardev,2025-05-05T04:58:11Z,2025-05-05T14:03:31+00:00,5,9.09
13311,mtmd : rename llava directory to mtmd,ngxson,2025-05-05T09:54:33Z,2025-05-05T14:02:55+00:00,2,4.14
13320,CUDA: fix logic for clearing padding with -ngl 0,JohannesGaessler,2025-05-05T18:22:10Z,2025-05-05T20:32:14+00:00,2,2.17
13321,clip : refactor graph builder,ngxson,2025-05-05T19:52:42Z,2025-05-06T20:40:24+00:00,1,24.8
13323,CUDA: fix --split-mode row for MMQ,JohannesGaessler,2025-05-05T21:45:37Z,2025-05-06T06:36:47+00:00,1,8.85
13324,vulkan: scalar flash attention implementation,jeffbolznv,2025-05-06T00:53:11Z,2025-05-10T06:07:07+00:00,4,101.23
13325,musa: remove nrows_x in mul_mat_q_process_tile,yeahdongcn,2025-05-06T01:37:59Z,2025-05-07T07:48:23+00:00,1,30.17
13326,vulkan: Allow up to 4096 elements for mul_mat_id row_ids,jeffbolznv,2025-05-06T02:51:47Z,2025-05-09T07:23:41+00:00,1,76.53
13328,Support tie embedding for chatglm models,piDack,2025-05-06T03:48:11Z,2025-05-07T07:23:11+00:00,2,27.58
13330,common: Warn when we can't match samplers for a sampler sequence.,ycros,2025-05-06T05:58:28Z,2025-05-07T08:23:28+00:00,1,26.42
13331,convert : qwen2/3moe : set yarn metadata if present,CISC,2025-05-06T06:58:55Z,2025-05-06T09:12:06+00:00,2,2.22
13336,llama : fix build_ffn without gate,ngxson,2025-05-06T08:54:57Z,2025-05-06T12:25:40+00:00,1,3.51
13337,CUDA: fix bad asserts for partial offload,JohannesGaessler,2025-05-06T09:26:23Z,2025-05-06T11:58:52+00:00,1,2.54
13342,cmake : remove arm64 msvc presets,slaren,2025-05-06T17:08:23Z,2025-05-06T18:15:32+00:00,1,1.12
13343,sycl: addressing non-contiguous src1 mul_mats (nc and batched),Alcpz,2025-05-06T17:09:45Z,2025-05-08T09:08:01+00:00,4,39.97
13344,sampling: Don't consider -infinity values in top_n_sigma,oobabooga,2025-05-06T17:10:08Z,2025-05-06T18:24:15+00:00,1,1.24
13345,sampling: make top_n_sigma no-op at <=0 rather than <0,DocShotgun,2025-05-06T17:18:48Z,2025-05-06T20:36:24+00:00,2,3.29
13349,convert : support rope_scaling type and rope_type,CISC,2025-05-07T07:16:40Z,2025-05-08T13:34:29+00:00,1,30.3
13353,rpc : add rpc_msg_set_tensor_hash_req,rgerganov,2025-05-07T12:43:07Z,2025-05-09T07:31:08+00:00,2,42.8
13357,SYCL: Fix test-backend-ops crashes with SYCL-Graph,EwanC,2025-05-07T14:24:34Z,2025-05-16T11:10:07+00:00,1,212.76
13362,ci : move release workflow to a separate file,slaren,2025-05-07T17:39:11Z,2025-05-08T11:15:28+00:00,2,17.6
13364,llama : print size and type of overridden tensors,slaren,2025-05-07T19:17:08Z,2025-05-08T11:15:15+00:00,2,15.97
13365,"server : (webui) revamp the input area, plus many small UI improvements",ngxson,2025-05-07T21:02:15Z,2025-05-08T13:37:30+00:00,11,16.59
13366,mtmd: Expose helper_decode_image_chunk,mattjcly,2025-05-07T21:54:25Z,2025-05-08T18:25:39+00:00,15,20.52
13370,llama-run: add support for downloading models from ModelScope,yeahdongcn,2025-05-08T01:24:24Z,2025-05-09T09:25:51+00:00,2,32.02
13381,mtmd: Fix the calculation of n_tokens for smolvlm,awkrail,2025-05-08T09:32:10Z,2025-05-08T13:03:54+00:00,1,3.53
13382,musa: restore MUSA graph settings in CMakeLists.txt,yeahdongcn,2025-05-08T09:45:45Z,2025-07-14T10:43:04+00:00,1,1608.96
13383,sycl: simplify bin_bcast_kernel,AD2605,2025-05-08T10:41:02Z,2025-05-15T15:39:52+00:00,9,172.98
13384,CUDA: fix crash on large batch size for MoE models,JohannesGaessler,2025-05-08T14:24:45Z,2025-05-09T10:14:04+00:00,1,19.82
13386,Add `--no-op-offload` to improve `-ot` pp perf in MoE models like llama4 400B,hjc4869,2025-05-08T15:32:44Z,2025-05-11T12:18:39+00:00,5,68.77
13389,Add --parse-special for enabling parsing of special tokens in imatrix calculation,bartowski1182,2025-05-08T19:33:00Z,2025-05-09T09:53:58+00:00,1,14.35
13393,server : (webui) rename has_multimodal --> modalities,ngxson,2025-05-08T21:55:38Z,2025-05-09T07:06:37+00:00,3,9.18
13395,llama : do not crash if there is no CPU backend,slaren,2025-05-08T22:39:43Z,2025-05-09T11:02:08+00:00,2,12.37
13396,vulkan: enable fp16 for gcn 3 and 4 chips,netrunnereve,2025-05-09T03:46:14Z,2025-05-10T00:33:33+00:00,2,20.79
13397,mtmd : fix batch_view for m-rope,ngxson,2025-05-09T07:40:29Z,2025-05-09T09:18:03+00:00,1,1.63
13398,llama : one-off chat template fix for Mistral-Small-2503,ngxson,2025-05-09T08:03:20Z,2025-05-09T09:17:51+00:00,1,1.24
13406,sycl: enable dpcpp nightly builds with oneMKL and oneDNN,AD2605,2025-05-09T14:08:41Z,2025-05-12T05:15:32+00:00,5,63.11
13410,llama-bench : accept ranges for integer parameters,slaren,2025-05-09T17:17:03Z,2025-05-12T11:08:22+00:00,4,65.86
13413,Use tagged version of llguidance that does not break the build,HRKings,2025-05-09T18:50:17Z,2025-05-09T20:15:40+00:00,1,1.42
13415,CUDA: fix FlashAttention on Turing,JohannesGaessler,2025-05-09T21:35:15Z,2025-05-10T07:16:52+00:00,1,9.69
13416,arg : add env var to control mmproj,ngxson,2025-05-09T21:40:09Z,2025-05-10T06:16:30+00:00,1,8.61
13422,mtmd : support InternVL 2.5 and 3,ngxson,2025-05-10T10:19:45Z,2025-05-10T14:26:42+00:00,4,4.12
13423,vocab : add ByteDance-Seed/Seed-Coder,CISC,2025-05-10T11:09:10Z,2025-05-10T20:08:08+00:00,1,8.98
13424,llguidance : init tokenizer slices,CISC,2025-05-10T11:42:50Z,2025-05-10T15:19:52+00:00,1,3.62
13426,ci: `free_disk_space` flag enabled for intel variant,Thammachart,2025-05-10T12:24:11Z,2025-05-10T14:34:48+00:00,2,2.18
13429,Webui dynamic config,ServeurpersoCom,2025-05-10T15:10:18Z,2025-09-01T06:48:58+00:00,1,2727.64
13435,"CUDA: faster Deepseek FA, add Turing support",JohannesGaessler,2025-05-10T17:15:59Z,2025-05-14T14:08:21+00:00,1,92.87
13436,tools : fix invalid free(),aumfer,2025-05-10T18:11:39Z,2025-05-11T15:08:26+00:00,3,20.95
13438,CUDA: fix race conditions in FlashAttention kernels,JohannesGaessler,2025-05-10T19:09:38Z,2025-05-10T20:22:48+00:00,1,1.22
13439,CUDA: fix crash with partial offloading of MoE,JohannesGaessler,2025-05-10T19:49:19Z,2025-05-11T14:09:33+00:00,1,18.34
13442,mtmd : move helpers to dedicated file,ngxson,2025-05-10T21:00:17Z,2025-05-11T09:34:23+00:00,1,12.57
13443,mtmd : support InternVL 3 38B and 78B mmproj,city96,2025-05-10T23:36:24Z,2025-05-11T09:35:52+00:00,1,9.99
13451,scripts : exit compare-llama-bench.py gracefully when there's nothing to compare,CISC,2025-05-11T10:44:44Z,2025-05-11T14:20:39+00:00,1,3.6
13455,scripts : support arbitrary input file formats in compare-llama-bench.py,CISC,2025-05-11T14:29:12Z,2025-05-13T13:31:13+00:00,12,47.03
13457,ggml : add mrope kernel for metal,ngxson,2025-05-11T20:33:54Z,2025-05-12T08:29:14+00:00,1,11.92
13459,mtmd : use RMS norm for InternVL 3 38B and 78B mmproj,city96,2025-05-11T21:33:51Z,2025-05-11T22:39:06+00:00,1,1.09
13460,"mtmd : remove libllava, remove clip-quantize-cli (⚠️ breaking change)",ngxson,2025-05-11T22:33:06Z,2025-05-13T13:33:59+00:00,1,39.01
13469,CUDA: fix misaligned synchronization in FA,JohannesGaessler,2025-05-12T07:29:04Z,2025-05-12T08:51:21+00:00,1,1.37
13470,context : fix state io for memory-less contexts,ggerganov,2025-05-12T07:36:05Z,2025-05-12T12:12:27+00:00,2,4.61
13472,Support Seed-Coder chat template,yeahdongcn,2025-05-12T07:47:21Z,2025-07-14T10:43:57+00:00,6,1514.94
13477,Allow content null for tool call,anudit,2025-05-12T10:37:54Z,2025-05-12T11:56:42+00:00,1,1.31
13478,clip : cap max image size 1024 for qwen vl model,ngxson,2025-05-12T10:40:52Z,2025-05-12T13:06:51+00:00,1,2.43
13482,sycl : Overcoming workaround for mmap() allocation on Windows,s-Nick,2025-05-12T13:04:37Z,2025-05-20T00:54:44+00:00,9,179.84
13487,"llama-bench : add defrag-thold, check for invalid ranges",slaren,2025-05-12T19:48:27Z,2025-05-12T22:31:37+00:00,1,2.72
13503,sycl: Use oneMath on non-WIN32,jounjj,2025-05-13T11:06:31Z,2025-05-13T14:32:42+00:00,1,3.44
13506,vulkan: KHR_coopmat flash attention,jeffbolznv,2025-05-13T12:42:17Z,2025-05-14T09:55:26+00:00,2,21.22
13509,ggml-cpu: Update KleidiAI to v1.6 and fix include directives,eddnjjn,2025-05-13T13:55:35Z,2025-05-13T15:02:28+00:00,1,1.11
13510,clip : clip.h become private API (⚠️ breaking change),ngxson,2025-05-13T13:59:21Z,2025-05-13T15:07:21+00:00,1,1.13
13511,server: Allow pasting file from clipboard,luca020400,2025-05-13T14:56:39Z,2025-05-14T06:54:56+00:00,1,15.97
13513,docs: Update link to ggml-org in multimodal.md,ddpasa,2025-05-13T16:11:30Z,2025-05-14T07:59:12+00:00,2,15.79
13514,scripts : fix compare-llama-bench.py show parameter,CISC,2025-05-13T18:10:08Z,2025-05-14T06:41:01+00:00,1,12.51
13517,vulkan: workaround FA compile failures on macos,jeffbolznv,2025-05-13T21:49:53Z,2025-05-14T04:15:51+00:00,1,6.43
13519,arm64: optimize q6_k_q8_k kernel with i8mm,cyb70289,2025-05-14T01:36:48Z,2025-05-14T19:53:52+00:00,1,18.28
13521,"[server] accept requests where 'content' is missing entirely, not just null or empty",andysalerno,2025-05-14T03:59:52Z,2025-05-14T18:02:45+00:00,1,14.05
13525,webui : use fflate for more deterministic gzip compress,ngxson,2025-05-14T06:22:24Z,2025-05-14T08:26:12+00:00,1,2.06
13526,webui: Allow pasting file from clipboard,luca020400,2025-05-14T06:24:47Z,2025-05-14T08:07:32+00:00,1,1.71
13532,ci : upgraded oneAPI version in SYCL workflows and dockerfile,Alcpz,2025-05-14T08:42:56Z,2025-05-19T10:46:09+00:00,3,122.05
13533,server : fix cache_tokens bug with no cache_prompt,ngxson,2025-05-14T09:50:08Z,2025-05-14T11:35:08+00:00,1,1.75
13535,server : passthrough the /models endpoint during loading,ggerganov,2025-05-14T11:19:39Z,2025-05-14T12:42:10+00:00,2,1.38
13536,sycl: disable reorder for sycl mulmat,sgeor255,2025-05-14T11:25:25Z,2025-05-20T09:34:15+00:00,5,142.15
13537,CUDA: fix crash on large batch size for quant. MoE,JohannesGaessler,2025-05-14T12:21:59Z,2025-05-14T14:41:02+00:00,3,2.32
13539,llama : fix quantize with dl backends,slaren,2025-05-14T13:06:39Z,2025-05-14T14:12:37+00:00,1,1.1
13540,fix: proper error handling for missing elements in messages array (OpenAI compatible backend),pwilkin,2025-05-14T13:15:19Z,2025-05-15T06:40:58+00:00,1,17.43
13541,Fix build on OpenBSD,percypiper,2025-05-14T15:32:33Z,2025-05-25T12:35:54+00:00,1,261.06
13544,sycl : reviewing the backend documentation,Alcpz,2025-05-14T17:19:26Z,2025-05-19T13:38:20+00:00,19,116.31
13547,kv-cache : fix out-of-bounds view during reserve graph,ggerganov,2025-05-14T19:06:10Z,2025-05-14T20:15:15+00:00,3,1.15
13548,bench : handle decode errors,ggerganov,2025-05-14T19:37:36Z,2025-05-15T02:57:02+00:00,1,7.32
13550,Granite Four,gabe-l-hart,2025-05-14T20:13:13Z,2025-07-11T00:20:13+00:00,60,1372.12
13551,webui : improve accessibility for visually impaired people,ngxson,2025-05-14T21:19:26Z,2025-05-16T19:49:01+00:00,1,46.49
13554,vulkan: use scalar FA rather than coopmat2 when N==1,jeffbolznv,2025-05-15T04:26:23Z,2025-05-17T06:35:47+00:00,1,50.16
13556,vulkan: move common FA code to flash_attn_base.comp,jeffbolznv,2025-05-15T05:36:37Z,2025-05-17T07:14:55+00:00,1,49.64
13561,gguf-py : add support for sub_type (in arrays) in GGUFWriter add_key_value method,CISC,2025-05-15T10:05:54Z,2025-05-29T13:36:05+00:00,4,339.5
13562,webui : handle PDF input (as text or image) + convert pasted long content to file,ngxson,2025-05-15T10:32:47Z,2025-05-15T12:24:50+00:00,3,1.87
13567,convert : fix conversion for llama 4,ngxson,2025-05-15T14:38:56Z,2025-05-15T15:40:07+00:00,1,1.02
13569,gguf-py : Fix disconnect-before-connect crash on Kubuntu,danielzgtg,2025-05-15T14:48:22Z,2025-05-15T16:47:10+00:00,1,1.98
13573,minja: sync ,ochafik,2025-05-15T19:05:32Z,2025-05-15T22:29:10+00:00,1,3.39
13575,ci : add ppc64el to build-linux-cross,CISC,2025-05-15T20:33:19Z,2025-05-16T12:54:24+00:00,1,16.35
13577,server : do not return error when running out of context (with ctx shift disabled),ngxson,2025-05-15T22:34:42Z,2025-05-16T19:50:01+00:00,2,21.26
13582,sycl : fixed compilation warnings,lslusarczyk,2025-05-16T07:34:47Z,2025-05-16T10:15:29+00:00,1,2.68
13584,CUDA: skip fully masked-out KV in FA vec kernel,JohannesGaessler,2025-05-16T08:03:29Z,2025-05-20T12:45:07+00:00,6,100.69
13587,SYCL: Avoid using SYCL-Graph for unsupported nodes,EwanC,2025-05-16T11:08:02Z,2025-05-22T08:24:09+00:00,5,141.27
13589,llama : print hint when loading a model when no backends are loaded,slaren,2025-05-16T12:50:35Z,2025-05-16T14:38:07+00:00,1,1.79
13591,readme : clarify the list of dependencies and their license,ngxson,2025-05-16T15:30:05Z,2025-05-16T18:04:18+00:00,2,2.57
13592,releases : use arm version of curl for windows arm releases,slaren,2025-05-16T16:32:48Z,2025-05-16T17:36:51+00:00,1,1.07
13595,fix: use the current build config for `vulkan-shaders-gen`,giladgd,2025-05-17T03:06:17Z,2025-05-17T18:26:44+00:00,4,15.34
13602,ggml: aarch64: Implement SVE F32 kernels for Mamba Model,vineelabhinav,2025-05-17T09:25:17Z,2025-05-30T10:37:57+00:00,7,313.21
13607,Vulkan: Support fp32 accumulator in quantized matmul to fix GLM4-32B incoherence,0cc4m,2025-05-17T15:25:11Z,2025-05-19T15:54:08+00:00,1,48.48
13608,server : added --no-prefill-assistant flag,isaac-mcfadyen,2025-05-17T16:01:15Z,2025-05-17T21:59:48+00:00,1,5.98
13611,SYCL: Add non contiguous support in RMS_NORM and NORM kernels,qnixsynapse,2025-05-18T06:39:49Z,2025-05-26T15:40:36+00:00,16,201.01
13617,added load_progress_callback to common_params,psocolovsky,2025-05-18T17:24:34Z,2025-05-19T19:17:37+00:00,5,25.88
13623,mtmd : add ultravox audio input,ngxson,2025-05-18T21:15:01Z,2025-05-22T18:42:48+00:00,10,93.46
13626,vulkan: small fixes,netrunnereve,2025-05-19T03:07:20Z,2025-05-20T21:35:16+00:00,1,42.47
13627,[CANN]: add the basic supports of Flash Attention kernel,shibizhao,2025-05-19T05:53:01Z,2025-05-26T02:20:18+00:00,6,164.45
13634,server : fix first message identification,doringeman,2025-05-19T11:46:02Z,2025-05-21T13:07:57+00:00,2,49.37
13639,Fix GLM4 incoherence with fp16 accumulators,0cc4m,2025-05-19T17:10:15Z,2025-05-20T08:11:56+00:00,1,15.03
13640,sycl: Add more debug prints,Rbiessy,2025-05-19T18:11:23Z,2025-05-26T08:28:53+00:00,9,158.29
13642,releases : build CPU backend separately (windows),slaren,2025-05-19T20:57:49Z,2025-05-21T20:09:57+00:00,1,47.2
13643,sycl: add find_package call for OpenCL,AD2605,2025-05-19T22:04:28Z,2025-05-26T08:30:08+00:00,3,154.43
13647,musa: Upgrade MUSA SDK version to rc4.0.1 and use mudnn::Unary::IDENTITY op to accelerate D2D memory copy,yeahdongcn,2025-05-20T06:13:43Z,2025-05-21T01:58:49+00:00,2,19.75
13650,mtmd-helper : bug fix to token batching in mtmd,l3utterfly,2025-05-20T07:38:40Z,2025-05-20T16:55:31+00:00,1,9.28
13653,llama : remove llama_kv_cache_view API + remove deprecated,ggerganov,2025-05-20T08:55:33Z,2025-05-20T13:13:16+00:00,1,4.3
13657,use LOG_WARN to replace `std::cerr`,foldl,2025-05-20T10:43:23Z,2025-05-23T04:33:08+00:00,2,65.83
13659,Add the endpoints /api/tags and /api/chat,R-Dson,2025-05-20T12:07:50Z,2025-05-21T13:15:27+00:00,9,25.13
13660,kv-cache : simplify the interface,ggerganov,2025-05-20T14:51:26Z,2025-05-21T12:11:13+00:00,3,21.33
13667,ggml : add ggml_gelu_erf(),ngxson,2025-05-20T16:48:03Z,2025-05-21T14:26:33+00:00,3,21.64
13676,model : disable SWA for Phi models,ggerganov,2025-05-21T06:55:41Z,2025-05-21T10:09:21+00:00,7,3.23
13680,server : improve error reporting,ggerganov,2025-05-21T10:01:51Z,2025-05-21T16:46:56+00:00,1,6.75
13685,examples : switch retrieval to llama_encode,CISC,2025-05-21T13:12:11Z,2025-05-21T14:57:38+00:00,1,1.76
13686,convert : add qwen2vl support for unsloth merges,antichristHater,2025-05-21T13:32:57Z,2025-05-21T16:40:35+00:00,1,3.13
13692,server : pad small embedding batches,ggerganov,2025-05-21T16:37:12Z,2025-05-22T13:33:40+00:00,1,20.94
13693,model : jina-embeddings-v3 support,CISC,2025-05-21T18:37:57Z,2025-08-28T13:49:51+00:00,10,2371.2
13695,vulkan: support CPY from any type to itself,jeffbolznv,2025-05-21T20:19:36Z,2025-05-23T04:45:03+00:00,1,32.42
13696,vulkan: Disable coopmat/coopmat2/bfloat extensions if glslc doesn't support it,jeffbolznv,2025-05-21T21:17:42Z,2025-05-23T04:33:45+00:00,1,31.27
13699,common: Include torch package for s390x,taronaeo,2025-05-22T07:51:02Z,2025-05-22T18:31:29+00:00,1,10.67
13701,gguf-py : correct charsmap parameter typing,CISC,2025-05-22T09:30:24Z,2025-05-22T12:25:05+00:00,1,2.91
13702,sycl: Remove waits from async functions call  ,s-Nick,2025-05-22T09:45:52Z,2025-05-22T11:54:43+00:00,3,2.15
13705,[CANN]Support OP MUL_MAT_ID Q8 && Q4,noemotiovon,2025-05-22T12:06:05Z,2025-05-23T08:47:53+00:00,1,20.7
13706,kv-cache : rework kv_cell,ggerganov,2025-05-22T13:05:16Z,2025-05-25T13:34:37+00:00,7,72.49
13707,release : fix windows hip release,slaren,2025-05-22T13:37:24Z,2025-05-22T22:21:37+00:00,1,8.74
13711,Replace alert and confirm with custom modals.,igardev,2025-05-22T16:48:02Z,2025-05-31T09:56:08+00:00,2,209.13
13714,server : support audio input,ngxson,2025-05-22T20:25:23Z,2025-05-23T09:03:47+00:00,1,12.64
13719,ggml : add ggml_gelu_erf() CUDA kernel,ngxson,2025-05-23T06:14:44Z,2025-05-24T11:06:47+00:00,1,28.87
13720,ggml : riscv: add xtheadvector support,xctan,2025-05-23T08:35:43Z,2025-05-27T13:21:37+00:00,1,100.77
13739,SYCL: Implement few same quantized type copy kernels,qnixsynapse,2025-05-24T06:28:51Z,2025-06-07T13:28:20+00:00,3,342.99
13745,docs : add Moondream2 pre-quantized link,ddpasa,2025-05-24T10:35:19Z,2025-05-25T12:04:49+00:00,2,25.49
13746,kv-cache : refactor + add llama_memory_state_i,ggerganov,2025-05-24T11:12:05Z,2025-05-31T07:24:05+00:00,24,164.2
13749,SYCL: add gelu_erf kernel,qnixsynapse,2025-05-24T13:15:20Z,2025-05-27T15:22:59+00:00,8,74.13
13750,Move GLM4 f32 attention fix to the correct function,0cc4m,2025-05-24T13:31:28Z,2025-05-24T14:49:13+00:00,1,1.3
13755,SYCL: Add mrope kernel,qnixsynapse,2025-05-24T15:45:34Z,2025-05-30T14:10:57+00:00,6,142.42
13756,releases : enable openmp in windows cpu backend build,slaren,2025-05-24T18:02:01Z,2025-05-24T20:27:03+00:00,1,2.42
13757,convert : fix nomic-bert-moe mask token,CISC,2025-05-24T19:07:56Z,2025-06-01T16:07:21+00:00,6,188.99
13760,mtmd : add support for Qwen2-Audio and SeaLLM-Audio,ngxson,2025-05-24T21:09:11Z,2025-05-25T12:06:32+00:00,2,14.96
13768,Qwen3 MoE should also work with tie_word_embeddings,estibi,2025-05-25T06:25:18Z,2025-05-25T08:29:43+00:00,1,2.07
13770,server: fix/test add_generation_prompt param,ochafik,2025-05-25T07:11:23Z,2025-05-25T09:45:49+00:00,1,2.57
13771,`server`: add `--reasoning-budget 0` to disable thinking (incl. qwen3 w/ enable_thinking:false),ochafik,2025-05-25T08:03:32Z,2025-05-25T23:30:51+00:00,4,15.46
13773,tests : improve UGM tokenizer test coverage,CISC,2025-05-25T11:37:15Z,2025-05-25T14:22:29+00:00,1,2.75
13777,Add proper implementation of ollama's /api/chat,R-Dson,2025-05-25T14:14:44Z,2025-05-26T21:48:53+00:00,4,31.57
13779,webui : bump max upload file size to 500MB,ngxson,2025-05-25T15:49:15Z,2025-05-25T17:02:18+00:00,2,1.22
13782,server: args for draft model cache types (#11200),aa956,2025-05-25T17:48:36Z,2025-06-19T13:01:03+00:00,1,595.21
13783,vulkan: mark IM2COL as supporting non-contig,jeffbolznv,2025-05-25T20:23:13Z,2025-05-26T04:02:07+00:00,1,7.65
13784,"mtmd : support Qwen 2.5 Omni (input audio+vision, no audio output)",ngxson,2025-05-25T21:25:19Z,2025-05-27T12:06:10+00:00,3,38.68
13785,server: fix regression on streamed non-chat completion w/ stops,ochafik,2025-05-25T22:09:03Z,2025-05-26T13:16:37+00:00,1,15.13
13786,server: fix streaming crashes,ochafik,2025-05-25T22:20:39Z,2025-05-26T15:03:57+00:00,2,16.72
13787,"opencl: add new ops - `argsort`, `div`, `sub`, `addrows`, `sigmoid`, `group_norm`",lhez,2025-05-26T00:05:13Z,2025-05-27T19:56:08+00:00,1,43.85
13790,opencl: mark `MUL_MAT` supports non-contiguous tensors for f32,lhez,2025-05-26T06:23:53Z,2025-05-27T19:53:14+00:00,1,37.49
13791,cuda : avoid cuGetErrorString when not needed,ggerganov,2025-05-26T06:39:23Z,2025-05-26T19:14:52+00:00,1,12.59
13792,Add support for VK_EXT_debug_utils to add labels to Vulkan objects.,mtavenrath,2025-05-26T07:46:47Z,2025-06-21T06:17:12+00:00,5,622.51
13797,examples : allow extracting embeddings from decoder contexts,ggerganov,2025-05-26T09:22:47Z,2025-05-26T11:03:54+00:00,1,1.69
13800,"`server`: fix format of streamed tool call deltas (diff name, fix id location)",ochafik,2025-05-26T11:44:22Z,2025-05-26T13:56:49+00:00,1,2.21
13803,examples/training: Fix file name in README,standby24x7,2025-05-26T12:46:54Z,2025-05-26T14:55:24+00:00,1,2.14
13804,server: --offline mode,ochafik,2025-05-26T13:15:04Z,2025-05-26T21:34:28+00:00,5,8.32
13806,scripts : add option to compare commits in Debug,ggerganov,2025-05-26T13:50:25Z,2025-05-26T19:24:02+00:00,1,5.56
13810,docs: remove link for llama-cli function calling,bandoti,2025-05-26T19:06:52Z,2025-05-27T11:52:41+00:00,1,16.76
13811,ggml-cpu: x86 feature detection is specific to x86,ckastner,2025-05-26T19:13:11Z,2025-05-27T11:18:39+00:00,1,16.09
13813,ggml-vulkan: adds support for op CONV_TRANSPOSE_1D,etasnadi,2025-05-26T23:28:15Z,2025-06-04T20:02:00+00:00,21,212.56
13814,ggml : allow CUDA graphs when using pipeline parallelism,slaren,2025-05-27T00:05:54Z,2025-05-27T11:05:19+00:00,1,10.99
13817,vulkan: use timestamp queries for GGML_VULKAN_PERF,jeffbolznv,2025-05-27T02:26:06Z,2025-05-27T16:39:07+00:00,4,14.22
13818,ggml: improve ggml_backend_cuda_cpy_tensor_async,koush,2025-05-27T03:52:43Z,2025-06-04T20:26:41+00:00,8,208.57
13824,ggml : add ggml_repeat_4d,ngxson,2025-05-27T12:11:02Z,2025-05-27T13:53:55+00:00,3,1.71
13826,sycl: quantize and reorder the input to q8_1 when reorder is enabled,AD2605,2025-05-27T12:32:43Z,2025-06-02T09:12:20+00:00,9,140.66
13830,convert: handle when model's tokenization method relies on Mecab,huydt84,2025-05-27T15:28:03Z,2025-06-03T09:21:49+00:00,18,161.9
13833,llama : use n_swa + n_ubatch cells for SWA cache,ggerganov,2025-05-27T17:16:45Z,2025-05-31T12:57:44+00:00,2,91.68
13834,kv-cache : avoid modifying recurrent cells when setting inputs,compilade,2025-05-27T18:30:41Z,2025-06-10T22:20:14+00:00,6,339.83
13836,convert : fix tensor naming conflict for llama 4 vision,ngxson,2025-05-27T20:44:31Z,2025-05-28T08:05:54+00:00,1,11.36
13837,[CANN]: Add SOC TYPE printing in cmake configuration processing,leo-pony,2025-05-28T02:53:18Z,2025-05-28T03:54:20+00:00,1,1.02
13838,convert: small addition to support LlamaModel,huydt84,2025-05-28T03:56:36Z,2025-05-28T14:34:18+00:00,2,10.63
13840,"OpenCL: Add concat, tsembd, upscale, tanh, pad and repeat",rmatif,2025-05-28T05:39:27Z,2025-06-02T23:53:37+00:00,8,138.24
13841,gguf/utility: return full content on size < 0,Beinsezii,2025-05-28T05:49:36Z,2025-05-28T21:50:21+00:00,2,16.01
13842,musa: enable fp16 mma (all) and cublas on qy2,yeahdongcn,2025-05-28T06:40:57Z,2025-06-26T04:11:59+00:00,8,693.52
13843,ggml: aarch64: Implement SVE F32 kernels for vector functions,vineelabhinav,2025-05-28T07:02:30Z,2025-05-29T06:01:33+00:00,4,22.98
13844,convert : fix rwkv bos/eos token,CISC,2025-05-28T07:46:10Z,2025-05-30T12:50:43+00:00,1,53.08
13846,tests : add test-tokenizers-remote,CISC,2025-05-28T08:01:41Z,2025-06-04T19:41:30+00:00,19,179.66
13847,convert : allow partial update to the chkhsh pre-tokenizer list,ngxson,2025-05-28T09:23:36Z,2025-05-30T10:24:37+00:00,14,49.02
13853,server: fix remove 'image_url'/'input_audio' json-object effectlly for 'llama_params' in multimodal-model-mode,flyinskyin2013,2025-05-28T10:45:53Z,2025-05-28T14:33:54+00:00,2,3.8
13858,Add support for BertForSequenceClassification reranking,huydt84,2025-05-28T12:55:20Z,2025-05-28T17:01:58+00:00,5,4.11
13862,"docs : add ""Quick start"" section for new users",ngxson,2025-05-28T15:10:16Z,2025-06-03T11:09:36+00:00,3,139.99
13866,mtmd : move helpers to dedicated library (⚠️ breaking change),ngxson,2025-05-28T16:19:55Z,2025-05-28T20:35:22+00:00,1,4.26
13870,llama : fix KV shift for qwen2vl,ngxson,2025-05-28T18:14:41Z,2025-05-28T20:35:32+00:00,1,2.35
13873,"finetune: SGD optimizer, more CLI args",graehl,2025-05-28T20:26:00Z,2025-08-14T10:03:58+00:00,71,1861.63
13875,llama : add RobertaForSequenceClassification reranker support,CISC,2025-05-28T21:42:59Z,2025-05-29T06:15:01+00:00,1,8.53
13880,tests : remove json.hpp from a test,ggerganov,2025-05-29T06:17:50Z,2025-05-29T09:17:17+00:00,1,2.99
13882,ggml: aarch64: Implement SVE F32 kernels for Mamba Sequential Scan Algorithm,vineelabhinav,2025-05-29T07:36:32Z,2025-05-29T09:18:43+00:00,2,1.7
13883,cmake: Factor out CPU architecture detection,ckastner,2025-05-29T08:15:41Z,2025-05-29T10:50:26+00:00,2,2.58
13885,sycl: Add reorder to Q6_K mmvq implementation,s-Nick,2025-05-29T10:31:53Z,2025-06-09T09:47:07+00:00,19,263.25
13886,arm64: optimize q4_k_q8_k kernel with i8mm,cyb70289,2025-05-29T10:35:39Z,2025-05-29T11:39:21+00:00,7,1.06
13887,musa: extract ggml_cuda_mul_mat_batched_cublas_gemm_batched_ex,yeahdongcn,2025-05-29T11:21:00Z,2025-06-04T10:20:05+00:00,2,142.98
13890,cmake: Guard GGML_CPU_ALL_VARIANTS by architecture,ckastner,2025-05-29T12:40:25Z,2025-05-29T23:28:55+00:00,2,10.81
13892,ggml-cpu : split arch-specific implementations,xctan,2025-05-29T13:38:45Z,2025-06-09T14:47:13+00:00,8,265.14
13895,CUDA: add a prop in ggml_cuda_device_infor for distinguish iGPU or dGPU in cuda (#13856),Yangxiaoz,2025-05-29T16:39:17Z,2025-05-31T06:48:04+00:00,9,38.15
13900,llama : add support for jina-reranker-v2,CISC,2025-05-29T18:42:05Z,2025-05-29T19:42:31+00:00,1,1.01
13901,sync : vendor,ggerganov,2025-05-29T19:22:05Z,2025-05-30T13:25:46+00:00,3,18.06
13903,cmake: mtmd: install PUBLIC_HEADERs and allow static linking (#13902),65a,2025-05-29T20:47:55Z,2025-05-30T16:07:43+00:00,2,19.33
13907,Add support for DistilBert,huydt84,2025-05-30T01:12:50Z,2025-05-30T09:56:03+00:00,8,8.72
13911,model: minicpm should use llm_build_granite,zkh2016,2025-05-30T07:03:41Z,2025-05-30T08:31:48+00:00,1,1.47
13912,remove WIP since PR has been merged,pepijndevos,2025-05-30T07:57:29Z,2025-06-15T06:06:37+00:00,1,382.15
13914,[Ascend NPU] Enable labeler,shink,2025-05-30T08:57:43Z,2025-06-09T03:20:06+00:00,1,234.37
13917,"mtmd : drop `_shared` from `libmtmd` name, merge helpers into libmtmd (⚠️ breaking change)",ngxson,2025-05-30T10:13:25Z,2025-05-31T08:14:29+00:00,1,22.02
13919,cuda : prevent using split buffers with 3d/4d matrices,slaren,2025-05-30T13:29:21Z,2025-05-30T14:37:19+00:00,1,1.13
13922,sched : avoid changing cur_copy when a graph is already allocated,slaren,2025-05-30T14:40:42Z,2025-05-30T16:56:19+00:00,1,2.26
13926,CUDA: fix typo in FlashAttention code,JohannesGaessler,2025-05-30T16:26:10Z,2025-05-30T19:22:03+00:00,1,2.93
13931,`chat`: allow unclosed thinking tags,ochafik,2025-05-30T22:53:46Z,2025-05-31T15:26:10+00:00,1,16.54
13933,`server`: update deepseek reasoning format (pass reasoning_content as diffs),ochafik,2025-05-30T23:49:19Z,2025-06-02T17:15:44+00:00,5,65.44
13936,vulkan: automatically deduce size of push constants,jeffbolznv,2025-05-31T04:34:04Z,2025-06-05T05:17:59+00:00,1,120.73
13937,vulkan: fix warnings in perf logger querypool code,jeffbolznv,2025-05-31T04:42:49Z,2025-06-03T18:30:23+00:00,2,85.79
13938,chore: added badge and link to release,Olexandr88,2025-05-31T06:11:51Z,2025-06-05T07:50:55+00:00,1,121.65
13939,opencl: add `backend_synchronize`,lhez,2025-05-31T06:17:33Z,2025-06-02T23:54:58+00:00,1,65.62
13940,llama : support multiple classifier outputs and labels,CISC,2025-05-31T07:07:50Z,2025-06-06T07:03:25+00:00,30,143.93
13943,ggml: check if non-native endian model is being loaded,taronaeo,2025-05-31T13:08:06Z,2025-06-01T14:53:58+00:00,8,25.76
13944,ci: add LoongArch cross-compile build,wojiushixiaobai,2025-05-31T14:21:16Z,2025-06-07T13:39:11+00:00,4,167.3
13954,convert : fix vocab padding code for bert models,CISC,2025-06-01T09:33:12Z,2025-06-01T15:23:11+00:00,2,5.83
13960,ci:  Update windows-2019 to windows-2022,abhishek-iitmadras,2025-06-01T19:58:38Z,2025-06-04T13:56:13+00:00,1,65.96
13961,mtmd : fix memory leak in mtmd_helper_eval_chunk_single,ngxson,2025-06-01T21:14:31Z,2025-06-02T14:29:28+00:00,5,17.25
13966,"""Fix: Handle mixed-case 'Power' strings in POWER CPU detection""",shalinib-ibm,2025-06-02T09:33:22Z,2025-06-02T12:18:36+00:00,1,2.75
13973,sycl: GGML_SYCL_DISABLE_OPT on by default for all Intel Devices,ShanoToni,2025-06-02T13:36:08Z,2025-06-25T16:09:56+00:00,4,554.56
13979,Hybrid recurrent cache,gabe-l-hart,2025-06-02T19:50:36Z,2025-06-19T05:08:14+00:00,77,393.29
13980,llama : allow building all tests on windows when not using shared libs,slaren,2025-06-02T20:59:57Z,2025-06-09T18:03:09+00:00,1,165.05
13985,kv-cache : fix unified::seq_rm to work with seq_id < 0,ggerganov,2025-06-03T09:25:59Z,2025-06-04T06:50:32+00:00,1,21.41
13988,kv-cache : refactor the update/defrag mechanism,ggerganov,2025-06-03T11:04:08Z,2025-06-04T15:58:21+00:00,1,28.9
13991,CUDA: fix FTZ in FA for Gemma 3,JohannesGaessler,2025-06-03T12:31:54Z,2025-06-04T06:57:05+00:00,1,18.42
13996,"releases : use dl backend for linux release, remove arm64 linux release",slaren,2025-06-03T19:47:34Z,2025-06-04T11:15:54+00:00,1,15.47
13997,"ci : remove cuda 11.7 releases, switch runner to windows 2022",slaren,2025-06-03T20:14:46Z,2025-06-04T13:37:40+00:00,1,17.38
13998,llama-graph : use ggml_repeat_4d,ngxson,2025-06-03T20:31:33Z,2025-06-04T08:11:26+00:00,2,11.66
14001,vulkan: Enable VK_KHR_cooperative_matrix extension for Intel Xe2 GPUs,rillomas,2025-06-04T02:20:08Z,2025-06-05T14:00:30+00:00,5,35.67
14002,[CANN]:Replace aclrtMemsetSync with InplaceZero operator for zero tensor creation,luyhcsu,2025-06-04T02:58:07Z,2025-07-04T03:50:07+00:00,3,720.87
14003,opencl: preliminary support for Q4_0 mul_mat_id using matvec,lhez,2025-06-04T06:00:14Z,2025-06-10T23:55:59+00:00,1,161.93
14005,Fix CUDA build failure on AutoDL cloud platforms,pockers21,2025-06-04T10:26:11Z,2025-06-05T13:25:29+00:00,1,26.99
14006,memory : migrate from llama_kv_cache to more generic llama_memory,ggerganov,2025-06-04T10:29:32Z,2025-06-05T12:29:22+00:00,2,26.0
14012,llama-chat : Do not throw when tool parsing fails,p1-0tr,2025-06-04T12:48:12Z,2025-06-14T16:25:15+00:00,1,243.62
14013,llama : allow using mmap without PrefetchVirtualMemory,slaren,2025-06-04T15:07:20Z,2025-06-05T09:57:42+00:00,1,18.84
14016,server: Enable mtmd in llama-server `/completion` endpoint,92MING,2025-06-04T19:33:10Z,2025-08-22T08:10:57+00:00,1,1884.63
14017,tests : add test-tokenizers-repo,CISC,2025-06-04T19:40:10Z,2025-06-11T15:16:32+00:00,2,163.61
14023,ggml-cpu: fix uncaught underscore terminators for s390x,taronaeo,2025-06-05T08:08:44Z,2025-06-18T17:06:50+00:00,2,320.97
14028,llama : default pooling last for qwen3,ngxson,2025-06-05T11:48:05Z,2025-06-05T12:53:57+00:00,2,1.1
14030,llama : deprecate llama_kv_self_ API,ggerganov,2025-06-05T12:57:27Z,2025-06-06T11:11:15+00:00,3,22.23
14031,gguf-py : add add_classifier_output_labels method to writer,CISC,2025-06-05T13:32:34Z,2025-06-05T15:42:31+00:00,1,2.17
14033,cuda : fix device sync on buffer clear,slaren,2025-06-05T15:18:19Z,2025-06-09T14:36:26+00:00,2,95.3
14034,sycl: Adding additional cpy dbg print output,ShanoToni,2025-06-05T15:55:45Z,2025-06-13T07:51:39+00:00,10,183.93
14035,llama : add thread safety test,slaren,2025-06-05T17:03:27Z,2025-06-16T15:11:43+00:00,6,262.14
14037,ggml-cpu: optimise assembly calls for hsum on s390x,taronaeo,2025-06-05T17:45:44Z,2025-06-18T17:10:09+00:00,2,311.41
14047,vulkan : fix build failure caused by vulkan-shaders-gen install,AsbjornOlling,2025-06-06T10:43:11Z,2025-06-16T13:38:36+00:00,5,242.92
14049,ARM: Fixes and additions to CPU feature detection,ckastner,2025-06-06T11:31:09Z,2025-06-09T07:28:27+00:00,5,67.95
14050,llama : fix llama_model_chat_template with template name (LLM_KV with suffix),CISC,2025-06-06T11:41:56Z,2025-06-07T12:13:12+00:00,1,24.52
14062,webui: Wrap long numbers instead of infinite horizontal scroll,am17an,2025-06-08T05:58:22Z,2025-06-11T14:42:25+00:00,2,80.73
14065,webui: add server info to chat message,am17an,2025-06-08T09:26:22Z,2025-07-07T09:38:10+00:00,6,696.2
14069,cuda : fix buffer type check with integrated GPUs,slaren,2025-06-08T17:39:49Z,2025-06-08T18:39:56+00:00,1,1.0
14074,Add geglu activation function,huydt84,2025-06-08T23:11:13Z,2025-06-09T04:15:31+00:00,1,5.07
14076,rpc: nicer error message for RPC server crash,isaac-mcfadyen,2025-06-09T00:17:37Z,2025-06-10T06:41:01+00:00,1,30.39
14077,graph : fix geglu,ggerganov,2025-06-09T04:43:24Z,2025-06-09T14:17:31+00:00,5,9.57
14079,server : fix LRU check,ggerganov,2025-06-09T06:40:08Z,2025-06-09T09:57:58+00:00,1,3.3
14080,Implement GGML_CPU_ALL_VARIANTS for ARM,ckastner,2025-06-09T07:23:59Z,2025-06-11T19:07:44+00:00,1,59.73
14090,llama : support GEGLU for jina-bert-v2,CISC,2025-06-09T19:12:43Z,2025-06-10T16:02:09+00:00,4,20.82
14100,scripts: Fix remote option in Windows (#14102),pqnet,2025-06-10T11:37:02Z,2025-06-19T10:21:40+00:00,2,214.74
14103,convert : fix duplicate key DeepSeek-R1 conversion error,CISC,2025-06-10T11:52:39Z,2025-06-10T21:29:53+00:00,2,9.62
14104,server: Fixed speculative decoding stats to use `#accepted \ #tested` rather than `#accepted \ #drafted`,jukofyork,2025-06-10T13:02:42Z,2025-06-10T15:48:07+00:00,1,2.76
14106,vulkan: force device 0 in CI,jeffbolznv,2025-06-10T14:02:28Z,2025-06-10T15:53:47+00:00,2,1.86
14109,vulkan: Track descriptor pools/sets per-context,jeffbolznv,2025-06-10T16:20:14Z,2025-06-11T05:19:26+00:00,1,12.99
14116,vulkan: Better thread-safety for command pools/buffers,jeffbolznv,2025-06-11T03:48:27Z,2025-06-11T14:48:53+00:00,1,11.01
14118,llama-model : add dots.llm1 architecture support (#14044),Noeda,2025-06-11T06:10:26Z,2025-06-15T07:52:06+00:00,3,97.69
14120,Pass --keep to llama-server,MightyAlex200,2025-06-11T08:35:05Z,2025-06-11T10:43:43+00:00,1,2.14
14122,Update multimodal.md,ddpasa,2025-06-11T08:44:29Z,2025-06-13T13:17:53+00:00,6,52.56
14125,sycl: Remove not needed copy f16->f32 for dnnl mul mat,ShanoToni,2025-06-11T11:45:33Z,2025-06-12T13:15:12+00:00,3,25.49
14126,cmake : handle whitepsaces in path during metal build,ggerganov,2025-06-11T12:50:28Z,2025-06-12T07:14:25+00:00,1,18.4
14127,vulkan: mutex around vkQueueSubmit,jeffbolznv,2025-06-11T15:34:28Z,2025-06-16T06:21:08+00:00,1,110.78
14128,chore : clean up relative source dir paths,CISC,2025-06-11T15:46:47Z,2025-06-11T17:04:23+00:00,1,1.29
14140,context : round n_tokens to next multiple of n_seqs when reserving,compilade,2025-06-12T05:32:08Z,2025-06-12T06:56:04+00:00,2,1.4
14144,sycl: fix docker image,sgeor255,2025-06-12T09:42:58Z,2025-06-13T16:32:56+00:00,2,30.83
14145,vocab : prevent heap overflow when vocab is too small,ggerganov,2025-06-12T10:05:30Z,2025-06-13T05:03:54+00:00,1,18.97
14146,ggml-cpu : rework weak alias on apple targets,xctan,2025-06-12T10:34:24Z,2025-06-16T05:54:16+00:00,2,91.33
14152,sycl: Bump oneMath commit,EwanC,2025-06-12T14:16:58Z,2025-06-13T07:45:37+00:00,2,17.48
14153,batch : rework llama_batch_allocr,ggerganov,2025-06-12T14:36:14Z,2025-06-13T10:47:56+00:00,6,20.2
14156,Improve build-info.cpp generation,ckastner,2025-06-12T21:57:46Z,2025-06-13T06:51:35+00:00,1,8.9
14158,ggml : implement REGLU/GEGLU/SWIGLU ops,CISC,2025-06-12T23:24:57Z,2025-06-29T09:04:10+00:00,22,393.65
14164,Add NeoBERT,huydt84,2025-06-13T07:20:52Z,2025-06-16T12:53:41+00:00,10,77.55
14165,Make cls_b and cls_out_b optional in ranking,huydt84,2025-06-13T07:31:14Z,2025-06-13T08:34:08+00:00,1,1.05
14169,compare llama-bench: add option to plot,am17an,2025-06-13T08:59:33Z,2025-06-14T08:34:20+00:00,8,23.58
14172,batch : add LLAMA_BATCH_DEBUG environment variable,ggerganov,2025-06-13T14:09:23Z,2025-06-13T15:35:00+00:00,1,1.43
14179,cmake: clean up external project logic for vulkan-shaders-gen,bandoti,2025-06-13T22:27:44Z,2025-06-16T13:32:14+00:00,1,63.08
14180,When listening on a unix domain socket don't print http:// and port,ericcurtin,2025-06-14T08:35:16Z,2025-06-15T21:36:23+00:00,5,37.02
14181,ggml : implement GLU for split up/gate,CISC,2025-06-14T11:17:06Z,2025-06-18T14:11:08+00:00,3,98.9
14183,HIP: Replace usage of depricated preprocessor macro __AMDGCN_WAVEFRONT_SIZE__,IMbackK,2025-06-14T20:31:07Z,2025-06-15T13:45:27+00:00,1,17.24
14185,Add support for Arcee AI's upcoming AFM model,bartowski1182,2025-06-14T21:52:49Z,2025-06-15T23:04:06+00:00,8,25.19
14194,Allow override when adding value to ggufwriter,huydt84,2025-06-15T12:28:27Z,2025-06-16T07:20:59+00:00,3,18.88
14197,quantize: Use UINT32 if there's an INT KV override,EAddario,2025-06-15T15:14:33Z,2025-06-15T16:53:45+00:00,1,1.65
14200,gguf-py: Make sentencepiece optional,Ahajha,2025-06-15T18:44:15Z,2025-06-19T13:56:12+00:00,7,91.2
14202,HIP: disable rocwmma on gfx12 by default until rocm 7.0,IMbackK,2025-06-15T20:32:38Z,2025-06-16T11:47:39+00:00,1,15.25
14205,ggml-cpu : fix static linking issue,xctan,2025-06-16T06:55:29Z,2025-06-16T13:13:13+00:00,1,6.3
14206,ggml: Add Android support for GGML_CPU_ALL_VARIANTS,chaxu01,2025-06-16T07:28:35Z,2025-06-16T09:47:57+00:00,2,2.32
14215,sycl: Cleanup codepaths in Get Rows in sycl backend,ShanoToni,2025-06-16T14:08:28Z,2025-06-19T10:40:21+00:00,4,68.53
14217,ubatch : new splitting logic,ggerganov,2025-06-16T15:12:50Z,2025-06-20T07:14:14+00:00,3,88.02
14221,ggml-cpu : remove the weak alias trick,xctan,2025-06-16T17:18:48Z,2025-06-17T09:58:32+00:00,2,16.66
14222,common : suggest --jinja when autodetection fails,CISC,2025-06-16T17:52:31Z,2025-06-16T19:58:42+00:00,1,2.1
14226,cmake: remove shader-gen step-targets from ggml-vulkan,bandoti,2025-06-16T19:19:23Z,2025-06-17T20:33:25+00:00,1,25.23
14231,musa: fix build warning (unused variable),yeahdongcn,2025-06-17T02:57:43Z,2025-06-17T09:48:09+00:00,1,6.84
14236,Mtmd: add a way to select device for vision encoder,stduhpf,2025-06-17T10:20:58Z,2025-07-22T10:51:03+00:00,3,840.5
14238,MODEL: Falcon-H1 support,younesbelkada,2025-06-17T11:36:40Z,2025-07-04T14:05:02+00:00,3,410.47
14244,sycl: add usage of enqueue_functions extension,s-Nick,2025-06-17T14:02:06Z,2025-06-20T13:07:21+00:00,12,71.09
14246,"llama-chat : fix multiple system messages for gemma, orion",ngxson,2025-06-17T15:21:30Z,2025-06-18T07:58:43+00:00,1,16.62
14247,mtmd : refactor llava-uhd preprocessing logic,ngxson,2025-06-17T17:37:18Z,2025-06-18T08:43:57+00:00,1,15.11
14248,convert : fix null head_dim AutoConfig regression,CISC,2025-06-17T17:53:35Z,2025-06-18T07:52:07+00:00,1,13.98
14249,Vulkan: Fix host-pinned memory for large allocations,0cc4m,2025-06-17T20:27:11Z,2025-06-19T07:15:42+00:00,2,34.81
14254,opencl: ref count `ggml_backend_opencl_context` and refactor profiling,lhez,2025-06-18T06:41:43Z,2025-06-24T18:46:25+00:00,1,156.08
14258,ggml: Add Apple support for GGML_CPU_ALL_VARIANTS,chaxu01,2025-06-18T09:06:32Z,2025-06-18T11:40:07+00:00,3,2.56
14261,fix: resolve gcc compile warnings,fanyang89,2025-06-18T12:58:55Z,2025-06-19T12:49:49+00:00,1,23.85
14262,CUDA: mul_mat_v support for batch sizes > 1,JohannesGaessler,2025-06-18T13:11:09Z,2025-06-23T11:11:31+00:00,1,118.01
14264,docs: add s390x build documentation,taronaeo,2025-06-18T15:55:27Z,2025-06-18T17:10:26+00:00,2,1.25
14265,CUDA: add conv_2d_dw,am17an,2025-06-18T16:32:47Z,2025-06-20T01:50:24+00:00,3,33.29
14270,llama-bench : add --no-warmup flag (#14224),s2010,2025-06-18T20:57:16Z,2025-06-19T10:24:12+00:00,1,13.45
14272,llama : improve sep token handling,CISC,2025-06-19T00:43:02Z,2025-06-20T12:04:10+00:00,9,35.35
14273,llamafile: support s390x SIMD instruction set,taronaeo,2025-06-19T06:56:08Z,2025-06-19T09:48:54+00:00,1,2.88
14274,ggml : add ggml_set_rows,rgerganov,2025-06-19T08:10:14Z,2025-06-27T13:41:41+00:00,12,197.52
14277,ggml: Update KleidiAI to v1.9.0,chaxu01,2025-06-19T10:11:19Z,2025-06-20T07:51:02+00:00,1,21.66
14281,ggml-cpu : remove unnecesary arm feature detection,slaren,2025-06-19T12:50:31Z,2025-06-19T19:24:14+00:00,1,6.56
14285,kv-cache : use ggml_set_rows,ggerganov,2025-06-19T16:30:58Z,2025-07-03T07:53:35+00:00,1,327.38
14286,Implement GGML_CPU_ALL_VARIANTS for PowerPC,ckastner,2025-06-19T16:32:11Z,2025-06-20T12:17:32+00:00,1,19.76
14287,CUDA: add conv_2d_transpose,am17an,2025-06-19T16:35:12Z,2025-06-20T14:48:24+00:00,2,22.22
14288,cuda : synchronize graph capture and cublas handle destruction,slaren,2025-06-19T19:32:35Z,2025-06-20T11:57:36+00:00,1,16.42
14290,Fix Windows Null Pointer Bug and Enhance Memory Operations in ggml-sycl,MengAiDev,2025-06-20T05:36:23Z,2025-06-26T11:53:35+00:00,3,150.29
14293,docs : fix the link to `llama.h`,david20571015,2025-06-20T08:06:22Z,2025-06-20T17:43:35+00:00,1,9.62
14309,"GitHub workflow: set RPATH to ""@loader_path"" / ""$ORIGIN"" to ensure executables and dynamic libraries search for dependencies in their origin directory.",rotemdan,2025-06-20T20:39:31Z,2025-07-02T16:37:17+00:00,1,283.96
14311,Fix Llama 4 conversion,danielhanchen,2025-06-21T02:50:07Z,2025-06-21T04:32:02+00:00,1,1.7
14313,CUDA: add mean operation,am17an,2025-06-21T06:00:03Z,2025-06-22T04:39:55+00:00,1,22.66
14314,gguf-py : fix Qwen3-Embedding eos token,CISC,2025-06-21T06:18:47Z,2025-06-21T16:12:05+00:00,1,9.89
14316,ggml: adds CONV_2D op and direct GEMM Vulkan implementation,etasnadi,2025-06-21T11:05:01Z,2025-07-19T19:59:09+00:00,27,680.9
14317,ggml-cpu: enable IBM NNPA Vector Intrinsics,taronaeo,2025-06-21T13:11:24Z,2025-06-25T21:49:04+00:00,18,104.63
14319,common : use std::string_view now that we target c++17,CISC,2025-06-21T17:56:37Z,2025-06-22T05:37:43+00:00,1,11.69
14322,Fix appearance of the chats list context menu for the browser Safari,rntk,2025-06-22T07:38:23Z,2025-06-29T17:29:58+00:00,1,177.86
14323,HIP: enable vec fattn on RDNA4,IMbackK,2025-06-22T07:49:16Z,2025-06-22T14:51:23+00:00,1,7.04
14324,CUDA/HIP: optimize mmv paths taken for HIP/CDNA,IMbackK,2025-06-22T08:35:42Z,2025-06-23T23:12:57+00:00,9,38.62
14326,mtmd: fix Pixtral OOM with large images by capping image_size to 1024,yuiseki,2025-06-22T11:41:49Z,2025-06-22T12:44:57+00:00,1,1.05
14327,run : avoid double tokenization,retr0reg,2025-06-22T14:48:16Z,2025-06-22T17:28:06+00:00,2,2.66
14330,gguf-py : fix SpecialVocab parsing when post_processor is null,CISC,2025-06-22T16:43:59Z,2025-06-22T17:46:17+00:00,1,1.04
14333,vulkan: lock accesses of pinned_memory vector,jeffbolznv,2025-06-22T20:47:27Z,2025-06-28T15:17:10+00:00,1,138.5
14334,vulkan: update windows SDK in CI,jeffbolznv,2025-06-22T22:14:48Z,2025-06-23T08:19:24+00:00,1,10.08
14336,Fixes for rwkv-world template and the missing inputs.use_jinja in llama-cli,MollySophia,2025-06-23T02:14:18Z,2025-06-23T11:56:19+00:00,1,9.7
14341,Make the shell scripts cross-platform,vedranmiletic,2025-06-23T09:03:06Z,2025-06-30T08:17:18+00:00,3,167.24
14345,"vulkan: Increase workgroup size for GLU, for performance",jeffbolznv,2025-06-23T13:33:44Z,2025-06-28T16:46:37+00:00,2,123.21
14346,ci: add workflow for relocatable cmake package,bandoti,2025-06-23T14:41:20Z,2025-06-23T18:30:52+00:00,1,3.83
14350,main : honor --verbose-prompt on interactive prompts,CISC,2025-06-23T22:25:47Z,2025-06-24T07:31:00+00:00,1,9.09
14352,docs: Fix server API key doc for /props (move it to /health),pnb,2025-06-23T23:57:26Z,2025-06-24T08:59:11+00:00,1,9.03
14354,Add script to test op perf and compare,yeahdongcn,2025-06-24T06:01:39Z,2025-08-01T00:50:25+00:00,3,906.81
14360,server : fix assistant prefilling when content is an array,CISC,2025-06-24T10:02:50Z,2025-07-05T07:17:14+00:00,1,261.24
14361,CUDA: add bf16 and f32 support to cublas_mul_mat_batched,am17an,2025-06-24T10:07:33Z,2025-06-28T17:30:53+00:00,8,103.39
14362,cmake : use LLAMA_BUILD_NUMBER when defining LLAMA_INSTALL_VERSION,mbaudier,2025-06-24T11:27:25Z,2025-06-24T13:05:31+00:00,1,1.64
14363,llama : add high-throughput mode,ggerganov,2025-06-24T12:37:21Z,2025-07-16T13:35:42+00:00,13,528.97
14366,vulkan: Add fusion support for RMS_NORM+MUL,jeffbolznv,2025-06-24T20:12:47Z,2025-06-29T07:43:36+00:00,22,107.51
14368,test-backend-ops: add support for specifying output format,yeahdongcn,2025-06-25T03:36:31Z,2025-07-05T04:10:53+00:00,29,240.57
14371,docs: fix broken url in main readme,justinclift-prvidr,2025-06-25T07:29:24Z,2025-07-20T17:44:24+00:00,5,610.25
14373,Q2k interleaving implementation - x86/x64 SIMD,Srihari-mcw,2025-06-25T12:52:00Z,2025-08-01T06:20:33+00:00,3,881.48
14374,webui: preserve partial content when streaming errors occur,Aaryan-549,2025-06-25T13:14:22Z,2025-09-27T00:08:20+00:00,1,2242.9
14378,vulkan: handle noncontig in the final case of ggml_vk_get_cpy_pipeline,jeffbolznv,2025-06-25T16:44:48Z,2025-06-28T15:36:40+00:00,2,70.86
14381,ggml : do not output unprintable characters on GGUF load failure,CISC,2025-06-25T19:20:28Z,2025-06-25T21:26:51+00:00,2,2.11
14388,Add Conv2d for CPU,am17an,2025-06-26T10:03:19Z,2025-06-30T15:57:04+00:00,7,101.9
14390,llama : return mistral-v7-tekken as default template only,CISC,2025-06-26T10:20:16Z,2025-06-26T13:01:14+00:00,1,2.68
14392,compare-commits.sh: support both llama-bench and test-backend-ops,yeahdongcn,2025-06-26T11:26:26Z,2025-08-01T00:47:27+00:00,14,853.35
14395,SYCL: disable faulty fp16 exp kernel,qnixsynapse,2025-06-26T14:21:40Z,2025-06-29T15:37:58+00:00,10,73.27
14398,cmake: regen vulkan shaders when shaders-gen sources change,bandoti,2025-06-26T15:38:54Z,2025-06-26T16:46:53+00:00,1,1.13
14399,Add explanation to --no-mmap in llama server,malte-j,2025-06-26T15:42:26Z,2025-06-29T18:26:21+00:00,1,74.73
14400,model : gemma3n text-only,ngxson,2025-06-26T15:54:13Z,2025-06-26T17:34:02+00:00,10,1.66
14403,OpenCL: add conv2d kernel,rmatif,2025-06-26T19:12:32Z,2025-07-21T17:03:19+00:00,3,597.85
14407,[CANN] weight format to nz for Ascend310P3,tqgy6,2025-06-27T01:49:31Z,2025-07-23T03:58:00+00:00,1,626.14
14408,model : add support for ERNIE 4.5 0.3B model,ownia,2025-06-27T02:35:56Z,2025-06-28T14:08:21+00:00,3,35.54
14410,graph : make llm_graph_context destructor virtual,ggerganov,2025-06-27T06:41:57Z,2025-06-27T18:42:02+00:00,1,12.0
14411,[CANN]update aclnnGroupedMatmulV2 to aclnnGroupedMatmulV3,noemotiovon,2025-06-27T07:11:05Z,2025-07-01T08:47:30+00:00,1,97.61
14417,ggml : add ggml_scale_bias,ngxson,2025-06-27T09:23:38Z,2025-07-09T16:16:12+00:00,10,294.88
14425,model : add hunyuan moe,ngxson,2025-06-27T16:03:30Z,2025-07-08T08:24:06+00:00,9,256.34
14427,vulkan: Fix GGML_VULKAN_SHADER_DEBUG_INFO,jeffbolznv,2025-06-27T20:10:16Z,2025-06-28T03:35:30+00:00,1,7.42
14431,ci : fix windows build and release,CISC,2025-06-28T06:10:23Z,2025-06-28T07:57:07+00:00,1,1.78
14432,[CANN] Fix a bug related to enabling async_mode,bachelor-dou,2025-06-28T07:34:40Z,2025-06-28T09:35:41+00:00,1,2.02
14445,ggml : implement GEGLU_ERF and GEGLU_QUICK ops,CISC,2025-06-29T14:46:57Z,2025-07-03T21:07:22+00:00,4,102.34
14449,vulkan: support softmax/FA batch and broadcast,jeffbolznv,2025-06-29T18:35:07Z,2025-07-01T08:32:56+00:00,5,37.96
14450,convert : correct gemma 3n conversion,ngxson,2025-06-29T19:04:53Z,2025-07-03T08:03:06+00:00,6,84.97
14451,vulkan: Split large mul_mat_id to fit in shared memory,jeffbolznv,2025-06-29T20:10:17Z,2025-07-01T08:43:08+00:00,2,36.55
14452,Remove redundant include path in CMakeLists.txt,xiaobing318,2025-06-30T02:19:04Z,2025-06-30T09:48:24+00:00,1,7.49
14455,vulkan : add GELU_ERF,CISC,2025-06-30T06:37:38Z,2025-07-01T08:14:21+00:00,1,25.61
14456,"opencl: add `GEGLU`, `REGLU`, `SWIGLU`",lhez,2025-06-30T06:55:39Z,2025-07-01T07:19:16+00:00,1,24.39
14462,ggml-cpu: sycl: Re-enable exp f16,Rbiessy,2025-06-30T11:32:34Z,2025-06-30T12:52:02+00:00,6,1.32
14472,Add Vulkan images to docker.md,xek,2025-07-01T06:45:53Z,2025-07-01T13:44:11+00:00,1,6.97
14475,CUDA: add softmax broadcast,am17an,2025-07-01T08:40:24Z,2025-07-02T12:34:25+00:00,9,27.9
14476,opencl : add GELU_ERF,CISC,2025-07-01T08:42:44Z,2025-07-05T06:24:56+00:00,2,93.7
14481,Callback before abort,ScaledLizard,2025-07-01T13:05:59Z,2025-07-02T05:19:31+00:00,3,16.23
14482,llama : reuse compute graphs,ggerganov,2025-07-01T14:19:09Z,2025-07-17T16:08:33+00:00,21,385.82
14483,ggml: backward pass for split swiglu,JohannesGaessler,2025-07-01T14:20:42Z,2025-07-03T15:05:18+00:00,1,48.74
14485,vulkan: unpack more values at a time for iquants mat mul,netrunnereve,2025-07-01T16:48:14Z,2025-07-06T10:29:36+00:00,1,113.69
14488,opencl: update `upscale` to support `align corners`,lhez,2025-07-01T22:48:23Z,2025-07-02T07:07:43+00:00,1,8.32
14490,opencl: preventing buffer overflows in debugging utils,jeffzhou2000,2025-07-02T03:49:10Z,2025-07-02T12:38:10+00:00,3,8.82
14491,opencl : skip empty nodes on cgraph compute,EZForever,2025-07-02T03:59:50Z,2025-07-02T11:00:05+00:00,1,7.0
14492,github : add OpenCL backend to issue templates,EZForever,2025-07-02T04:05:01Z,2025-07-02T05:41:35+00:00,1,1.61
14494,simple-chat : fix context-exceeded condition,ggerganov,2025-07-02T05:31:45Z,2025-07-02T11:12:08+00:00,2,5.67
14497,"CUDA: add dynamic shared mem to softmax, refactor general usage",am17an,2025-07-02T08:07:55Z,2025-07-02T23:45:11+00:00,2,15.62
14498,musa: upgrade musa sdk to rc4.2.0,yeahdongcn,2025-07-02T08:18:13Z,2025-07-24T19:05:37+00:00,3,538.79
14500,CUDA: broadcasting for FlashAttention mask,JohannesGaessler,2025-07-02T08:53:41Z,2025-07-02T11:42:12+00:00,1,2.81
14501,ggml : remove kompute backend,ggerganov,2025-07-02T10:59:42Z,2025-07-03T04:48:32+00:00,1,17.81
14502,model : add support for apple/DiffuCoder-7B-cpGRPO,gabriellarson,2025-07-02T11:52:09Z,2025-07-02T22:45:53+00:00,1,10.9
14503,mtmd : Fix 32-bit narrowing issue in export-lora and mtmd clip,kiwi142857,2025-07-02T12:33:50Z,2025-07-25T11:08:05+00:00,2,550.57
14504,sycl: Fix conditional enabling following arch checks for ggml-sycl,s-Nick,2025-07-02T13:55:16Z,2025-07-03T09:00:03+00:00,1,19.08
14505,ggml : fix FA mask dim 2 and 3,ggerganov,2025-07-02T14:11:57Z,2025-07-03T07:46:57+00:00,3,17.58
14509,vulkan: support mixed/deepseekR1 FA head sizes,jeffbolznv,2025-07-02T20:14:09Z,2025-07-03T18:21:14+00:00,1,22.12
14510,opencl: broadcast for `soft_max`,lhez,2025-07-03T05:45:30Z,2025-07-03T18:22:24+00:00,1,12.62
14512,batch : add n_used count,ggerganov,2025-07-03T08:19:03Z,2025-07-04T06:04:59+00:00,1,21.77
14515,graph : prepare for 4D mask,ggerganov,2025-07-03T09:32:37Z,2025-07-04T06:05:37+00:00,2,20.55
14517,kv-cache : prepare K/V buffers for separation,ggerganov,2025-07-03T12:55:17Z,2025-07-09T09:55:03+00:00,2,141.0
14518,vulkan: Handle updated FA dim2/3 definition,jeffbolznv,2025-07-03T14:55:59Z,2025-07-05T07:26:04+00:00,3,40.5
14521,ggml: Add initial WebGPU backend,reeselevine,2025-07-03T19:21:57Z,2025-07-16T15:18:51+00:00,9,307.95
14529,CUDA: add bf16 and i32 to getrows,am17an,2025-07-04T08:53:07Z,2025-07-07T13:45:43+00:00,1,76.88
14534,llama: add initial support for Falcon-H1 model family,ibrahimkhadraoui,2025-07-04T14:21:39Z,2025-07-09T08:03:49+00:00,86,113.7
14535,OpenCL: add tiled mul_mat_f16_f32,rmatif,2025-07-04T17:53:10Z,2025-07-10T21:58:12+00:00,5,148.08
14544,server: Add ability to mount server at prefix,oluwandabira,2025-07-05T21:54:32Z,2025-07-08T08:47:33+00:00,5,58.88
14545,vulkan: fix rms_norm+mul fusion,jeffbolznv,2025-07-05T22:53:10Z,2025-07-06T08:08:17+00:00,1,9.25
14547,opencl: add `set_rows` for `f16` and `f32`,lhez,2025-07-06T07:25:03Z,2025-07-10T18:48:52+00:00,3,107.4
14551,CUDA: add set rows for f32 and f16,am17an,2025-07-06T14:30:54Z,2025-07-12T13:31:38+00:00,8,143.01
14554,vulkan: optimize flash attention split_k_reduce,jeffbolznv,2025-07-06T20:21:15Z,2025-07-08T18:11:42+00:00,1,45.84
14555,vulkan: optimizations for deepseek prompt processing,jeffbolznv,2025-07-06T22:36:19Z,2025-07-12T09:51:58+00:00,2,131.26
14560,model : add PLaMo-2 model,mitmul,2025-07-07T05:08:38Z,2025-07-15T16:11:42+00:00,18,203.05
14561,musa: fix build warnings (unused variable),yeahdongcn,2025-07-07T06:15:27Z,2025-07-07T23:58:31+00:00,1,17.72
14562,SYCL: Initial set_rows kernel implementation,qnixsynapse,2025-07-07T08:36:51Z,2025-07-10T08:29:38+00:00,29,71.88
14563,CUDA: add bilinear interpolation for upscale,am17an,2025-07-07T08:53:30Z,2025-07-08T02:11:18+00:00,2,17.3
14568,llama : remove ggml_cont where possible,CISC,2025-07-07T15:20:53Z,2025-07-07T19:35:08+00:00,3,4.24
14571,llama : fix incorrect minicpm3 v_states shape,CISC,2025-07-07T19:39:47Z,2025-07-07T21:35:35+00:00,1,1.93
14572,quantize: fix minor logic flaw in --tensor-type,EAddario,2025-07-07T20:51:25Z,2025-07-13T16:02:17+00:00,1,139.18
14574,vulkan: increase timeout for CI,jeffbolznv,2025-07-07T22:33:10Z,2025-07-08T07:38:31+00:00,2,9.09
14575,memory : fix broken batch splits for recurrent cache,compilade,2025-07-08T01:28:47Z,2025-07-08T15:37:48+00:00,1,14.15
14580,cuda : fix rope with partial rotation and non-cont src,ggerganov,2025-07-08T06:01:25Z,2025-07-08T07:15:21+00:00,1,1.23
14581,model : add SmolLM3,ngxson,2025-07-08T08:57:12Z,2025-07-08T16:07:01+00:00,2,7.16
14582,vulkan: fix rope with partial rotation and non-cont src,jeffbolznv,2025-07-08T12:10:24Z,2025-07-08T13:21:21+00:00,1,1.18
14584,model : fix hunyuan moe chat template,stevenkuang-tencent,2025-07-08T15:20:31Z,2025-07-08T16:29:29+00:00,1,1.15
14586,convert : fix smollm3 jinja template,ngxson,2025-07-08T23:15:18Z,2025-07-09T06:26:13+00:00,1,7.18
14587,vulkan: support SET_ROWS,jeffbolznv,2025-07-09T02:58:48Z,2025-07-12T10:12:26+00:00,1,79.23
14589,model : add skt/A.X-4.0 model vocabulary,Bing-su,2025-07-09T05:17:35Z,2025-07-09T08:22:31+00:00,1,3.08
14591,docker : add cann build pipline,diannaojiang,2025-07-09T07:57:01Z,2025-08-01T02:02:34+00:00,3,546.09
14596,"metal : fuse add, mul",ggerganov,2025-07-09T14:06:45Z,2025-07-18T17:37:26+00:00,1,219.51
14597,Smoldocling support,ryan-mangeno,2025-07-09T14:17:23Z,2025-07-10T17:41:00+00:00,14,27.39
14598,Docs: script to auto-generate ggml operations docs,am17an,2025-07-09T15:29:40Z,2025-07-10T15:29:01+00:00,12,23.99
14602,cuda : support Falcon-H1 state size for SSM_SCAN,compilade,2025-07-09T17:44:34Z,2025-07-10T03:54:38+00:00,4,10.17
14603,llama : remove llm_graph_input_one,ngxson,2025-07-09T18:56:41Z,2025-07-09T21:09:28+00:00,1,2.21
14605,llama : minor coding style fix for smollm3,ngxson,2025-07-09T21:56:25Z,2025-07-10T07:00:20+00:00,1,9.07
14608,cmake : llguidance build parser library only,EZForever,2025-07-10T02:49:39Z,2025-07-10T05:19:13+00:00,1,2.49
14609,cmake : bump llguidance version to v1.0.1,EZForever,2025-07-10T03:32:22Z,2025-07-10T05:19:37+00:00,1,1.79
14613,cmake : do not search for curl libraries by ourselves,EZForever,2025-07-10T08:58:51Z,2025-07-10T12:29:05+00:00,1,3.5
14617,sycl: Batched mulmat rework for oneDNN dispatch,ShanoToni,2025-07-10T15:20:44Z,2025-07-14T09:37:35+00:00,3,90.28
14618,SYCL: use 1D kernel for set_rows,qnixsynapse,2025-07-10T15:34:03Z,2025-07-14T09:37:55+00:00,2,90.06
14620,llama : support LiquidAI LFM2 hybrid model family,tdakhran,2025-07-10T16:00:52Z,2025-07-11T18:27:02+00:00,21,26.44
14624,"HIP: Enable Matrix cores for MMQ Kernels, Enable stream-K for CDNA 3",deepsek,2025-07-10T18:31:04Z,2025-07-26T22:28:15+00:00,80,387.95
14626,model : add Midm-2.0 model,Bing-su,2025-07-11T05:51:58Z,2025-07-11T07:36:05+00:00,1,1.74
14628,CUDA: 4D FlashAttention support,JohannesGaessler,2025-07-11T08:07:53Z,2025-07-11T11:10:21+00:00,1,3.04
14630,Add EXAONE 4.0 model architecture,lgai-exaone,2025-07-11T09:15:34Z,2025-07-18T08:45:49+00:00,20,167.5
14631,llama : move enum llama_vocab_pre_type to implementation,ggerganov,2025-07-11T09:41:16Z,2025-07-11T10:46:07+00:00,1,1.08
14634,HIP: Add HIP 7.0+ compatibility for hipBLAS compute types,slojosic-amd,2025-07-11T10:42:22Z,2025-07-11T16:55:00+00:00,1,6.21
14638,common: add config presets for falcon,0xs1d,2025-07-11T22:31:28Z,2025-09-19T14:21:41+00:00,2,1671.84
14644,Support diffusion models: Add Dream 7B,am17an,2025-07-12T07:33:07Z,2025-07-16T12:03:51+00:00,16,100.51
14645,server : fix pooled embedding output,iamlemec,2025-07-12T08:21:49Z,2025-07-12T10:21:02+00:00,1,1.99
14650,readme : add LFM2 to models section,tdakhran,2025-07-12T14:07:14Z,2025-07-12T17:07:08+00:00,1,3.0
14651,test-backend-ops : cover lfm2 cases in test_ssm_conv,tdakhran,2025-07-12T14:35:42Z,2025-07-12T17:10:14+00:00,1,2.58
14653,vulkan: add RTE variants for glu/add/sub/mul/div,jeffbolznv,2025-07-12T18:54:48Z,2025-07-15T19:32:11+00:00,2,72.62
14654,Model : Add support for Kimi-K2,gabriellarson,2025-07-12T19:03:04Z,2025-07-15T19:54:22+00:00,11,72.86
14656,Add CMake presets for Linux and GCC,YavorGIvanov,2025-07-12T23:41:51Z,2025-07-13T05:12:37+00:00,1,5.51
14657,Add ELU CUDA support,YavorGIvanov,2025-07-12T23:59:02Z,2025-07-13T09:33:16+00:00,1,9.57
14658,Model: Add support for Ernie 4.5 MoE,pwilkin,2025-07-13T00:29:15Z,2025-07-17T21:15:32+00:00,18,116.77
14659,Add Pad Reflect 1D CUDA support,YavorGIvanov,2025-07-13T00:46:13Z,2025-08-22T11:06:29+00:00,1,970.34
14660,Add missing unary ops Metal support,YavorGIvanov,2025-07-13T01:13:50Z,2025-07-13T05:38:14+00:00,1,4.41
14665,llama : add jinja template for rwkv-world,MollySophia,2025-07-13T13:50:56Z,2025-07-13T23:43:43+00:00,5,9.88
14668,scripts: benchmark for HTTP server throughput,JohannesGaessler,2025-07-13T20:04:19Z,2025-07-14T11:14:31+00:00,7,15.17
14672,llama-context: add ability to get logits,am17an,2025-07-14T08:10:31Z,2025-07-14T13:01:42+00:00,1,4.85
14673,PPC:Refactor llamafile_sgemm code,shalinib-ibm,2025-07-14T10:01:26Z,2025-07-14T13:16:42+00:00,1,3.25
14675,bug fix: handle saving/loading null layers in recurrent memory,l3utterfly,2025-07-14T10:53:28Z,2025-07-23T08:16:41+00:00,6,213.39
14676,kleidiai: add support for get_rows,chaxu01,2025-07-14T11:36:26Z,2025-07-21T13:49:52+00:00,4,170.22
14677,sycl: Hotfix for non dnnl codepath,ShanoToni,2025-07-14T12:38:36Z,2025-07-14T17:12:42+00:00,1,4.57
14683,vulkan: fix noncontig check for mat_mul_id splitting,jeffbolznv,2025-07-14T21:43:41Z,2025-07-15T19:51:10+00:00,5,22.12
14687,cuda: fix build warnings in set-rows.cu (unused variable),yeahdongcn,2025-07-15T05:42:12Z,2025-07-15T07:28:53+00:00,1,1.78
14695,scripts: synthetic prompt mode for server-bench.py,JohannesGaessler,2025-07-15T12:36:35Z,2025-07-16T07:33:28+00:00,1,18.95
14701,convert : add pre-computed hashes first to prevent order mishaps,CISC,2025-07-15T21:02:47Z,2025-07-16T06:51:12+00:00,1,9.81
14704,convert : only check for tokenizer folder if we need it,CISC,2025-07-15T21:51:50Z,2025-07-16T06:52:05+00:00,1,9.0
14705,model : fix parallel processing for lfm2,tdakhran,2025-07-15T23:01:41Z,2025-07-17T07:22:12+00:00,1,32.34
14707,vulkan: Add logging for bf16 features to ggml_vk_print_gpu_info (#13274),Peter0x44,2025-07-16T02:05:26Z,2025-07-19T15:58:03+00:00,1,85.88
14710,server : fix handling of the ignore_eos flag,ggerganov,2025-07-16T04:51:23Z,2025-07-16T09:13:58+00:00,6,4.38
14711,Support Cosyvoice2-0.5B By allowing Qwen2 architecture to have a optional bias tensor,tempstudio,2025-07-16T05:00:45Z,2025-07-16T15:02:07+00:00,1,10.02
14712,CUDA: set_rows + cpy.cu refactor,am17an,2025-07-16T05:26:36Z,2025-07-18T06:54:18+00:00,1,49.46
14716,Bug: fix inputs to conv1d in mamba layer of plamo2,mitmul,2025-07-16T08:20:46Z,2025-07-16T10:12:22+00:00,1,1.86
14717,convert : make hf token optional,CISC,2025-07-16T09:45:37Z,2025-07-16T21:17:43+00:00,1,11.54
14720,ggml : add asserts,ggerganov,2025-07-16T10:30:15Z,2025-07-16T11:43:32+00:00,1,1.22
14723,ci : disable failing vulkan crossbuilds,CISC,2025-07-16T11:55:45Z,2025-07-16T23:52:08+00:00,2,11.94
14725,Fix parameter order issue for hybrid memory initialization,dinerburger,2025-07-16T15:16:27Z,2025-07-16T19:17:26+00:00,1,4.02
14726,nix: use optionalAttrs for `env` mkDerivation attrset argument,amozeo,2025-07-16T18:39:50Z,2025-07-17T22:18:16+00:00,1,27.64
14731,feat: Add optional prompt processing progress streaming,baonudesifeizhai,2025-07-17T04:18:04Z,2025-09-06T11:34:29+00:00,5,1231.27
14732,use max work group size for device to replace the magic number,NeoZhangJianyu,2025-07-17T05:30:03Z,2025-07-18T02:23:14+00:00,3,20.89
14736,Documentation: Update build.md's Vulkan section,rspOverflow,2025-07-17T12:18:41Z,2025-07-19T10:18:36+00:00,1,46.0
14737,Improve Mistral models integration with llama.cpp,juliendenize,2025-07-17T12:30:40Z,2025-08-11T08:07:49+00:00,20,595.62
14741,Fix Gemma3n not executed as CUDA_GRAPH on NVGPUs,ORippler,2025-07-17T16:29:52Z,2025-07-18T11:35:32+00:00,1,19.09
14743,metal: SSM_SCAN performance,gabe-l-hart,2025-07-17T17:45:49Z,2025-07-25T16:47:39+00:00,14,191.03
14744,[ROCm] Fix HIP version check for HIPBLAS V2 API compatibility,danielholanda,2025-07-17T17:48:49Z,2025-07-23T00:23:02+00:00,1,126.57
14748,Move the graph placeholder message to debug mode,Nexesenex,2025-07-17T23:26:47Z,2025-07-18T04:25:54+00:00,1,4.99
14750,Fix MinicpmV model converter and clip to avoid using hardcode.,gryffindor-rr,2025-07-18T06:24:43Z,2025-08-11T14:12:12+00:00,4,583.79
14753,graph : avoid huge warm-up graphs for MoE models,ggerganov,2025-07-18T08:47:28Z,2025-07-18T11:31:15+00:00,2,2.73
14763,cuda : implement bf16 cpy ops and enable bf16 cont,CISC,2025-07-18T20:55:22Z,2025-07-22T10:33:10+00:00,6,85.63
14770,Vulkan: Fix fprintf format-security warning,0cc4m,2025-07-19T09:46:17Z,2025-07-19T15:47:53+00:00,1,6.03
14771,Add LLaDA 8b Diffusion model,am17an,2025-07-19T09:50:27Z,2025-07-31T11:49:10+00:00,33,289.98
14775,Contrib: add 0cc4m as codeowner for Vulkan backend,0cc4m,2025-07-19T16:17:42Z,2025-07-19T20:47:22+00:00,1,4.49
14779,Clang-format: local files first + fix BinPacking,am17an,2025-07-20T03:14:54Z,2025-07-20T11:42:34+00:00,1,8.46
14780,Fix link for tools/perplexity in README.md,am17an,2025-07-20T06:42:50Z,2025-07-20T18:13:47+00:00,6,11.52
14783,server: Add parse_special option to /tokenize endpoint,IsaacDynamo,2025-07-20T12:50:53Z,2025-07-21T07:24:51+00:00,1,18.57
14789,vulkan/cuda: Fix im2col when KW!=KH,jeffbolznv,2025-07-21T01:08:43Z,2025-07-21T11:35:40+00:00,2,10.45
14790,cuda: remove linking to cublasLt,yeahdongcn,2025-07-21T01:28:14Z,2025-07-21T23:45:26+00:00,2,22.29
14794,llama : fix `--reverse-prompt` crashing issue,MollySophia,2025-07-21T08:25:26Z,2025-07-21T09:38:36+00:00,1,1.22
14797,sycl: Fix im2col,Rbiessy,2025-07-21T14:49:49Z,2025-07-21T16:39:29+00:00,3,1.83
14799,server : allow setting `--reverse-prompt` arg,MollySophia,2025-07-21T14:58:19Z,2025-07-22T01:24:23+00:00,1,10.43
14800,CUDA: add fused rms norm,am17an,2025-07-21T15:16:13Z,2025-07-23T01:25:42+00:00,16,34.16
14801,docs: add libcurl-dev install hint for Linux distros,PouyaGhahramanian,2025-07-21T15:31:48Z,2025-07-24T09:26:44+00:00,4,65.92
14803,opencl : fix im2col when KW!=KH,CISC,2025-07-21T15:37:42Z,2025-07-21T20:55:10+00:00,1,5.29
14806,opencl: remove unreachable `return`,lhez,2025-07-22T05:35:49Z,2025-07-22T06:53:30+00:00,1,1.29
14809,opencl: tiled mul_mat with local memory for f16 and f32 ,lhez,2025-07-22T07:08:26Z,2025-07-30T21:56:55+00:00,1,206.81
14814,sycl: unified semantics of block offset calculation,Alcpz,2025-07-22T11:44:28Z,2025-07-24T10:09:57+00:00,6,46.42
14815,sycl: refactor quantization to q8_1 ,Alcpz,2025-07-22T12:25:05Z,2025-07-28T10:05:54+00:00,10,141.68
14817,vulkan: fix rms_norm_mul to handle broadcasting dim0,jeffbolznv,2025-07-22T14:18:11Z,2025-07-22T15:35:22+00:00,1,1.29
14818,gguf: use an unordered_map for faster duplicate tensor name lookups,struct,2025-07-22T15:15:03Z,2025-07-22T17:34:36+00:00,1,2.33
14822,CUDA: fix quantized KV cache + multiple sequences,JohannesGaessler,2025-07-22T19:52:06Z,2025-07-23T10:35:53+00:00,1,14.73
14823,convert: text-only support for GLM-4.1V-9B-Thinking,jacekpoplawski,2025-07-22T23:14:14Z,2025-07-23T21:23:57+00:00,3,22.16
14825,graph : reduce splits for recurrent and hybrid models,compilade,2025-07-23T03:07:40Z,2025-07-31T05:02:46+00:00,3,193.92
14837,CUDA: fix compilation with GGML_CUDA_F16,JohannesGaessler,2025-07-23T15:15:20Z,2025-07-23T16:22:30+00:00,1,1.12
14838,Code health: Remove invalid `portPos` specifiers from graph dumping to dot files,ORippler,2025-07-23T16:31:41Z,2025-07-25T11:29:57+00:00,1,42.97
14839,SvelteKit-based WebUI,allozaur,2025-07-23T16:44:31Z,2025-09-17T17:29:14+00:00,7,1344.75
14840,"CUDA: fix overflow in FA, tune performance",JohannesGaessler,2025-07-23T17:40:54Z,2025-07-23T19:43:25+00:00,5,2.04
14841,opencl: add fused `rms_norm_mul`,lhez,2025-07-23T21:42:35Z,2025-07-25T15:12:14+00:00,1,41.49
14842,imatrix : use GGUF by default,compilade,2025-07-24T02:37:07Z,2025-08-03T20:00:05+00:00,2,257.38
14843,sycl: fix undefined variable in work group size check,djeong20,2025-07-24T02:47:19Z,2025-07-24T04:50:42+00:00,1,2.06
14850,fix: restore MiniCPM inference after Granite Four changes,jk3456a,2025-07-24T07:09:28Z,2025-07-24T09:50:52+00:00,1,2.69
14852,chat : fix kimi-k2 chat template,ngxson,2025-07-24T09:41:41Z,2025-07-24T11:59:57+00:00,1,2.3
14853,context : perform output reorder lazily upon access after sync,ggerganov,2025-07-24T11:43:28Z,2025-07-24T13:31:48+00:00,1,1.81
14855,sched : fix multiple evaluations of the same graph with pipeline parallelism,slaren,2025-07-24T14:04:47Z,2025-07-25T08:07:26+00:00,1,18.04
14860,vulkan: skip empty set_rows to avoid invalid API usage,jeffbolznv,2025-07-24T18:40:07Z,2025-07-27T09:05:34+00:00,2,62.42
14862,mtmd : add support for Voxtral,ngxson,2025-07-24T19:44:47Z,2025-07-28T13:01:48+00:00,7,89.28
14864,Adding chat template support for Granite model,smdesai,2025-07-24T21:18:16Z,2025-08-06T18:27:31+00:00,10,309.15
14865,Extend test case filtering,tlemo,2025-07-24T23:16:39Z,2025-07-28T16:04:27+00:00,9,88.8
14868,GGML: Check for null buffers in get/set/copy tensor RPC endpoints,struct,2025-07-25T02:45:31Z,2025-07-25T10:17:03+00:00,1,7.53
14869,musa: fix build warnings (unused variable),yeahdongcn,2025-07-25T03:09:09Z,2025-07-26T02:36:02+00:00,1,23.45
14870,context : restore preemptive sched reset when LLAMA_SET_ROWS=0,ggerganov,2025-07-25T08:05:52Z,2025-07-25T11:28:06+00:00,1,3.37
14872,vulkan : add fp16 support for the conv_2d kernel,Green-Sky,2025-07-25T09:45:45Z,2025-07-27T10:04:33+00:00,7,48.31
14874,docs: update HOWTO‑add‑model.md for ModelBase and new model classes,wooksong,2025-07-25T11:33:00Z,2025-07-25T14:25:05+00:00,8,2.87
14875,Support intern-s1,RunningLeon,2025-07-25T11:33:20Z,2025-08-07T16:20:40+00:00,19,316.79
14878,model: add hunyuan dense,stevenkuang-tencent,2025-07-25T13:23:05Z,2025-08-01T13:31:12+00:00,20,168.14
14880,ggml-cpu: disable GGML_NNPA by default due to instability,taronaeo,2025-07-25T13:39:49Z,2025-07-25T17:09:04+00:00,1,3.49
14883,SYCL: Add set_rows support for quantized types ,qnixsynapse,2025-07-26T06:34:07Z,2025-07-28T15:02:15+00:00,4,56.47
14884,CANN: Implement GLU ops,hipudding,2025-07-26T07:37:08Z,2025-07-26T09:56:18+00:00,1,2.32
14889,Docs: add instructions in ops.md + simplify backend csv,am17an,2025-07-26T15:25:04Z,2025-07-27T01:36:43+00:00,2,10.19
14892,Fix kq_scale for the attention layers of PLaMo2,mitmul,2025-07-26T17:45:33Z,2025-07-27T07:38:45+00:00,4,13.89
14895,llama : clarify comment about pp and tg graphs [no ci],danbev,2025-07-27T05:15:11Z,2025-07-27T10:10:52+00:00,1,4.93
14896,make rope_yarn_log_mul optional for deepseek2,gabriellarson,2025-07-27T05:31:38Z,2025-07-27T08:18:37+00:00,2,2.78
14897,ggml-cpu : deduplicate scalar implementations,xctan,2025-07-27T07:01:40Z,2025-07-28T15:40:24+00:00,5,32.65
14898,Add support for SmallThinker model series,wdl339,2025-07-27T07:17:12Z,2025-07-28T11:47:00+00:00,13,28.5
14899,Vulkan: Fix minor debug mode issues,0cc4m,2025-07-27T08:57:24Z,2025-07-31T15:46:55+00:00,5,102.83
14900,Vulkan: add ops docs,0cc4m,2025-07-27T11:14:43Z,2025-07-27T13:33:08+00:00,1,2.31
14903,Vulkan: Add Integer Dot Product mul_mat_vec shader for legacy quants,0cc4m,2025-07-27T15:31:17Z,2025-09-01T14:19:07+00:00,15,862.8
14905,quantize: update README.md,EAddario,2025-07-27T16:15:30Z,2025-07-27T21:31:12+00:00,1,5.26
14907,cuda : add softcap fusion,CISC,2025-07-27T19:45:20Z,2025-07-29T12:22:03+00:00,8,40.61
14910,opencl: add ops docs,lhez,2025-07-28T04:17:42Z,2025-07-28T16:50:17+00:00,1,12.54
14914,ops : update BLAS,ggerganov,2025-07-28T06:43:49Z,2025-07-28T08:01:03+00:00,1,1.29
14916,CUDA: fix pointer incrementation in FA,JohannesGaessler,2025-07-28T10:40:03Z,2025-07-28T12:30:22+00:00,1,1.84
14917,llama-bench : use local GPUs along with RPC servers,rgerganov,2025-07-28T12:19:53Z,2025-07-28T15:59:04+00:00,1,3.65
14919,CUDA: add roll,am17an,2025-07-28T12:32:49Z,2025-07-29T06:45:19+00:00,1,18.21
14924,CUDA: skip masked KV slices for all FA kernels,JohannesGaessler,2025-07-28T18:12:49Z,2025-07-30T13:46:13+00:00,1,43.56
14927,embeddings: fix extraction of CLS pooling results,iamlemec,2025-07-28T19:46:20Z,2025-07-30T05:25:05+00:00,1,33.65
14929,server-bench: make seed choice configurable,JohannesGaessler,2025-07-28T20:19:25Z,2025-07-29T08:40:50+00:00,3,12.36
14930,HIP: add GGML_HIP_MMQ_MFMA option to allow disableing the MFMA path.,IMbackK,2025-07-28T21:00:51Z,2025-07-29T15:44:31+00:00,1,18.73
14931,HIP: Ignore unsupported unroll transformation in fattn-vec,IMbackK,2025-07-28T21:11:12Z,2025-07-29T15:43:43+00:00,1,18.54
14933,vulkan: optimizations for direct convolution,jeffbolznv,2025-07-29T02:36:45Z,2025-08-02T07:57:04+00:00,3,101.34
14934,vulkan: coopmat2 mul_mat optimizations,jeffbolznv,2025-07-29T04:46:57Z,2025-08-02T09:21:38+00:00,1,100.58
14935,[CANN] update ops docs,bachelor-dou,2025-07-29T06:55:46Z,2025-07-30T00:39:24+00:00,1,17.73
14937,bug-fix: handle broken UTF-8 sequences in common_chat_parse(),kallewoof,2025-07-29T07:44:13Z,2025-07-29T15:05:38+00:00,1,7.36
14939,model: Add support for GLM 4.5 family of models (#14921),sammcj,2025-07-29T08:18:17Z,2025-08-04T18:29:25+00:00,67,154.19
14943,CANN: Add ggml_set_rows,hipudding,2025-07-29T11:54:21Z,2025-07-29T14:36:44+00:00,1,2.71
14944,ggml : fix field name when new ggml_backend,aisk,2025-07-29T13:19:01Z,2025-08-08T12:37:23+00:00,1,239.31
14945,"HIP: remove the use of __HIP_PLATFORM_AMD__, explicitly support only AMD targets",IMbackK,2025-07-29T16:20:21Z,2025-07-29T18:23:05+00:00,1,2.05
14946,server: Add openai-style logit_bias support,lukasstraub2,2025-07-29T18:05:37Z,2025-07-31T12:08:23+00:00,2,42.05
14947,SYCL: experimental gemv kernel for q4_K ,Alcpz,2025-07-29T18:05:45Z,2025-08-01T11:58:40+00:00,4,65.88
14949,HIP: enable mfma mmq on gfx908 and gfx90a for select datatypes and shapes,IMbackK,2025-07-29T18:43:09Z,2025-07-30T15:38:07+00:00,5,20.92
14961,tests : update for LLAMA_SET_ROWS=1,ggerganov,2025-07-30T07:39:20Z,2025-07-30T12:12:02+00:00,5,4.54
14962,chat : fix multiple tool_calls on hermes-2-pro,jhen0409,2025-07-30T09:13:19Z,2025-08-02T10:04:48+00:00,1,72.86
14964,server : add support for `embd_normalize` parameter,danbev,2025-07-30T10:02:46Z,2025-07-30T16:07:12+00:00,1,6.07
14968,Refactor: Merge build_moe_ffn_from_probs function into build_moe_ffn,wdl339,2025-07-30T14:08:18Z,2025-07-31T12:12:20+00:00,1,22.07
14973,fix: using combined imatrix to quantise models triggers an error #14952,EAddario,2025-07-30T16:32:10Z,2025-07-30T19:11:56+00:00,1,2.66
14975,ggml: initial IBM zDNN backend,taronaeo,2025-07-30T17:58:22Z,2025-08-15T13:11:22+00:00,7,379.22
14978,ggml: WebGPU backend host improvements and style fixing,reeselevine,2025-07-30T22:22:34Z,2025-08-04T15:52:43+00:00,2,113.5
14981,Create Pmll.cpp,drQedwards,2025-07-31T03:53:51Z,2025-07-31T09:19:50+00:00,1,5.43
14982,vulkan: Use coopmat2 for conv2d,jeffbolznv,2025-07-31T04:00:43Z,2025-08-03T12:23:58+00:00,1,80.39
14983,Support MiniCPM-V 4.0,tc-mb,2025-07-31T05:42:06Z,2025-07-31T15:22:17+00:00,3,9.67
14984,"opencl: add f16 variants for `add`, `sub`, `mul`, `div`",lhez,2025-07-31T06:56:25Z,2025-08-01T11:15:44+00:00,1,28.32
14985,CANN: Improve loading efficiency after converting weights to NZ format.,hipudding,2025-07-31T07:18:38Z,2025-07-31T11:47:20+00:00,3,4.48
14987,OpenCL: add initial FA support,rmatif,2025-07-31T12:31:52Z,2025-08-16T08:05:55+00:00,12,379.57
14990,"llama : allow other bufts when overriding to CPU, add --no-repack option",slaren,2025-07-31T13:40:28Z,2025-07-31T16:11:34+00:00,1,2.52
14992,llama : add simple option to enable CPU for MoE weights (--cpu-moe),slaren,2025-07-31T16:10:36Z,2025-07-31T18:15:41+00:00,2,2.08
14993,Fix params bug in diffusion example,am17an,2025-07-31T16:13:29Z,2025-07-31T17:22:58+00:00,1,1.16
14994,imatrix : fix 3d activation handling for hybrid and recurrent models,compilade,2025-07-31T16:41:08Z,2025-08-03T19:49:13+00:00,1,75.13
14995,quantize: skip tensor override when in fallback mode,EAddario,2025-07-31T17:36:22Z,2025-07-31T19:32:18+00:00,1,1.93
14997,Handling delta.reasoning_content in llama-server's webUI,mostlygeek,2025-07-31T18:57:25Z,2025-08-03T06:42:54+00:00,2,59.76
15001,server: enable token array inputs for OAI API,JohannesGaessler,2025-07-31T22:37:45Z,2025-08-02T08:12:41+00:00,1,33.58
15003,llama-bench: rename DB table name from test to llama_bench,yeahdongcn,2025-08-01T02:37:54Z,2025-08-02T09:20:41+00:00,1,30.71
15005,fix compile bug when the BASE_CUDA_DEV_CONTAINER is based on Ubuntu 2…,simevo,2025-08-01T06:42:42Z,2025-08-14T15:45:28+00:00,1,321.05
15014,CUDA: fix MMQ nwarps for AMD with warp_size==32,JohannesGaessler,2025-08-01T16:31:31Z,2025-08-01T18:47:33+00:00,1,2.27
15015,vulkan: Support ne[3]>1 in noncontig matrix-vector multiply,jeffbolznv,2025-08-01T16:43:12Z,2025-08-02T08:48:30+00:00,2,16.09
15023,Support conversion of Qwen3-Embedding models,iamlemec,2025-08-01T21:27:41Z,2025-08-02T08:44:50+00:00,2,11.29
15025,cuda: make im2col faster,leejet,2025-08-02T04:10:04Z,2025-08-02T14:15:36+00:00,1,10.09
15029,opencl: fix adreno compiler feature detection logic,lhez,2025-08-02T06:59:29Z,2025-08-02T17:51:18+00:00,1,10.86
15030,Fix Qwen3-Embedding pre-tokenizer hash,iamlemec,2025-08-02T09:28:59Z,2025-08-02T10:51:02+00:00,1,1.37
15032,ci : check that pre-tokenizer hashes are up-to-date,CISC,2025-08-02T11:00:58Z,2025-08-02T12:39:01+00:00,1,1.63
15035,CUDA: use mma FA kernel for gqa > 4 on RTX 4000,JohannesGaessler,2025-08-02T11:43:28Z,2025-08-02T14:37:08+00:00,1,2.89
15045,fix tokenizer for JetBrain Mellum,csabakecskemeti,2025-08-02T22:48:58Z,2025-08-03T19:38:18+00:00,1,20.82
15050,memory : handle kv_unified for hybrid models,compilade,2025-08-03T05:05:45Z,2025-08-03T19:43:07+00:00,1,14.62
15051,model : add text-only support for Kimi-VL,gabriellarson,2025-08-03T06:18:08Z,2025-08-03T14:56:25+00:00,1,8.64
15057,ggml-cpu : add basic RVV support for vector f32 ops,xctan,2025-08-03T16:55:03Z,2025-08-27T08:44:22+00:00,1,567.82
15062,vulkan: fix build when using glslang that does not support coopmat2,jeffbolznv,2025-08-04T01:05:25Z,2025-08-04T05:09:19+00:00,1,4.07
15065,CANN: add optional support for ACL Graph execution,noemotiovon,2025-08-04T07:02:11Z,2025-08-06T06:12:42+00:00,9,47.18
15070,CANN: GGML_OP_CPY optimization,noemotiovon,2025-08-04T11:59:44Z,2025-08-12T08:12:13+00:00,2,188.21
15071,quantize : fix confusing error message if ftype is invalid,CISC,2025-08-04T14:13:48Z,2025-08-04T16:11:02+00:00,1,1.95
15072,OpenCL: fix profiling crash in llama-bench,rmatif,2025-08-04T14:39:06Z,2025-08-06T21:17:52+00:00,1,54.65
15074,cmake: Add GGML_BACKEND_DIR option,ckastner,2025-08-04T16:17:27Z,2025-08-04T19:29:14+00:00,1,3.2
15075,gguf-py : add --chat-template-file to gguf_new_metadata,CISC,2025-08-04T16:34:09Z,2025-08-04T19:01:48+00:00,1,2.46
15076,imatrix : warn when GGUF imatrix is saved without .gguf suffix,compilade,2025-08-04T19:32:08Z,2025-08-04T21:26:53+00:00,1,1.91
15078,ggml: WebGPU disable SET_ROWS for now,reeselevine,2025-08-05T00:42:47Z,2025-08-05T23:26:38+00:00,3,22.73
15080,context : fix index overflow on huge outputs,compilade,2025-08-05T02:50:00Z,2025-08-05T09:27:45+00:00,1,6.63
15081,webui: fix markdown table,dindinw,2025-08-05T06:29:49Z,2025-08-05T11:56:44+00:00,2,5.45
15086,chat : only remove double bos/eos if added,CISC,2025-08-05T10:57:52Z,2025-08-05T18:43:36+00:00,1,7.76
15091,llama : add gpt-oss,ggerganov,2025-08-05T15:19:49Z,2025-08-05T19:10:36+00:00,4,3.85
15092,sycl: fix mul_mat selection,Rbiessy,2025-08-05T15:21:41Z,2025-08-05T16:39:55+00:00,2,1.3
15094,ggml: Skip backend library linking code when GGML_BACKEND_DL=ON ,ckastner,2025-08-05T16:47:39Z,2025-08-07T11:45:41+00:00,4,42.97
15103,HIP: Add option to export kernel resource useage metrics,IMbackK,2025-08-05T21:50:37Z,2025-08-07T14:44:14+00:00,1,40.89
15108,mtmd: server: Support multimodal data prompt in /completions and /embeddings endpoint of server,65a,2025-08-06T01:54:28Z,2025-08-22T08:10:14+00:00,30,390.26
15109,vulkan: Add env var to disable host visible vidmem,jeffbolznv,2025-08-06T02:09:11Z,2025-08-07T20:07:11+00:00,1,41.97
15111,gguf-py : add Numpy MXFP4 de/quantization support,compilade,2025-08-06T03:26:50Z,2025-08-08T21:48:26+00:00,5,66.36
15114,model : fix hunyuan chat template,stevenkuang-tencent,2025-08-06T07:54:19Z,2025-08-06T09:48:31+00:00,1,1.9
15115,ggml: aarch64: Implement SVE F16 kernels for vector functions ,Vithulep,2025-08-06T09:24:38Z,2025-09-01T18:13:16+00:00,2,632.81
15121,opencl: add `swiglu_oai` and  `add_id`,lhez,2025-08-06T12:39:26Z,2025-08-06T19:12:17+00:00,1,6.55
15126,vulkan: support flash attention sinks,jeffbolznv,2025-08-06T14:44:56Z,2025-08-07T20:44:20+00:00,2,29.99
15131,CUDA: GEMM for FP32/FP16/BF16 and ne11 <= 16,JohannesGaessler,2025-08-06T17:38:12Z,2025-08-07T08:53:21+00:00,1,15.25
15132,"CUDA: Optimize `reduce_rows_f32` kernel, leading up to 25x perf improvement on kernel-level and 10% perf increase for Gemma3n",ORippler,2025-08-06T18:04:33Z,2025-08-13T08:04:47+00:00,15,158.0
15133,scripts: fix crash when --tool is not set,JohannesGaessler,2025-08-06T18:12:06Z,2025-08-07T06:50:30+00:00,1,12.64
15134,requirements : fix PyTorch uint64 compatibility,danbev,2025-08-06T18:48:18Z,2025-08-07T03:31:49+00:00,1,8.73
15137,ggml: Add basic SET_ROWS support in WebGPU,reeselevine,2025-08-06T19:31:56Z,2025-08-06T22:14:40+00:00,1,2.71
15140,OpenCL: allow mixed f16/f32 add,rmatif,2025-08-06T21:43:50Z,2025-08-12T09:42:41+00:00,1,131.98
15145,ggml: SVE support for exponential functions,s-goto-11,2025-08-07T06:05:04Z,2025-09-01T18:13:49+00:00,1,612.15
15150,kleidiai: fix unsigned overflow bug,chaxu01,2025-08-07T11:11:04Z,2025-08-11T07:59:26+00:00,3,92.81
15151,sycl: Fix and disable more configurations of mul_mat,Rbiessy,2025-08-07T13:38:51Z,2025-08-12T11:58:22+00:00,2,118.33
15152,opencl: support sinks in `soft_max` (attn sinks),lhez,2025-08-07T14:06:17Z,2025-08-08T04:47:03+00:00,1,14.68
15153,convert : support non-mxfp4 HF model,ngxson,2025-08-07T16:11:23Z,2025-08-07T21:26:03+00:00,3,5.24
15157,CUDA: attention sinks for mma FlashAttention,JohannesGaessler,2025-08-07T19:41:33Z,2025-08-08T06:19:58+00:00,5,10.64
15158,GPT-OSS: parse commentary tool calls; handle glued 'json'; add unit tests  (#15102),Nerexis,2025-08-07T20:30:32Z,2025-08-15T10:46:17+00:00,2,182.26
15161,vendor: sync minja,ochafik,2025-08-08T01:24:25Z,2025-08-08T09:45:18+00:00,1,8.35
15172,server : enable -td and -tbd parameters,CISC,2025-08-08T12:56:14Z,2025-08-13T13:43:00+00:00,2,120.78
15176,kv-cache : log (debug) all streams in find_slot,danbev,2025-08-08T14:08:42Z,2025-08-11T09:21:20+00:00,1,67.21
15178,CUDA: add attention sinks for tile and wmma,am17an,2025-08-08T17:16:07Z,2025-08-09T12:00:24+00:00,2,18.74
15179,"server-bench: external OAI servers, sqlite",JohannesGaessler,2025-08-08T17:59:59Z,2025-08-08T21:04:36+00:00,6,3.08
15181,gpt-oss: implement harmony parsing,aldehir,2025-08-08T18:51:44Z,2025-08-14T14:23:12+00:00,1,139.52
15182,ggml: add `conv3d` op,rmatif,2025-08-08T19:25:37Z,2025-08-22T13:33:15+00:00,1,330.13
15188,ggml-rpc: chunk send()/recv() to avoid EINVAL for very large tensors over RPC (macOS & others),Tak-RS,2025-08-09T01:19:39Z,2025-08-13T05:54:30+00:00,7,100.58
15191," common : add --override-tensor-draft, --cpu-moe-draft and --n-cpu-moe-draft parameters",copilot-swe-agent,2025-08-09T07:18:28Z,2025-08-13T10:44:41+00:00,13,99.44
15195,Fix GPT-OSS Harmony format end token handling crash,trvon,2025-08-09T17:53:38Z,2025-08-09T20:25:09+00:00,2,2.53
15200,"fix: llama_memory_seq_rm(mem, -1, ...)",leok7v,2025-08-09T21:35:44Z,2025-08-12T05:23:19+00:00,1,55.79
15208,CANN: Add broadcast for softmax and FA,hipudding,2025-08-10T08:58:45Z,2025-08-11T14:50:31+00:00,1,29.86
15214,ci : add copilot-setup-steps.yml,CISC,2025-08-10T15:15:19Z,2025-08-13T07:07:13+00:00,1,63.87
15221,ci : Fix -Werror=return-type in clip.cpp so ci/run.sh can run without issue,michaelgiba,2025-08-11T01:52:26Z,2025-08-21T10:06:47+00:00,3,248.24
15230,model : gpt-oss simulate think tags when reasoning,aldehir,2025-08-11T08:58:32Z,2025-08-11T21:44:09+00:00,4,12.76
15236,musa: fix failures in test-backend-ops for mul_mat_id op,yeahdongcn,2025-08-11T11:21:19Z,2025-08-12T02:02:52+00:00,1,14.69
15241,Fix HIP warp synchronization function conflicts for ROCm 7.0+,slojosic-amd,2025-08-11T13:04:57Z,2025-08-12T17:20:02+00:00,2,28.25
15246,vulkan: perf_logger improvements,jeffbolznv,2025-08-11T16:10:51Z,2025-08-14T13:38:10+00:00,3,69.46
15252,vulkan: fuse adds,jeffbolznv,2025-08-11T21:14:52Z,2025-08-16T16:48:22+00:00,3,115.56
15258,musa: fix build warnings,yeahdongcn,2025-08-12T02:17:34Z,2025-08-20T02:17:37+00:00,1,192.0
15260,CUDA cmake: add compiler flag to add lineinfo,am17an,2025-08-12T06:03:33Z,2025-08-12T09:21:46+00:00,1,3.3
15267,ci: Enable GGML_CPU_ALL_VARIANTS for ARM,ckastner,2025-08-12T14:13:30Z,2025-08-14T14:22:59+00:00,1,48.16
15270,opencl: add initial mxfp4 support via mv,lhez,2025-08-12T16:29:07Z,2025-08-15T16:52:15+00:00,1,72.39
15273,HIP: disable sync warp shuffel operators from clr amd_warp_sync_functions.h,IMbackK,2025-08-12T17:16:43Z,2025-08-12T20:15:12+00:00,1,2.97
15278,server : filter out harmony thought messages,aldehir,2025-08-13T01:54:51Z,2025-08-13T10:28:22+00:00,1,8.56
15281,"vulkan: optimize rms_norm, and allow the work to spread across multiple SMs",jeffbolznv,2025-08-13T04:23:36Z,2025-08-23T18:16:17+00:00,6,253.88
15282,vulkan.Dockerfile: install vulkan SDK using tarball,yeahdongcn,2025-08-13T05:31:00Z,2025-08-23T06:58:57+00:00,1,241.47
15285,HIP: Cleanup hipification header,IMbackK,2025-08-13T07:27:38Z,2025-08-14T14:23:56+00:00,1,30.94
15286,"Add comprehensive Copilot instructions with Python environment, server testing, and git clang-format",copilot-swe-agent,2025-08-13T07:31:15Z,2025-08-21T09:47:52+00:00,15,194.28
15288,ggml: riscv: add riscv spacemit backend,alex-spacemit,2025-08-13T08:29:45Z,2025-09-29T14:50:45+00:00,14,1134.35
15293,server : add SWA checkpoints,ggerganov,2025-08-13T13:49:52Z,2025-08-14T11:59:50+00:00,7,22.17
15295,fix(nix): remove non-functional llama-cpp cachix cache from flake.nix,basnijholt,2025-08-13T17:06:05Z,2025-08-13T18:21:32+00:00,1,1.26
15296,HIP: bump requirement to rocm 6.1,IMbackK,2025-08-13T17:07:47Z,2025-08-13T18:44:30+00:00,3,1.61
15300,cuda : fix GGML_CUDA_GRAPHS=OFF,CISC,2025-08-13T20:14:04Z,2025-08-14T10:22:07+00:00,2,14.13
15303,perplexity: give more information about constraints on failure,kallewoof,2025-08-14T04:43:05Z,2025-08-14T06:16:32+00:00,4,1.56
15304,perplexity: Provide a helpful hint for has_cpl case in split_equal error.,kallewoof,2025-08-14T06:43:11Z,2025-08-14T11:03:31+00:00,1,4.34
15309,chat : include kwargs in template example,slaren,2025-08-14T11:27:53Z,2025-08-14T17:28:29+00:00,1,6.01
15314,"OpenCL: add fused group_norm/norm, mul, add",rmatif,2025-08-14T13:54:01Z,2025-08-27T06:36:05+00:00,1,304.7
15317,test-opt: fix backend support check,JohannesGaessler,2025-08-14T15:11:59Z,2025-08-15T09:23:17+00:00,1,18.19
15320,eval-callback : stop on first NaN,ggerganov,2025-08-14T15:40:43Z,2025-08-14T19:10:52+00:00,4,3.5
15321,CUDA: fix negative KV_max values in FA,JohannesGaessler,2025-08-14T15:47:45Z,2025-08-14T21:21:24+00:00,1,5.56
15324,ci : fix ios-xcode-build,CISC,2025-08-14T17:45:50Z,2025-08-15T12:02:40+00:00,16,18.28
15326,    common : use common_chat_templates for add_bos and add_eos,danbev,2025-08-14T18:09:20Z,2025-08-15T17:50:52+00:00,1,23.69
15327,aLoRA Support,gabe-l-hart,2025-08-14T19:32:40Z,2025-09-05T23:32:40+00:00,1,532.0
15328,ci : move ccache action to ggml-org fork,slaren,2025-08-14T19:36:47Z,2025-08-15T10:27:02+00:00,1,14.84
15331,CANN: fix ggml_cann_rms_norm,yuchuan-cao,2025-08-14T20:14:59Z,2025-08-22T08:24:41+00:00,2,180.16
15334,vulkan: Add missing bounds checking to scalar/coopmat1 mul_mat_id,jeffbolznv,2025-08-14T23:15:48Z,2025-08-16T08:58:38+00:00,2,33.71
15335,CANN: optimize the rope ops,YangShuai52,2025-08-15T01:50:32Z,2025-08-19T13:28:23+00:00,5,107.63
15337,vulkan: Support mul_mat_id with f32 accumulators,jeffbolznv,2025-08-15T03:56:18Z,2025-08-16T09:18:32+00:00,1,29.37
15340,vulkan : fix compile warnings on macos,ggerganov,2025-08-15T08:37:02Z,2025-08-15T13:28:28+00:00,2,4.86
15342,vulkan : fix out-of-bounds access in argmax kernel,ggerganov,2025-08-15T09:38:21Z,2025-08-15T14:16:37+00:00,4,4.64
15346,sched : copy only the used experts when offloading prompt processing,slaren,2025-08-15T14:26:27Z,2025-08-20T23:35:29+00:00,2,129.15
15347,model : support vision LiquidAI LFM2-VL family,tdakhran,2025-08-15T16:35:39Z,2025-08-16T21:33:54+00:00,5,28.97
15352,vulkan: disable spirv-opt for bfloat16 shaders,jeffbolznv,2025-08-15T18:13:38Z,2025-08-18T05:56:29+00:00,1,59.71
15354,vulkan: Optimize argsort,jeffbolznv,2025-08-15T19:14:19Z,2025-08-17T08:41:45+00:00,3,37.46
15355,vulkan: Use larger workgroups for mul_mat_vec when M is small,jeffbolznv,2025-08-15T19:45:10Z,2025-08-17T16:08:57+00:00,1,44.4
15357,Fix broken build: require updated pip to support --break-system-packages,danchev,2025-08-16T05:53:29Z,2025-08-18T10:50:48+00:00,5,52.96
15361,server : export max observed n_past value,okuvshynov,2025-08-16T14:45:58Z,2025-08-17T22:28:59+00:00,1,31.72
15365,ci : fix hang in windows-hip build/release,CISC,2025-08-17T08:43:54Z,2025-08-17T11:30:24+00:00,1,2.77
15367,convert : force patch_embd weights to F16 or F32 to avoid broken GGUFs,CISC,2025-08-17T09:26:20Z,2025-08-17T12:47:42+00:00,3,3.36
15370,vulkan: support sqrt operation,ddwkim,2025-08-17T11:33:35Z,2025-08-17T14:03:09+00:00,1,2.49
15375,opencl: mark `argsort` unsupported if cols exceed workgroup limit,lhez,2025-08-17T14:48:43Z,2025-08-19T18:25:51+00:00,1,51.62
15379,ggml-quants : fix make_qp_quants NANs and IQ1 assertion errors,compilade,2025-08-17T19:52:52Z,2025-08-18T07:23:56+00:00,2,11.52
15380,llama : merge conts and reshapes and remove unnecessary cont,CISC,2025-08-17T20:07:27Z,2025-08-18T17:30:18+00:00,1,21.38
15385,ggml-cpu: add mxfp4 VSX intrinsics for Power9+ (ppc64le) hardware – 3-4x performance boost,mgiessing,2025-08-18T07:16:14Z,2025-08-19T08:54:31+00:00,4,25.64
15386,Add Trigger at PR creation,alitariq4589,2025-08-18T07:50:11Z,2025-08-21T12:52:16+00:00,6,77.03
15391,mtmd : clean up `clip_n_output_tokens`,ngxson,2025-08-18T12:13:12Z,2025-08-18T20:53:52+00:00,1,8.68
15392,vulkan : support conv_2d_dw with f16 weights,Acly,2025-08-18T12:18:45Z,2025-08-21T15:01:51+00:00,1,74.72
15393,vulkan : support ggml_mean,Acly,2025-08-18T12:28:01Z,2025-08-23T06:35:22+00:00,7,114.12
15404,Thinking model disabled assistant prefill,gabe-l-hart,2025-08-18T19:35:04Z,2025-09-05T20:31:24+00:00,12,432.94
15408,chat : clarify the meaning of reasoning_format,ngxson,2025-08-18T21:14:02Z,2025-08-19T08:29:36+00:00,3,11.26
15410,vulkan: Reuse conversion results in prealloc_y,jeffbolznv,2025-08-18T22:29:04Z,2025-08-21T14:55:00+00:00,5,64.43
15412,support interns1-mini,RunningLeon,2025-08-19T02:57:48Z,2025-08-25T06:32:16+00:00,7,147.57
15413,musa: handle __hgt2_mask (available starting from MUSA SDK rc4.3.0),yeahdongcn,2025-08-19T03:03:47Z,2025-08-19T10:33:47+00:00,2,7.5
15416,server : disable context shift by default,ggerganov,2025-08-19T07:11:10Z,2025-08-19T13:46:38+00:00,5,6.59
15419,[CANN] Optimize RMS_NORM using cache,noemotiovon,2025-08-19T09:36:33Z,2025-08-22T06:12:08+00:00,3,68.59
15420,Make Mistral community chat templates optional,juliendenize,2025-08-19T11:40:47Z,2025-08-21T09:19:50+00:00,6,45.65
15421,chat: handle gpt-oss return/end token inconsistency,danbev,2025-08-19T12:14:36Z,2025-08-20T12:26:01+00:00,1,24.19
15427,vulkan: optimize mul_mat_id loading row ids into shared memory,jeffbolznv,2025-08-19T15:51:43Z,2025-08-23T06:31:54+00:00,1,86.67
15431,vulkan: shorten pipeline name strings,jeffbolznv,2025-08-19T20:03:15Z,2025-08-20T14:33:14+00:00,2,18.5
15433,CUDA: replace GGML_CUDA_F16 with CUDA arch checks,JohannesGaessler,2025-08-19T20:51:10Z,2025-08-20T14:58:49+00:00,1,18.13
15434,llama: use FA + max. GPU layers by default,JohannesGaessler,2025-08-19T21:44:28Z,2025-08-30T14:32:10+00:00,13,256.8
15436,CANN: optimize rope cache,hipudding,2025-08-20T01:40:13Z,2025-08-25T02:33:24+00:00,3,120.89
15440,ggml WebGPU: add support for quantization types,reeselevine,2025-08-20T04:02:36Z,2025-08-22T18:28:03+00:00,1,62.42
15444,server: fix OpenAI Streaming API compatibility for usage statistics in chat streams,TeoZosa,2025-08-20T07:58:08Z,2025-08-20T22:10:08+00:00,3,14.2
15446,musa: add GGML_UNUSED_VARS,yeahdongcn,2025-08-20T08:29:11Z,2025-08-21T03:06:06+00:00,1,18.62
15447,lookahead : add sample command to readme,ggerganov,2025-08-20T09:05:46Z,2025-08-20T10:30:46+00:00,2,1.42
15451,CUDA: Accelerate MXFP4 table lookup using `__byte_perm`,Qeeweew,2025-08-20T10:33:32Z,2025-08-25T21:21:22+00:00,5,130.8
15454,CUDA: refactor FA support/selection code,JohannesGaessler,2025-08-20T13:33:35Z,2025-08-20T21:14:14+00:00,1,7.68
15455,examples : add model conversion tool/example,danbev,2025-08-20T14:03:18Z,2025-08-21T10:16:54+00:00,2,20.23
15456,vulkan: add exp operation,ddwkim,2025-08-20T15:38:19Z,2025-08-21T15:00:17+00:00,2,23.37
15457,examples : remove references to `make` in examples [no ci],danbev,2025-08-20T16:40:53Z,2025-08-21T04:12:28+00:00,1,11.53
15458,mtmd : support Kimi VL model,ngxson,2025-08-20T21:14:41Z,2025-08-26T10:54:19+00:00,7,133.66
15460,ggml : fix condition of im2col on Metal backend,ngxson,2025-08-20T23:26:58Z,2025-08-21T05:32:26+00:00,1,6.09
15466,common : fix incorrect print of non-ascii characters in the logging,DamonFool,2025-08-21T06:31:46Z,2025-08-21T08:54:34+00:00,1,2.38
15467,"kv-cache : drop the ""unified"" prefix",ggerganov,2025-08-21T08:29:54Z,2025-08-21T14:00:33+00:00,1,5.51
15472,llama : remove deprecated llama_kv_self API,ggerganov,2025-08-21T12:50:03Z,2025-08-21T16:13:45+00:00,1,3.4
15476,readme : model : mtdm : lfm2 improvements,tdakhran,2025-08-21T14:23:06Z,2025-08-22T07:29:08+00:00,2,17.1
15480,Introduction of CUDA Programmatic Dependent Launch to Llama.cpp,agray3,2025-08-21T15:38:20Z,2025-09-26T12:03:32+00:00,1,860.42
15486,ggml-cpu: Support Q5_0 and Q5_1 on s390x,taronaeo,2025-08-21T18:16:33Z,2025-08-22T08:11:04+00:00,1,13.91
15487,tests: Generate unique input values for count_equal,jeffbolznv,2025-08-21T18:26:41Z,2025-08-25T15:47:16+00:00,1,93.34
15489,vulkan: Rewrite synchronization to allow some overlap between nodes,jeffbolznv,2025-08-21T20:14:14Z,2025-08-23T07:33:36+00:00,1,35.32
15490,Model: Add support for Seed-OSS,pwilkin,2025-08-21T22:10:25Z,2025-08-23T13:21:52+00:00,23,39.19
15494,model : gpt-oss add response_format support,aldehir,2025-08-22T02:05:02Z,2025-08-22T16:04:09+00:00,3,13.99
15501,[CANN]Rope repeat Optimization,noemotiovon,2025-08-22T07:29:02Z,2025-08-25T02:32:22+00:00,1,67.06
15503,test-opt: allow slight inprecision,JohannesGaessler,2025-08-22T08:26:34Z,2025-08-22T21:47:02+00:00,2,13.34
15505,kv-cache : remove LLAMA_SET_ROWS checks,ggerganov,2025-08-22T12:21:01Z,2025-08-28T09:27:03+00:00,2,141.1
15506,vulkan: workaround MoltenVK compile failure in multi_add,jeffbolznv,2025-08-22T15:01:10Z,2025-08-24T08:48:21+00:00,2,41.79
15507,nvidia nemotron nano v2 (nemotronh),gabe-l-hart,2025-08-22T16:21:33Z,2025-08-29T00:39:32+00:00,1,152.3
15510,model : add GroveMoE support,CISC,2025-08-22T17:58:47Z,2025-09-25T17:50:29+00:00,9,815.86
15520,fix(trim): prevent assertion failure in trim function,LaffeyNyaa,2025-08-23T07:04:39Z,2025-08-23T08:38:30+00:00,1,1.56
15524,vulkan: apply MUL_MAT_ID subgroup optimization to non-coopmat devices,0cc4m,2025-08-23T11:55:32Z,2025-08-24T17:36:36+00:00,8,29.68
15525,"CUDA: MoE helper in device code, better tile sizes",JohannesGaessler,2025-08-23T12:46:50Z,2025-08-25T15:23:41+00:00,3,50.61
15526,vulkan: enable Conv2D for Apple after MoltenVK fixed the bug,0cc4m,2025-08-23T13:58:40Z,2025-08-24T08:48:53+00:00,1,18.84
15527,ggml WebGPU: remove userdata from request adapter callback,danbev,2025-08-23T14:07:42Z,2025-09-07T08:19:46+00:00,2,354.2
15531,metal: fix regression when no metal devices are present,booxter,2025-08-23T20:14:39Z,2025-08-25T15:27:35+00:00,5,43.22
15533,Deepseek V3.1 native tool calling support (OpenAI Style),createthis,2025-08-23T21:22:33Z,2025-09-08T14:59:48+00:00,49,377.62
15537,vulkan: Support FA with any multiple of 8 head sizes,jeffbolznv,2025-08-24T03:37:30Z,2025-08-24T09:24:26+00:00,1,5.78
15539,model : add grok-2 support,CISC,2025-08-24T13:49:45Z,2025-09-14T21:01:00+00:00,11,511.19
15543,tests: add test-backend-ops performance test for mul mat id,netrunnereve,2025-08-24T16:31:06Z,2025-08-26T15:42:49+00:00,1,47.2
15544,vulkan: Skip syncing for prealloc_y when it is reused,jeffbolznv,2025-08-24T17:43:55Z,2025-08-30T09:11:22+00:00,1,135.46
15545,vulkan: use memory budget extension to read memory usage,giladgd,2025-08-24T17:58:09Z,2025-09-01T19:17:42+00:00,5,193.33
15546,vulkan: mul_mat_id coopmat2 optimizations,jeffbolznv,2025-08-24T18:12:54Z,2025-08-31T07:06:43+00:00,1,156.9
15552,Model: Seed OSS thinking + tool call support,pwilkin,2025-08-24T23:53:32Z,2025-08-29T12:53:41+00:00,2,109.0
15555,convert : update Ernie 4.5 0.3B model name,ownia,2025-08-25T03:32:00Z,2025-08-25T09:15:06+00:00,3,5.72
15557,model-conversion : add model card template for embeddings [no ci],danbev,2025-08-25T05:18:11Z,2025-08-25T12:25:26+00:00,2,7.12
15558,PowerPC: Sgemm Optimization,shalinib-ibm,2025-08-25T06:53:11Z,2025-08-26T15:35:25+00:00,7,32.7
15560,opencl: fix support ops condition for `rms_norm`,lhez,2025-08-25T07:09:35Z,2025-08-25T21:18:09+00:00,1,14.14
15561,CANN: refactor mask handling and improve performance in FA,noemotiovon,2025-08-25T10:01:34Z,2025-08-27T09:21:41+00:00,3,47.34
15563,Add a warning for special devices,pt13762104,2025-08-25T10:56:17Z,2025-08-26T06:15:33+00:00,3,19.32
15564,model-conversion : set pooling type to none in logits.cpp,danbev,2025-08-25T11:40:54Z,2025-08-25T13:00:43+00:00,1,1.33
15565,vulkan: fix min subgroup 16 condition for mmid subgroup optimization,0cc4m,2025-08-25T13:43:17Z,2025-08-25T15:57:00+00:00,1,2.23
15568,vulkan: Remove splitting for mul_mat_id,jeffbolznv,2025-08-25T16:02:17Z,2025-08-26T04:42:44+00:00,1,12.67
15572,Addresses #15409 - Support for NVIDIA Nemotron-H hybrid architecture models DRAFT,jwjohns,2025-08-25T17:41:11Z,2025-08-28T19:20:30+00:00,6,73.66
15575,support MiniCPM-V 4.5,tc-mb,2025-08-25T20:21:19Z,2025-08-26T08:05:56+00:00,1,11.74
15577,metal : remove contiguous assertion for src0 in IM2COL,CISC,2025-08-25T21:47:40Z,2025-08-26T06:51:44+00:00,1,9.07
15579,fix mtmd ios build,fidoriel,2025-08-25T23:11:15Z,2025-08-26T18:05:50+00:00,1,18.91
15587,CUDA: return -1 for nonexistent compiled arch,JohannesGaessler,2025-08-26T10:41:51Z,2025-08-26T14:01:20+00:00,1,3.32
15591,common : add -m to bash completion for --model [no ci],danbev,2025-08-26T14:25:04Z,2025-08-27T08:28:53+00:00,1,18.06
15592,SYCL: fix rms_norm_mul_add for tensor dim not a multiple of sg_size,qnixsynapse,2025-08-26T14:36:41Z,2025-08-26T18:57:49+00:00,1,4.35
15599,tests : fix test-opt with GGML_BACKEND_DL,slaren,2025-08-26T18:41:52Z,2025-08-26T20:14:38+00:00,1,1.55
15604,"Change to info instead of debug, to explain reason for stopping.",jrincayc,2025-08-27T00:39:44Z,2025-08-28T07:48:20+00:00,2,31.14
15611,musa: fix build warnings,yeahdongcn,2025-08-27T07:40:20Z,2025-09-26T00:56:10+00:00,1,713.26
15614,kleidiai: fix GGML_ASSERT(*cur_backend_id != -1) failed,chaxu01,2025-08-27T10:18:06Z,2025-09-11T10:45:41+00:00,9,360.46
15615,HIP: Enable support for ggml_backend_cuda_register_host_buffer,IMbackK,2025-08-27T10:34:46Z,2025-08-27T11:58:54+00:00,1,1.4
15616,presets : add qwen3-30B-a3b FIM,ggerganov,2025-08-27T11:19:30Z,2025-08-27T12:48:07+00:00,1,1.48
15622,cuda: Add cublasLt_static linking when GGML_STATIC is enabled,matiaslin,2025-08-27T18:30:13Z,2025-08-28T00:32:36+00:00,2,6.04
15625,ggml : fix SSM_SCAN for n_groups > 1,compilade,2025-08-27T21:56:55Z,2025-08-28T14:11:36+00:00,2,16.24
15629,CANN: fix RoPE cache issue on multi-device,hipudding,2025-08-28T07:42:09Z,2025-09-01T00:57:00+00:00,6,89.25
15630,server : enable /slots by default and make it secure,ggerganov,2025-08-28T08:26:13Z,2025-08-31T17:11:58+00:00,2,80.76
15631,"CUDA: fuse adds, fuse add with rms norm",am17an,2025-08-28T09:38:18Z,2025-08-29T03:35:58+00:00,15,17.96
15632,Refactor server.cpp: Split monolithic file into modular components,copilot-swe-agent,2025-08-28T09:47:25Z,2025-09-03T20:56:31+00:00,1,155.15
15633,scripts: add sqlite3 check for compare-commits.sh,am17an,2025-08-28T09:55:21Z,2025-08-28T11:23:22+00:00,2,1.47
15634,ggml-cpu: fix invalid hsum build in debug s390x,taronaeo,2025-08-28T10:06:32Z,2025-08-28T14:39:27+00:00,1,4.55
15635,CUDA: add conv2d,mnehete32,2025-08-28T10:56:46Z,2025-08-28T18:33:04+00:00,5,7.61
15637,fix: Compute the full sum in llama-eval-callback,gabe-l-hart,2025-08-28T13:36:51Z,2025-08-28T20:27:36+00:00,1,6.85
15639,Hermes 2 tool calling : fixed crash when <tool_call> had a newline before it,ExtReMLapin,2025-08-28T14:19:41Z,2025-09-04T23:24:08+00:00,1,177.07
15647,tools: [SERVER] Added documentation for `parallel_tool_calls` param,ExtReMLapin,2025-08-28T20:53:23Z,2025-08-29T17:25:40+00:00,1,20.54
15649,vulkan: Allow fallback to sysmem memory when vidmem is full,jeffbolznv,2025-08-28T23:18:49Z,2025-08-31T06:30:54+00:00,3,55.2
15652,vulkan: clamp matmul and FA results to the max finite value,jeffbolznv,2025-08-29T03:44:30Z,2025-08-31T06:27:58+00:00,6,50.72
15658,CANN: Optimize MUL_MAT_ID,hipudding,2025-08-29T08:30:03Z,2025-09-01T00:57:23+00:00,1,64.46
15660,CUDA: fix bug in rms_norm fusion,am17an,2025-08-29T09:50:04Z,2025-08-29T13:30:06+00:00,1,3.67
15661,[CANN] Optimize compiler warning issues,noemotiovon,2025-08-29T09:56:02Z,2025-08-30T02:18:35+00:00,1,16.38
15662,model : avoid ggml_cont_3d for fused QKV weights,ggerganov,2025-08-29T10:34:14Z,2025-09-08T07:25:33+00:00,1,236.86
15663,ggml: update kleidiai to v1.13.0,chaxu01,2025-08-29T11:53:42Z,2025-08-30T16:03:42+00:00,1,28.17
15665,sampling : optimize samplers by reusing bucket sort,ggerganov,2025-08-29T14:13:35Z,2025-08-31T17:41:02+00:00,4,51.46
15666,vulkan : update ggml_vk_instance_validation_ext_available,danbev,2025-08-29T14:37:31Z,2025-09-03T18:24:50+00:00,3,123.79
15668,"scripts: strip ""AMD Instinct"" from GPU name",JohannesGaessler,2025-08-29T18:00:48Z,2025-08-29T20:04:08+00:00,1,2.06
15669,ggml: add ops for WAN video model (cuda && cpu),leejet,2025-08-29T19:19:50Z,2025-09-04T08:38:49+00:00,17,133.32
15670,removed obsolete doc,l29ah,2025-08-29T20:28:36Z,2025-08-29T22:12:53+00:00,1,1.74
15676,feat: nemotron thinking & toolcalling support,pwilkin,2025-08-29T23:23:59Z,2025-09-04T23:22:23+00:00,13,143.97
15679,vulkan : remove unused portability_enumeration_ext variable,danbev,2025-08-30T04:42:45Z,2025-08-31T06:46:42+00:00,1,26.07
15682,chat: Fix streaming parser for granite models,shun095,2025-08-30T07:44:50Z,2025-09-19T15:57:30+00:00,3,488.21
15683,CUDA: use FP32 arithmetic for conv2d,JohannesGaessler,2025-08-30T09:29:15Z,2025-08-30T14:20:32+00:00,1,4.85
15684,docs : update build.md to remove MSVC arm64 notes,slaren,2025-08-30T14:23:05Z,2025-08-30T15:51:29+00:00,1,1.47
15686,vulkan: handle large sizes for get_rows,jeffbolznv,2025-08-30T19:15:13Z,2025-08-31T08:13:27+00:00,1,12.97
15687,tests: large sizes for get_rows,jeffbolznv,2025-08-30T19:24:52Z,2025-09-08T04:23:41+00:00,1,200.98
15690,CUDA: fix build error from ambiguous __half conversions in conv2d,qnixsynapse,2025-08-31T04:14:41Z,2025-09-01T01:25:06+00:00,2,21.17
15691,gguf: gguf_writer refactor,Green-Sky,2025-08-31T12:43:26Z,2025-09-05T09:34:28+00:00,5,116.85
15695,ggml : WebGPU add TRANSPOSE and RESHAPE to supported ops,danbev,2025-08-31T13:17:05Z,2025-09-01T12:28:49+00:00,5,23.2
15702,vulkan: add missing clamps in new mul_mat_id paths,jeffbolznv,2025-08-31T17:37:49Z,2025-09-01T19:01:10+00:00,1,25.39
15705,vulkan: initialize vulkan-hpp to allow using extension function pointers,jeffbolznv,2025-08-31T22:03:33Z,2025-09-13T15:23:30+00:00,13,305.33
15706,OpenCL: add attention sinks support for FA kernels,rmatif,2025-08-31T22:04:26Z,2025-09-02T06:26:53+00:00,1,32.37
15708,convert : remove redundant code,DamonFool,2025-09-01T02:59:17Z,2025-09-01T15:53:31+00:00,6,12.9
15710,CANN: Support ext_factor in rope,hipudding,2025-09-01T03:47:27Z,2025-09-02T06:05:24+00:00,1,26.3
15712,CANN: Support eager execution mode under ACL graph compilation,noemotiovon,2025-09-01T08:57:13Z,2025-09-02T06:07:48+00:00,3,21.18
15715,"CUDA: Optimize `rms_norm_f32` kernel and its fused variants, giving 1-6% perf E2E",ORippler,2025-09-01T14:16:10Z,2025-09-03T17:59:16+00:00,5,51.72
15716,vulkan: Fix macro parameter order for f32 matmul shaders,jeffbolznv,2025-09-01T15:28:57Z,2025-09-02T06:37:02+00:00,1,15.13
15717,vulkan: disable large mmv subgroups on older Nvidia GPUs,0cc4m,2025-09-01T15:31:04Z,2025-09-01T18:58:36+00:00,1,3.46
15720,ggml-cpu : optimize RVV kernels,xctan,2025-09-01T16:52:26Z,2025-09-03T08:16:22+00:00,1,39.4
15722,ggml-backend: raise GGML_MAX_SPLIT_INPUTS,JohannesGaessler,2025-09-01T21:34:06Z,2025-09-01T23:14:55+00:00,1,1.68
15724,"vulkan: don't use std::string in load_shaders, to improve compile time",jeffbolznv,2025-09-01T22:39:05Z,2025-09-03T18:33:16+00:00,2,43.9
15729,vulkan: Use larger loads in scalar/coopmat1 matmul,jeffbolznv,2025-09-02T03:19:37Z,2025-09-07T16:53:07+00:00,1,133.56
15730,CANN: Resolve soft_max precision issue,hipudding,2025-09-02T06:32:04Z,2025-09-02T09:12:38+00:00,1,2.68
15732,opencl: initial `q8_0` mv support,lhez,2025-09-02T06:42:42Z,2025-09-21T21:48:44+00:00,2,471.1
15733,CANN: Mask unsupported TRANSPOSE_1D operator,hipudding,2025-09-02T07:58:33Z,2025-09-03T06:08:22+00:00,1,22.16
15735,CANN: Add RoPE contiguous check for 310I DUO device,hipudding,2025-09-02T09:36:37Z,2025-09-03T08:46:01+00:00,1,23.16
15736,CANN: Change the previously written float_t type to float,noemotiovon,2025-09-02T09:46:14Z,2025-09-03T02:43:53+00:00,1,16.96
15739,ggml-cpu: fixes instability in NNPA Vector Intrinsics,taronaeo,2025-09-02T11:44:44Z,2025-09-05T18:50:56+00:00,1,79.1
15740,vulkan: fix shaders gen when no integer dot is available,0cc4m,2025-09-02T11:45:33Z,2025-09-02T14:02:27+00:00,1,2.28
15744,Update `.clang-format` to use `BinPackArguments=true`,ORippler,2025-09-02T14:01:28Z,2025-09-02T17:40:37+00:00,1,3.65
15745,tests : add --list-ops and --show-coverage options,danbev,2025-09-02T14:18:09Z,2025-09-05T12:49:21+00:00,2,70.52
15746,llama: -fa 1/0/-1 aliases for -fa on/off/auto,JohannesGaessler,2025-09-02T14:41:21Z,2025-09-02T16:17:27+00:00,3,1.6
15754,fix: resolve unsigned integer initialization warnings in gguf.cpp,skrandy,2025-09-02T17:50:07Z,2025-09-02T19:27:30+00:00,1,1.62
15756,feat: add Jinja tester PySide6 simple app,pwilkin,2025-09-02T20:14:47Z,2025-09-04T23:05:12+00:00,8,50.84
15758,OpenCL: add hs=40 support to FA,rmatif,2025-09-02T22:04:49Z,2025-09-04T06:30:28+00:00,1,32.43
15760,CANN: fix acl_rstd allocation size in ggml_cann_rms_norm,noemotiovon,2025-09-03T03:43:24Z,2025-09-04T03:03:02+00:00,4,23.33
15761,model-conversion : add missing curl script [no ci],danbev,2025-09-03T04:47:20Z,2025-09-03T07:48:36+00:00,1,3.02
15762,Add the hardsigmoid and hardswish operators to Vulkan backend,relent95,2025-09-03T08:04:21Z,2025-09-03T18:22:55+00:00,2,10.31
15763,CANN: Refactor ND to NZ workspace to be per-device in Ascend backend,noemotiovon,2025-09-03T09:35:32Z,2025-09-04T12:20:14+00:00,6,26.75
15764,llama : fix incorrect model type for  Gemma 270M,danbev,2025-09-03T09:42:35Z,2025-09-03T11:35:50+00:00,1,1.89
15767,CUDA: Add mul_mat_id support for the mmf kernel,am17an,2025-09-03T13:11:30Z,2025-09-09T06:38:03+00:00,7,137.44
15768,Fix compiler warning in Vulkan code,ericcurtin,2025-09-03T13:32:25Z,2025-09-04T07:31:07+00:00,5,17.98
15769,"CUDA: faster tile FA (Pascal/AMD), headsize 256",JohannesGaessler,2025-09-03T14:03:19Z,2025-09-06T22:26:28+00:00,1,80.39
15770,model-conversion : fix pyright errors,danbev,2025-09-03T15:01:58Z,2025-09-03T16:28:36+00:00,1,1.44
15771,Document the new max GPU layers default in help,ericcurtin,2025-09-03T15:12:27Z,2025-09-04T09:49:44+00:00,5,18.62
15775,vulkan: fix mmv subgroup16 selection,0cc4m,2025-09-03T18:12:23Z,2025-09-03T20:55:11+00:00,2,2.71
15777,Rebase to latest master and add documentation for new 3D convolution functions,copilot-swe-agent,2025-09-03T20:51:48Z,2025-09-04T11:25:11+00:00,5,14.56
15780,server: add exceed_context_size_error type,ngxson,2025-09-03T23:27:31Z,2025-09-04T09:50:23+00:00,2,10.38
15783,ggml: allow casting between f32 and i32,ngxson,2025-09-04T02:33:16Z,2025-09-08T10:33:01+00:00,6,104.0
15784,CANN: Fix precision issue on 310I DUO multi-devices,hipudding,2025-09-04T03:25:51Z,2025-09-04T07:12:31+00:00,1,3.78
15790,Add docker protocol support for llama-server model loading,ericcurtin,2025-09-04T11:38:41Z,2025-09-12T15:31:50+00:00,45,195.89
15791,llama : set n_outputs to 1 to avoid 0 outputs mean-pooling,danbev,2025-09-04T12:12:45Z,2025-09-04T13:40:44+00:00,1,1.47
15792,Implement --log-colors with always/never/auto,ericcurtin,2025-09-04T12:32:02Z,2025-09-05T18:44:00+00:00,9,30.2
15794,vulkan: Support pad_ext,jeffbolznv,2025-09-04T13:03:54Z,2025-09-07T17:00:49+00:00,1,75.95
15795,vulkan: support im2col_3d,jeffbolznv,2025-09-04T13:43:00Z,2025-09-07T18:50:26+00:00,1,77.12
15797,ggml-backend : add GGML_BACKEND_DEVICE_TYPE_IGPU device type,slaren,2025-09-04T15:14:14Z,2025-09-11T20:47:38+00:00,1,173.56
15801,model-conversion : add --embeddings flag to modelcard.template [no ci],danbev,2025-09-04T16:32:02Z,2025-09-05T02:36:23+00:00,1,10.07
15802,"CUDA: fastdiv, launch bounds for mmvq + q8_1 quant",JohannesGaessler,2025-09-04T17:58:01Z,2025-09-05T14:07:03+00:00,5,20.15
15805,Add conv2d Implicit GEMM,bssrdf,2025-09-04T20:16:52Z,2025-09-18T17:38:47+00:00,9,333.37
15809,CANN: Switch to stream synchronization,noemotiovon,2025-09-05T02:54:25Z,2025-09-08T02:03:29+00:00,1,71.15
15811,kv-cache : fix SWA checks + disable cacheless iSWA,ggerganov,2025-09-05T05:21:01Z,2025-09-05T07:39:23+00:00,2,2.31
15813,CUDA: Conv2d Tensor Core,mnehete32,2025-09-05T06:36:50Z,2025-09-18T17:45:52+00:00,1,323.15
15814,CANN: implement LRU cache for ACL graphs in CANN backend,noemotiovon,2025-09-05T09:19:18Z,2025-09-10T07:29:13+00:00,4,118.17
15815,ggml : split graph allocations according to backend max buffer size,Acly,2025-09-05T09:42:31Z,2025-09-24T14:17:50+00:00,10,460.59
15817,kleidiai: generalize compute_forward_kv_cache to compute_forward_fp16,chaxu01,2025-09-05T12:57:16Z,2025-09-06T14:08:44+00:00,1,25.19
15821,ggml-cpu: drop support for nnpa intrinsics,taronaeo,2025-09-05T18:50:43Z,2025-09-06T03:27:28+00:00,3,8.61
15824,Add support for Qwen3-Reranker,iamlemec,2025-09-05T21:21:40Z,2025-09-25T08:53:09+00:00,2,467.52
15825,ci : exempt correct research label,CISC,2025-09-05T21:34:25Z,2025-09-05T23:21:16+00:00,1,1.78
15827,server : implement prompt processing progress report in stream mode,ngxson,2025-09-06T04:27:06Z,2025-09-06T11:35:04+00:00,2,7.13
15828,requirements : update transformers/torch for Embedding Gemma,danbev,2025-09-06T05:04:30Z,2025-09-09T04:06:52+00:00,2,71.04
15830,json : support `enum` values within `allOf`,aldehir,2025-09-06T07:27:13Z,2025-09-08T21:14:32+00:00,1,61.79
15832,metal : make the backend async,ggerganov,2025-09-06T09:28:47Z,2025-09-10T13:34:38+00:00,18,100.1
15836,server : speed up tests,ngxson,2025-09-06T11:29:57Z,2025-09-06T12:45:25+00:00,1,1.26
15839,"ggml-zdnn: fix #15414, activate FP16 and BF16 acceleration and incorrect zTensor free",taronaeo,2025-09-06T14:15:52Z,2025-09-12T18:39:52+00:00,3,148.4
15850,vulkan: sort graph to allow more parallel execution,jeffbolznv,2025-09-07T04:20:04Z,2025-09-08T18:10:07+00:00,13,37.83
15855,ggml-cpu: clean up s390x SIMD,taronaeo,2025-09-07T10:40:14Z,2025-09-07T18:18:28+00:00,1,7.64
15860,llama: print memory breakdown on exit,JohannesGaessler,2025-09-07T21:35:23Z,2025-09-24T14:53:48+00:00,4,401.31
15861,vulkan: Fix OOB accesses in soft_max_back,jeffbolznv,2025-09-07T23:32:19Z,2025-09-09T12:41:15+00:00,1,37.15
15862,vulkan: fix failing dequant shaders,jeffbolznv,2025-09-08T02:36:28Z,2025-09-13T15:29:43+00:00,3,132.89
15864,CPU: Fix loongarch lsx compilation error,junchao-loongson,2025-09-08T03:01:40Z,2025-09-25T09:22:55+00:00,1,414.35
15868,cuda : fix supports_op condition for get_rows when number of blocks is too large,ggerganov,2025-09-08T07:15:06Z,2025-09-08T10:56:52+00:00,1,3.7
15869,cuda : non-contiguous src0 not supported for PAD,CISC,2025-09-08T07:48:31Z,2025-09-08T09:55:44+00:00,1,2.12
15871,model-conversion : add embedding prompt file support,danbev,2025-09-08T08:14:21Z,2025-09-25T10:02:36+00:00,1,409.8
15872,"CUDA: Add `fastdiv` to `k_bin_bcast*`, giving 1-3% E2E performance",ORippler,2025-09-08T08:43:15Z,2025-09-10T20:04:03+00:00,4,59.35
15877,feat: Extra debugging support for model conversion,pwilkin,2025-09-08T11:22:14Z,2025-09-09T04:05:55+00:00,4,16.73
15878,media : add llama1 small icon,06kellyjac,2025-09-08T12:03:08Z,2025-09-08T18:57:01+00:00,1,6.9
15880,CUDA: generate_cu_files.py - add missing mxfp4,am17an,2025-09-08T15:30:27Z,2025-09-08T17:23:47+00:00,1,1.89
15881,contrib : add notes about merging PRs,ggerganov,2025-09-08T18:37:09Z,2025-09-09T05:42:10+00:00,2,11.08
15882,CUDA: fix GET_ROWS for large tensors,JohannesGaessler,2025-09-08T20:58:46Z,2025-09-09T06:11:01+00:00,1,9.2
15884,HIP: use v_dot2_f32_f16 instruction for FA,JohannesGaessler,2025-09-08T23:07:25Z,2025-09-09T12:04:44+00:00,1,12.96
15886,Workaround for subgroup arithmetic failing on MoltenVK with AMD GPUs,lksj92hs,2025-09-09T01:41:00Z,2025-09-09T12:01:16+00:00,1,10.34
15887,ci : cache ROCm installation in windows-latest-cmake-hip,danbev,2025-09-09T06:10:06Z,2025-09-10T03:23:19+00:00,1,21.22
15891,media : add transparent icon svg and png [no ci],06kellyjac,2025-09-09T09:08:47Z,2025-09-10T11:51:29+00:00,1,26.71
15893,llama : check returned fn ptrs from ggml_backend_reg_get_proc_address,danbev,2025-09-09T10:34:55Z,2025-09-10T03:33:58+00:00,1,16.98
15900,tests : filter out no-ops from coverage report,danbev,2025-09-09T14:21:47Z,2025-09-10T12:17:10+00:00,3,21.92
15903,Do not install tools on iOS targets,ykhrustalev,2025-09-09T15:42:32Z,2025-09-16T02:54:44+00:00,1,155.2
15905,"Fix Vulkan OOM error always showing up as ""no suitable memory type found""",0cc4m,2025-09-09T16:30:58Z,2025-09-09T20:26:03+00:00,1,3.92
15906,metal : make the backend async v2,ggerganov,2025-09-09T17:44:19Z,2025-09-10T14:52:36+00:00,2,21.14
15908,graph : support non-contiguous Q in build_attn_mha,CISC,2025-09-09T20:11:36Z,2025-09-10T17:08:59+00:00,3,20.96
15909,Extend the support of T5 models with different encoder-decoder layers,DamonFool,2025-09-10T03:03:20Z,2025-09-10T18:51:51+00:00,9,15.81
15910,"Revert ""sycl: add usage of enqueue_functions extension (#14244)"" ",NeoZhangJianyu,2025-09-10T03:25:40Z,2025-09-12T01:15:13+00:00,1,45.83
15912,CANN: Add ROPE sin/cos cache for reuse,noemotiovon,2025-09-10T07:45:02Z,2025-09-10T10:42:01+00:00,2,2.95
15915,devops: add s390x containers,taronaeo,2025-09-10T11:09:58Z,2025-09-23T05:59:35+00:00,4,306.83
15916,llama : bump max seq limit from 64 to 256,ggerganov,2025-09-10T11:17:24Z,2025-09-18T09:47:56+00:00,2,190.51
15917,ggml-cpu : fix padding in ggml_timestep_embedding,danbev,2025-09-10T11:18:45Z,2025-09-10T15:31:40+00:00,1,4.22
15922,ggml-cpu : add check for ARM MATMUL_INT8/i8mm support,danbev,2025-09-10T12:10:59Z,2025-09-11T13:39:12+00:00,1,25.47
15925,devops: add s390x & ppc64le CI,taronaeo,2025-09-10T13:05:13Z,2025-09-26T18:03:33+00:00,14,388.97
15926,CUDA: some micro-optimizations in mmf.cuh for mul_mat_id,am17an,2025-09-10T13:33:13Z,2025-09-15T09:35:11+00:00,2,116.03
15927,"CUDA: larger SRAM reads for tile FA, AMD FP16 dot",JohannesGaessler,2025-09-10T15:07:12Z,2025-09-11T19:19:58+00:00,2,28.21
15928,ggml : fix uninitialized is_on_grid in quantize_row_iq3_xxs_impl,CISC,2025-09-10T18:31:00Z,2025-09-23T08:25:20+00:00,15,301.91
15932,ggml : fix padding in timestep embedding kernels,danbev,2025-09-11T04:25:28Z,2025-09-16T13:25:57+00:00,2,129.01
15933,CANN: Disable acl_graph for prefill stage,hipudding,2025-09-11T06:26:56Z,2025-09-11T07:59:37+00:00,1,1.54
15934,nitpick : correct MB to MiB,ddh0,2025-09-11T08:24:41Z,2025-09-11T17:12:34+00:00,1,8.8
15935,CANN: Fix ggml_cann_set_device to avoid redundant device switches,noemotiovon,2025-09-11T08:37:01Z,2025-09-17T06:33:08+00:00,1,141.94
15938,ci : update macos-latest* jobs to use macos-latest,danbev,2025-09-11T12:53:56Z,2025-09-16T03:57:16+00:00,1,111.06
15939,vulkan: Make device memory check more portable,mbaudier,2025-09-11T17:53:05Z,2025-09-12T07:06:20+00:00,2,13.22
15943,docker : add configurable port support for health checking,haiyuewa,2025-09-12T05:58:48Z,2025-09-13T16:04:01+00:00,1,34.09
15944,opencl: fix concat crash on win arm64 with Adreno,lhez,2025-09-12T06:09:04Z,2025-09-21T23:42:10+00:00,1,233.55
15947,Vulkan iGPU device selection overhaul and PCI ID API support,0cc4m,2025-09-12T07:41:13Z,2025-09-12T11:24:21+00:00,1,3.72
15948,context : remove redundant explicit casting to the same type,haiyuewa,2025-09-12T08:47:49Z,2025-09-12T15:16:33+00:00,1,6.48
15951,llama : allow using iGPUs with --device,slaren,2025-09-12T15:25:46Z,2025-09-13T14:49:49+00:00,1,23.4
15952,llama-bench: add --n-cpu-moe support,jacekpoplawski,2025-09-12T16:13:45Z,2025-09-16T14:17:08+00:00,13,94.06
15956,CUDA: fix im2col_3d to respect non-contiguous inputs (views),jakekarnes42,2025-09-13T05:07:39Z,2025-09-15T22:28:31+00:00,1,65.35
15957,CUDA: Optimize PAD_REFLECT_1D,bugparty,2025-09-13T05:48:42Z,2025-09-18T18:26:04+00:00,8,132.62
15963,Add resumable downloads for llama-server model loading,ericcurtin,2025-09-13T10:28:31Z,2025-09-18T15:22:50+00:00,6,124.91
15965,ggml-zdnn: rm user mapped buffers,taronaeo,2025-09-13T14:18:00Z,2025-09-14T05:37:04+00:00,1,15.32
15966,metal : remove memory pools,ggerganov,2025-09-13T14:30:26Z,2025-09-14T19:02:32+00:00,1,28.54
15967,server : only attempt to enable thinking if using jinja,CISC,2025-09-13T16:04:34Z,2025-09-14T19:17:04+00:00,1,27.21
15972,"releases : update ROCM, add gfx1200, gfx1201, gfx1151",slaren,2025-09-13T22:01:48Z,2025-09-14T09:21:59+00:00,2,11.34
15976,vulkan: automatically remove unsupported devices,netrunnereve,2025-09-14T03:36:06Z,2025-09-17T07:35:37+00:00,3,75.99
15980,doc : update documentation for --tensor-split,rgerganov,2025-09-14T07:46:26Z,2025-09-14T09:10:07+00:00,2,1.39
15982,"CUDA: fix FA occupancy, optimize tile kernel",JohannesGaessler,2025-09-14T08:57:13Z,2025-09-17T13:32:42+00:00,1,76.59
15984,build: fix the build failures of Windows HIP release job,lcy0321,2025-09-14T10:30:09Z,2025-09-14T14:20:36+00:00,3,3.84
15987,Vulkan: Clean up mul_mm shader,0cc4m,2025-09-14T12:04:39Z,2025-09-14T14:56:29+00:00,1,2.86
15988,llama-run: Fix model download on Windows,npopov-vst,2025-09-14T12:24:21Z,2025-09-15T10:08:30+00:00,8,21.74
15991,SYCL: Add COUNT_EQUAL operator support,yael-works,2025-09-14T14:01:41Z,2025-09-15T16:51:35+00:00,11,26.83
15992,"releases : switch to rocWMMA develop branch, add gfx1151",slaren,2025-09-14T14:32:54Z,2025-09-15T21:38:42+00:00,2,31.1
15997,"docker : enable rocWMMA in ROCm images, add gfx1151",slaren,2025-09-14T20:26:45Z,2025-09-15T21:38:52+00:00,5,25.2
15999,fix KLD percentile output,ddh0,2025-09-15T04:40:17Z,2025-09-15T07:54:57+00:00,1,3.24
16002,examples : support encoder-decoder models in the simple example,DamonFool,2025-09-15T08:32:48Z,2025-09-17T07:29:00+00:00,1,46.94
16003,Add LLaDA-7b-MoE diffusion model,am17an,2025-09-15T08:50:24Z,2025-09-16T02:38:28+00:00,1,17.8
16008,ci : create git tags for released docker images,rgerganov,2025-09-15T12:49:04Z,2025-09-26T10:19:23+00:00,7,261.51
16010,ci : upload xcframework artifact from ios-xcode-build job,danbev,2025-09-15T13:38:21Z,2025-09-16T11:41:38+00:00,1,22.05
16015,Add Olmo3 implementation,2015aroras,2025-09-15T17:52:37Z,2025-09-17T07:01:59+00:00,4,37.16
16017,llama : add Cuda macro rules for .clang-format file,bugparty,2025-09-15T21:27:08Z,2025-09-16T06:59:19+00:00,1,9.54
16018,"GGML WebGPU: Support for ADD, MUL, RMS_NORM, GET_ROWS operators",reeselevine,2025-09-15T21:34:52Z,2025-09-17T20:09:40+00:00,1,46.58
16022,Vulkan: add conv_transpose_2d operation,relent95,2025-09-16T07:29:10Z,2025-09-22T08:04:02+00:00,13,144.58
16023,llama-quant : fix the verification of attention layers for encoder-decoder models,DamonFool,2025-09-16T10:54:37Z,2025-09-17T07:30:55+00:00,4,20.61
16029,ci : use macos-latest for arm64 webgpu build,danbev,2025-09-16T11:59:06Z,2025-09-16T13:27:53+00:00,4,1.48
16031,cmake : fix static linking for OpenMP on Unix-like systems,angt,2025-09-16T12:48:57Z,2025-09-18T21:07:18+00:00,1,56.31
16037,OpenCL: MoE MXFP4 kernel optimizations,shawngu-quic,2025-09-16T21:31:53Z,2025-09-18T19:03:34+00:00,1,45.53
16038,Fix corrupted memory error on json grammar initialization,dralves,2025-09-16T23:51:32Z,2025-09-17T08:08:02+00:00,1,8.28
16039,llama-bench: add --devices and --list-devices support,ssweens,2025-09-17T03:15:32Z,2025-09-19T22:15:21+00:00,4,67.0
16040,ci : revert back to macos-13 for macOS-latest-cmake-x64,danbev,2025-09-17T03:47:22Z,2025-09-17T07:34:09+00:00,1,3.78
16042,convert : add Llama4ForCausalLM,ngxson,2025-09-17T04:59:13Z,2025-09-17T17:18:21+00:00,1,12.32
16044,CANN: remove print,noemotiovon,2025-09-17T07:13:09Z,2025-09-18T01:26:33+00:00,1,18.22
16049,ggml-amx : fix ggml_amx_init() on generic Linux,angt,2025-09-17T11:38:52Z,2025-09-18T21:07:26+00:00,1,33.48
16052,server : include usage statistics only when user request them,rgerganov,2025-09-17T12:24:12Z,2025-09-18T10:36:57+00:00,2,22.21
16056,vulkan: use vec dot for matrix matrix multiplications,0cc4m,2025-09-17T16:34:17Z,2025-09-20T08:42:56+00:00,3,64.14
16059,vulkan: optimize UMA buffer operations and fix driver hangs,giuseppe,2025-09-17T21:54:13Z,2025-09-21T06:31:55+00:00,4,80.63
16060,cuda : add missing F32<->I32 entries in ggml_cuda_cpy_fn,CISC,2025-09-17T23:16:10Z,2025-09-18T11:28:22+00:00,1,12.2
16062,ggml : refactor forward_dup for cpu backend,ngxson,2025-09-18T02:09:36Z,2025-09-19T04:31:57+00:00,1,26.37
16065,metal : handle nil cv during pipeline creation,ggerganov,2025-09-18T05:33:39Z,2025-09-18T07:03:24+00:00,1,1.5
16076,Always show message actions for mobile UI + improvements for user message sizing,allozaur,2025-09-18T12:12:31Z,2025-09-26T13:59:07+00:00,6,193.78
16079,webui: add configurable base path support for subdirectory deployment,ServeurpersoCom,2025-09-18T12:32:37Z,2025-09-22T13:30:57+00:00,2,96.97
16082,rename optimize_graph to graph_optimize,jeffbolznv,2025-09-18T12:51:58Z,2025-09-18T18:46:18+00:00,4,5.91
16084,feat: Improve mobile UI for Settings Dialog,allozaur,2025-09-18T13:41:13Z,2025-09-19T07:52:28+00:00,1,18.19
16086,vulkan: fix validation error about VK_PIPELINE_CREATE_CAPTURE_STATISTICS_BIT_KHR,jeffbolznv,2025-09-18T14:06:12Z,2025-09-21T06:23:37+00:00,1,64.29
16091,CUDA: fix compilation on CC 6.0,JohannesGaessler,2025-09-18T16:10:57Z,2025-09-18T17:28:33+00:00,1,1.29
16101,chat : fix build on arm64,ngxson,2025-09-19T04:59:55Z,2025-09-19T06:02:51+00:00,1,1.05
16102,metal : fuse non-sequential nodes,ggerganov,2025-09-19T09:36:59Z,2025-09-28T06:34:05+00:00,3,212.95
16107,Fix incomplete chunck handling in the webui,Bramas,2025-09-19T13:16:36Z,2025-09-22T08:53:13+00:00,1,67.61
16109,server: fix SSE and OpenAI compatibility for error messages when streaming,BenjaminBruenau,2025-09-19T14:19:07Z,2025-09-20T05:56:31+00:00,4,15.62
16112,Model: GraniteDocling,gabe-l-hart,2025-09-19T14:39:04Z,2025-09-23T18:20:09+00:00,1,99.68
16113,contrib : update roles,ggerganov,2025-09-19T15:32:17Z,2025-09-22T07:58:02+00:00,2,64.43
16115,Implement progress bar and multi-connection downloads,ericcurtin,2025-09-19T16:41:16Z,2025-09-23T09:01:51+00:00,13,88.34
16116,ci : migrate ggml ci to self-hosted runners,ggerganov,2025-09-19T16:46:47Z,2025-09-21T13:50:45+00:00,1,45.07
16122,ggml : add ggml_op_is_empty,ggerganov,2025-09-20T09:11:09Z,2025-09-22T08:12:09+00:00,2,47.02
16123,ggml : extend ggml_can_fuse to work with non-sequential nodes,ggerganov,2025-09-20T09:21:59Z,2025-09-22T08:12:38+00:00,5,46.84
16124,"codeowners : claim responsibility for ci, models, gguf-py and convert",CISC,2025-09-20T09:30:11Z,2025-09-22T07:59:06+00:00,1,46.48
16126,mtmd: more optimized build_rope_2d,ngxson,2025-09-20T10:36:09Z,2025-09-29T04:13:44+00:00,1,209.63
16127,clang-tidy : disable warning about performance enum size,haiyuewa,2025-09-20T13:31:02Z,2025-09-22T17:57:46+00:00,1,52.45
16128,codeowners : update ownership for @ngxson and @allozuar,ngxson,2025-09-20T14:00:48Z,2025-09-22T08:10:58+00:00,2,42.17
16129,rpc : use GGML_LOG_* for logging,rgerganov,2025-09-20T14:05:58Z,2025-09-25T07:20:02+00:00,7,113.23
16130,CUDA: add a fused top-K MoE kernel,am17an,2025-09-20T14:19:10Z,2025-09-25T14:35:05+00:00,11,120.27
16135,vulkan: 64-bit im2col,jeffbolznv,2025-09-20T19:30:09Z,2025-09-28T06:38:37+00:00,2,179.14
16137,Enable `--offline` mode without curl support,angt,2025-09-20T20:09:39Z,2025-09-22T12:13:52+00:00,1,40.07
16140,common : remove unused local variables,haiyuewa,2025-09-21T04:43:48Z,2025-09-22T08:48:42+00:00,1,28.08
16151,vulkan: vec dot matrix multiplication fix,0cc4m,2025-09-21T17:17:53Z,2025-09-22T05:22:43+00:00,1,12.08
16156,vulkan: throw system error instead of SIGABRT during init on older devices,DmyMi,2025-09-21T18:41:24Z,2025-09-27T16:26:46+00:00,1,141.76
16157,webui: switch to hash-based routing (alternative of #16079),isaac-mcfadyen,2025-09-21T19:16:30Z,2025-09-26T15:36:48+00:00,2,116.34
16159,ggml : implement set_rows with i32 index,CISC,2025-09-21T21:25:26Z,2025-09-22T17:13:00+00:00,8,19.79
16160,vulkan: support arbitrary KV dimension in flash attention,jeffbolznv,2025-09-21T22:46:23Z,2025-09-27T20:43:39+00:00,1,141.95
16162,vulkan: support set_rows with i32 index type,jeffbolznv,2025-09-22T01:51:13Z,2025-09-22T14:08:29+00:00,2,12.29
16164,ggml-cpu: Respect cpumask settings with OpenMP,wishstudio,2025-09-22T03:09:15Z,2025-09-23T08:58:12+00:00,1,29.82
16165,vulkan: add RTE variants of exp shader,jeffbolznv,2025-09-22T03:15:39Z,2025-09-22T05:37:18+00:00,1,2.36
16172,minor: root cause in error message if loading backend library fails,rlewczuk,2025-09-22T08:48:23Z,2025-09-29T11:17:09+00:00,4,170.48
16174,codeowners : update + cleanup,ggerganov,2025-09-22T09:31:10Z,2025-09-22T15:20:21+00:00,4,5.82
16176,vulkan: handle mat_mul with A matrix > 4GB,jeffbolznv,2025-09-22T16:06:50Z,2025-09-28T01:36:34+00:00,2,129.5
16177,feat: Add conversion support in GraniteHybrid for non-hybrid (all attn),gabe-l-hart,2025-09-22T17:27:20Z,2025-09-22T18:40:10+00:00,7,1.21
16178,zdnn: refactor codebase + add docs,taronaeo,2025-09-22T18:38:52Z,2025-09-23T06:53:06+00:00,1,12.24
16183,ci: run the x64 and arm ci on the github machines instead,netrunnereve,2025-09-22T20:34:58Z,2025-09-25T05:06:06+00:00,2,56.52
16185,common : use cpp-httplib as a cURL alternative for downloads,angt,2025-09-22T23:17:35Z,2025-09-26T11:12:19+00:00,7,83.91
16190,codeowners : add @danbev to model-conversion example [no ci],danbev,2025-09-23T03:58:36Z,2025-09-23T06:13:22+00:00,1,2.25
16193,ggml-cpu: implement MXFP4 SIMD for s390x,taronaeo,2025-09-23T07:13:24Z,2025-09-26T10:27:26+00:00,3,75.23
16194,ci : enable Vulkan workflow on Mac,ggerganov,2025-09-23T07:32:42Z,2025-09-23T10:44:25+00:00,1,3.2
16199,Enhance text file detection logic for file attachments,allozaur,2025-09-23T13:09:16Z,2025-09-26T17:25:29+00:00,1,76.27
16202,tools/main: llama-cli: prevent spurious assistant token (#13402),vinkal-chudgar,2025-09-23T15:07:05Z,2025-09-29T07:03:12+00:00,3,135.94
16204,model : add label for LiquidAI LFM2-2.6B model,tdakhran,2025-09-23T16:33:27Z,2025-09-24T11:42:27+00:00,1,19.15
16208,CUDA: refactor and deduplicate vector FA kernels,JohannesGaessler,2025-09-23T22:26:46Z,2025-09-27T16:45:07+00:00,1,90.31
16212,vendors: update miniaudio version,taronaeo,2025-09-24T06:25:54Z,2025-09-25T15:38:10+00:00,1,33.2
16216,metal : relax reorder conditions,ggerganov,2025-09-24T07:51:52Z,2025-09-25T08:29:42+00:00,1,24.63
16222,Improve Mobile UI for dialogs and action dropdowns,allozaur,2025-09-24T12:16:04Z,2025-09-29T08:37:20+00:00,2,116.35
16224,vulkan : make the vulkan.hpp dynamic dispatcher instance private,Acly,2025-09-24T12:33:01Z,2025-09-27T20:41:03+00:00,2,80.13
16225,metal : extend mat-mat multiplication support,ggerganov,2025-09-24T12:45:24Z,2025-09-28T06:34:45+00:00,1,89.82
16229,codeowners: add ownership of zdnn backend [no ci],taronaeo,2025-09-24T15:18:53Z,2025-09-24T16:25:05+00:00,1,1.1
16231,devops: fix s390x docker release failure,taronaeo,2025-09-24T16:44:37Z,2025-09-25T03:36:30+00:00,1,10.86
16235,vulkan: support GET_ROWS for k-quants,jeffbolznv,2025-09-24T21:36:05Z,2025-09-27T10:36:11+00:00,1,61.0
16240,musa: upgrade musa sdk to 4.3.0,yeahdongcn,2025-09-25T04:52:43Z,2025-09-26T00:56:38+00:00,2,20.07
16243,server : add support for external server for tests,danbev,2025-09-25T07:53:41Z,2025-09-25T09:36:47+00:00,1,1.72
16249,ci : add AMD runners and workflows,ggerganov,2025-09-25T12:12:57Z,2025-09-29T14:51:48+00:00,1,98.65
16255,Allow viewing conversations even when llama server is down,allozaur,2025-09-25T14:52:28Z,2025-09-26T16:35:42+00:00,1,25.72
16257,build : fix build-ios-device,angt,2025-09-25T15:29:59Z,2025-09-26T10:39:35+00:00,1,19.16
16264,common : fix reasoning before forced tool call via tool_choice = required,crat0z,2025-09-26T01:12:25Z,2025-09-28T18:13:50+00:00,1,65.02
16273,minicpm: make embedding_scale residual_scale logit_scale optional with legacy defaults.,vinkal-chudgar,2025-09-26T09:32:11Z,2025-09-26T21:28:29+00:00,1,11.94
16274,metal : report OOM errors,ggerganov,2025-09-26T09:33:16Z,2025-09-26T11:14:28+00:00,1,1.69
16277,CUDA: mul_mat_id for mmf for bs <= 64 for f16 and bs <= 32 for f32,am17an,2025-09-26T12:23:42Z,2025-09-27T16:49:32+00:00,3,28.43
16289,Show message actions by default,allozaur,2025-09-27T11:28:41Z,2025-09-27T17:56:40+00:00,1,6.47
16290,server : remove old LLAMA_SERVER_SSL,angt,2025-09-27T11:40:10Z,2025-09-27T16:17:08+00:00,1,4.62
16292,vulkan: Fix validation failure in quantized flash attention,jeffbolznv,2025-09-27T17:29:52Z,2025-09-29T04:50:37+00:00,1,35.35
16297,docs: fixed a few typos in the README of the LLaMA.cpp HTTP Server,ImadSaddik,2025-09-28T09:26:03Z,2025-09-28T11:04:46+00:00,2,1.65
16307,ggml : fix GGML_F32_VEC_FMA argument order in ggml_vec_mad1_f32,CISC,2025-09-28T14:35:01Z,2025-09-28T21:15:03+00:00,1,6.67
16312,Preserve zero values in WebUI chat settings inputs and textareas,ServeurpersoCom,2025-09-28T16:21:34Z,2025-09-29T07:08:41+00:00,1,14.79
16318,ggml : fix dependencies for ggml_set_rows,ggerganov,2025-09-28T19:09:51Z,2025-09-29T05:41:28+00:00,1,10.53
16321,perplexity : show more kl-divergence data,ddh0,2025-09-28T20:40:00Z,2025-09-29T06:30:46+00:00,1,9.85
16323,ggml : check cuda and metal argsort limits and add test,CISC,2025-09-28T23:43:41Z,2025-09-29T09:09:01+00:00,4,9.42
16326,Fix thinking blocks with quotes + add handling `[THINK]...[/THINK]` blocks,ServeurpersoCom,2025-09-29T08:57:30Z,2025-09-29T16:49:48+00:00,1,7.87
